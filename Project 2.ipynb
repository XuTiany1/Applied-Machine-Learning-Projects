{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required library for data processing\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Import required library for dataset visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n"
      ],
      "metadata": {
        "id": "zNrCWoW8GjAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Acquire the data\n",
        "\n",
        "Two image datasets in your experiments: **Fashion MNIST**\n",
        "and **CIFAR-10**\n",
        "\n",
        "You can use existing machine learning libraries (such as PyTorch) to load the dataset, and should use\n",
        "the default train and test partitions.\n",
        "\n",
        "\n",
        "Note that while working with multilayer perceptrons, after loading the data, you will have to vectorize it so that it has the appropriate dimensions.\n",
        "\n",
        "Also, do not forget to normalize the training and test set\n",
        "\n"
      ],
      "metadata": {
        "id": "MByVeURhEDQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 1: Fashion MNIST Dataset\n",
        "\n",
        "https://medium.com/@aaysbt/fashion-mnist-data-training-using-pytorch-7f6ad71e96f4\n",
        "\n",
        "https://www.kaggle.com/datasets/zalando-research/fashionmnist/code"
      ],
      "metadata": {
        "id": "A5jy6qZNFwso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transform to convert images to tensors and normalize\n",
        "\n",
        "transform_FMNIST = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,),(0.5,),)])\n",
        "\n",
        "'''\n",
        "transforms.ToTenser convert PIL image(L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n",
        "or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "'''\n",
        "\n",
        "'''\n",
        "transform.Normalize Normalize a tensor image with mean and standard\n",
        "deviation. Tensor image size should be (C x H x W) to be normalized\n",
        "which we already did use transforms.ToTenser.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "_R-SxXTfHdCL",
        "outputId": "5274774e-fa0e-4f18-d3c1-95d28892b780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntransform.Normalize Normalize a tensor image with mean and standard\\ndeviation. Tensor image size should be (C x H x W) to be normalized\\nwhich we already did use transforms.ToTenser.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load train and test datasets\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "FMNIST_training_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                            download=True,\n",
        "                                            train=True,\n",
        "                                            transform=transform_FMNIST)\n",
        "\n",
        "FMINIST_trainloader = torch.utils.data.DataLoader(FMNIST_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "FMNIST_test_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                        download=True,\n",
        "                                        train=False,\n",
        "                                        transform=transform_FMNIST)\n",
        "\n",
        "FMINIST_testloader = torch.utils.data.DataLoader(FMNIST_test_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "\n",
        "'''\n",
        "torch.utils.data.Dataloader takes our data train or test data with parameter\n",
        "batch_size and shuffle. batch_size define the how many samples per batch to\n",
        "load, and shuffle parameter set the True to have the data reshuffled\n",
        "at every epoch.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "6FMM_ZhvFsnn",
        "outputId": "bff977e0-159b-4a03-c9f3-c5e0351b4f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 112531881.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 6633527.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 68223714.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 21106820.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntorch.utils.data.Dataloader takes our data train or test data with parameter\\nbatch_size and shuffle. batch_size define the how many samples per batch to\\nload, and shuffle parameter set the True to have the data reshuffled\\nat every epoch.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The data must be preprocessed before training the network.\n",
        "# If you inspect the first image in the training set, you will see that the\n",
        "# pixel values fall in the range of 0 to 255:\n",
        "\n",
        "# Scale these values to a range of 0 to 1 before feeding them to the neural\n",
        "# network model. To do so, divide the values by 255. It's important that the\n",
        "# training set and the testing set be preprocessed in the same way:\n",
        "\n",
        "FMNIST_training_set.data = FMNIST_training_set.data / 255.0\n",
        "FMNIST_test_set.data = FMNIST_test_set.data / 255.0"
      ],
      "metadata": {
        "id": "Jra-C0PQdipC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Explore dataset\n",
        "\n",
        "# What does the training data shape look like?\n",
        "\n",
        "FMNIST_training_set.data.shape, FMNIST_training_set.targets.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgqph8fcxfVm",
        "outputId": "69ecf3a7-99e7-48dd-cd56-4bb5dba40a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 28, 28]), torch.Size([60000]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(FMNIST_training_set.data[i], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "ULkw1ZYUeRWn",
        "outputId": "186e6358-c926-402e-9797-4a45bce6c253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What does the testing data shape look like?\n",
        "\n",
        "FMNIST_test_set.data.shape, FMNIST_test_set.targets.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDuupbOkyFgH",
        "outputId": "0f3c0c8b-0980-4357-bdc2-e9cbb628052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000, 28, 28]), torch.Size([10000]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of an image data\n",
        "\n",
        "print(f\"Visualization of image with target label {FMNIST_training_set.targets[0]}:\")\n",
        "\n",
        "plt.imshow(FMNIST_training_set.data[0].squeeze(0))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "odZoR-FrzTKq",
        "outputId": "b22d7612-6583-4c8d-e40b-7f11d1343d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualization of image with target label 9:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a9f6155f1912>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Visualization of image with target label {FMNIST_training_set.targets[0]}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFMNIST_training_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0minterpolation_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         resample=None, url=None, data=None, **kwargs):\n\u001b[0;32m-> 2695\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2308\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mgca\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \"\"\"\n\u001b[1;32m   1627\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m             projection_class, pkw = self._process_projection_requirements(\n\u001b[1;32m    756\u001b[0m                 *args, **kwargs)\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterization_zorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Disable grid on init to use rcParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m         self.grid(self._gridOn, which=mpl.rcParams['axes.grid.which'],\n\u001b[1;32m   1314\u001b[0m                   axis=mpl.rcParams['axes.grid.axis'])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mgrid\u001b[0;34m(self, visible, which, axis, **kwargs)\u001b[0m\n\u001b[1;32m   3194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m     def ticklabel_format(self, *, axis='both', style='', scilimits=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mgrid\u001b[0;34m(self, visible, which, **kwargs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m             gridkw['gridOn'] = (not self._major_tick_kw['gridOn']\n\u001b[1;32m   1659\u001b[0m                                 if visible is None else visible)\n\u001b[0;32m-> 1660\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'major'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgridkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_tick_params\u001b[0;34m(self, which, reset, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'major'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_major_tick_kw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m                     \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'minor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'both'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, cls)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mtick\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                 \"_tick_class or reimplement _get_tick()\")\n\u001b[1;32m   1550\u001b[0m         \u001b[0mtick_kw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_major_tick_kw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tick_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_tick_label_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0;31m# x in axes coords, y in data coords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, axes, loc, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mgrid_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         )\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpolation_steps\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mGRIDLINE_INTERPOLATION_STEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         self.label1 = mtext.Text(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;34m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m# views\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subtask 2: CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "f7PC6CLrJnTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_CIFAR_10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "    # Numbers from normalization comes from this link:\n",
        "    # https://www.kaggle.com/code/fanbyprinciple/cifar10-explanation-with-pytorch\n",
        "])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Load the CIFAR-10 training set\n",
        "CIFAR_10_training_set = datasets.CIFAR10('~/.pytorch/CIFAR10_data', download=True, train=True, transform=transform_CIFAR_10)\n",
        "\n",
        "# Load the CIFAR-10 test set\n",
        "CIFAR_10_test_set = datasets.CIFAR10('~/.pytorch/CIFAR10_data', download=True, train=False, transform=transform_CIFAR_10)\n",
        "\n",
        "# Create the data loaders\n",
        "CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRHu9Gl4JrXt",
        "outputId": "07d04bc8-cd72-4db7-a5ef-4fa5c3c76609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/.pytorch/CIFAR10_data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29720274.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.pytorch/CIFAR10_data/cifar-10-python.tar.gz to /root/.pytorch/CIFAR10_data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_10_training_set.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWAxoVYZnzzo",
        "outputId": "c51d3b65-f768-4322-8c7b-1e5f4ed0a143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_10_training_set.data.reshape(-1,32*32*3).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAoNris5UtJR",
        "outputId": "a57c650e-bb1a-426d-aa18-34cafd0b98d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 3072)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Implement the MLP Layer\n",
        "\n",
        "An MLP is composed of three types of layers: (1) an input layer, (2) hidden layers, (3) an output layer\n",
        "\n",
        "You should implement it from scratch based on the code available in the slides\n",
        "\n",
        "Your implementation should include the backpropagation and the mini-batch gradient descent algorithm used (e.g., SGD)"
      ],
      "metadata": {
        "id": "OsoXiM9CMK7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarizing the operation of the perceptron:\n",
        "\n",
        "- **Step 1**: Initialize the weights and bias with small-randomized values;\n",
        "\n",
        "- **Step 2**: Propagate all values in the input layer until output layer(Forward Propagation)\n",
        "\n",
        "- **Step 3**: Update weight and bias in the inner layers(Backpropagation)\n",
        "\n",
        "- **Step 4**: Do it until that the stop criterion is satisfied !"
      ],
      "metadata": {
        "id": "R8uyTpTcRHWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FMNIST_training_set.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVDQeGheY4vn",
        "outputId": "ef0b88a7-f14b-482b-c4fc-80e6e75b71ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FMNIST_training_set.targets.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir6Aj6NDZC9S",
        "outputId": "7c9ffcfb-04bb-4b49-f852-8af8df06b710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(FMNIST_training_set.data.view(-1,28*28).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8XnNutMhYqc",
        "outputId": "203a634c-a8a3-41ce-a8d7-eef72e2d6b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60000, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP():\n",
        "\n",
        "        # Constructor for MLP class\n",
        "    def __init__(self,\n",
        "                 training_features, training_label,\n",
        "                 validation_features, validation_label,\n",
        "                 num_hidden_layers = 1,\n",
        "                 num_neurons = 1,\n",
        "                 l1_lambda = 0,\n",
        "                 l2_lambda = 0,\n",
        "                 ):\n",
        "\n",
        "      # Adds bias term to the training features\n",
        "      self.training_features = self.add_bias(training_features)\n",
        "\n",
        "      # Convert integer labels to one-hot encoded labels for training targets        ==> ASSUMING 10 CLASSES\n",
        "      self.training_label = self.one_hot_encode(training_label, num_classes=10)\n",
        "\n",
        "      # Add bias term to validation_features\n",
        "      self.validation_features = self.add_bias(validation_features)\n",
        "\n",
        "      # Convert integer labels to one-hot encoded labels for validation targets\n",
        "      self.validation_label = self.one_hot_encode(validation_label, num_classes=10)\n",
        "\n",
        "      # Initialize number of hidden layers\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "      # Initialize number of neurons in each layer\n",
        "      self.num_neurons = num_neurons\n",
        "\n",
        "      # Get the number of training samples\n",
        "        # shape[0] gives you the size of the first dimension of the numpy array\n",
        "      self.num_samples = self.training_features.shape[0]\n",
        "\n",
        "      #initilise L1 regularization strength.\n",
        "      self.l1_lambda = l1_lambda\n",
        "\n",
        "      #initilise L2 regularization strength.\n",
        "      self.l2_lambda = l2_lambda\n",
        "\n",
        "      # Defining a list containing size of each layer\n",
        "      input_layer_size = self.training_features.shape[1]\n",
        "      hidden_layer_sizes = [num_neurons] * num_hidden_layers\n",
        "      output_layer_size = self.training_label.shape[1]\n",
        "      all_layer_sizes = [input_layer_size] + hidden_layer_sizes + [output_layer_size]\n",
        "      self.layer_sizes = np.array(all_layer_sizes)\n",
        "\n",
        "      # Initialize weights\n",
        "      self.weight_init()\n",
        "\n",
        "      # Initialize empty lists to store training and validation losses, accuracies, training time for each epoch\n",
        "      self.train_loss = list()\n",
        "      self.train_acc = list()\n",
        "      self.val_loss = list()\n",
        "      self.val_acc = list()\n",
        "      self.train_time = list()\n",
        "      self.tot_time = list()\n",
        "      # A list of metrics is created to store references to the previously initialized lists.\n",
        "      self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
        "\n",
        "\n",
        "    # <==== Helper functions for constructor ====> STARTS HERE\n",
        "\n",
        "    # Parameters:\n",
        "      # data: is a 2D array -> each row is an input\n",
        "    # Return value: data matrix with a column of 1 added at the end\n",
        "    def add_bias(self, data):\n",
        "      return np.concatenate((data, np.ones((data.shape[0], 1))), axis=1)\n",
        "\n",
        "    # Parameters:\n",
        "      # labels: an array of integer class labels. For instance, if you have 3 samples and they belong to classes 1, 2, and 0 respectively, labels could look like [1, 2, 0]\n",
        "      # num_classes: this is an integer indicating the total number of unique classes.\n",
        "    # Return value: The function outputs a numpy array where each row\n",
        "                  # corresponds to a one-hot encoded vector representation\n",
        "                  # of the original integer label for a sample.\n",
        "    def one_hot_encode(self, labels, num_classes):\n",
        "\n",
        "      # Create a 1D array of labels\n",
        "        #  1. Converts the elements in labels to integers.\n",
        "        #  2. Reshapes the labels array into a 1D array\n",
        "      labels_1d = labels.astype(np.int).reshape(-1)\n",
        "\n",
        "      # Use the identity matrix to get one-hot encoded vectors\n",
        "        # 1. np.eye(num_classes): Creates an identity matrix of size num_classes x num_classes.\n",
        "        # 2. np.eye(num_classes)[labels_1d]: maps each integer label to a one-hot encoded vector.\n",
        "      one_hot_encoded = np.eye(num_classes)[labels_1d]\n",
        "\n",
        "      return one_hot_encoded\n",
        "\n",
        "    # <==== Helper functions for constructor ====> ENDS HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <==== Activation functions ====> STARTS HERE\n",
        "\n",
        "    # Parameter:\n",
        "      # x: can be a number, a numpy array, or any other valid input for numpy operations\n",
        "    # Return the integer passed through a sigmoid function\n",
        "    def sigmoid_forward(self, x):\n",
        "      return 1./(1. + np.exp(-x))\n",
        "\n",
        "    # Parameter:\n",
        "      # y: the output of applying the sigmoid function to some input.\n",
        "    # Return the derivative of the sigmoid function given an output\n",
        "    def sigmoid_derivative(self, y):\n",
        "      return y*(1-y)\n",
        "\n",
        "    # Parameters:\n",
        "      # x: can be a number, a numpy array, or any other valid input for numpy operations\n",
        "    # Returns the value after applying the ReLU function\n",
        "    def relu_forward(self, x):\n",
        "      return np.maximum(0, x)\n",
        "\n",
        "    # Parameters:\n",
        "      # x: the original input that was passed to the ReLU function\n",
        "    # Returns the derivative of the ReLU function given input x\n",
        "    def relu_derivative(self, x):\n",
        "      return (x > 0).astype(float)\n",
        "\n",
        "\n",
        "    # <==== Activation functions ====> ENDS HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <===== Predictive functions =====> STARTS HERE\n",
        "\n",
        "    # softmax function takes a vector of raw scores (often called logits) and\n",
        "    # converts them into normalized probabilities\n",
        "    # Parameter:\n",
        "      # x: usually a matrix where each row corresponds to a set of logits for a particular data sample.\n",
        "    # Return a matrix where each row contains the softmax-normalized probabilities corresponding to the logits in x\n",
        "    def softmax(self, x):\n",
        "\n",
        "      exponential_x = np.exp(x)\n",
        "        # 'axis=1' means summing over the columns for each row\n",
        "        # 'keepdims=True' argument ensures that the resulting sum maintains the same number of dimensions as the original matrix\n",
        "      return exponential_x/exponential_x.sum(axis=1,keepdims=True)\n",
        "\n",
        "\n",
        "    # Function convert the output probabilities of a neural network into discrete class predictions\n",
        "    # Parameter:\n",
        "      # x: a matrix where each row contains a set of probabilities (like those from the output of a softmax function).\n",
        "    # Return\n",
        "    def categorical(self, x):\n",
        "\n",
        "      # Initialize matrix of zeroes\n",
        "      categorical = np.zeros((x.shape[0],self.training_label.shape[1]))\n",
        "\n",
        "      # Finding the max probability and assigning classes\n",
        "      # Step 1: Find the number of rows in the matrix 'x'\n",
        "      num_rows = x.shape[0]\n",
        "\n",
        "      # Step 2: Generate an array of row indices\n",
        "      row_indices = np.arange(num_rows)\n",
        "\n",
        "      # Step 3: For each row in 'x', find the column index with the maximum value\n",
        "      max_col_indices = x.argmax(axis=1)\n",
        "\n",
        "      # Step 4: Initialize a matrix filled with zeros\n",
        "      categorical = np.zeros((num_rows, self.training_label.shape[1]))\n",
        "\n",
        "      # Step 5: Set the positions corresponding to the maximum values in 'x' to 1 in 'categorical'\n",
        "      for i in range(num_rows):\n",
        "          categorical[i, max_col_indices[i]] = 1\n",
        "\n",
        "      return categorical\n",
        "\n",
        "    # <===== Predictive functions =====> ENDS HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <===== Model evaluation functions =======> STARTS HERE\n",
        "\n",
        "    # This function calculates the categorical cross-entropy loss\n",
        "    def loss_function(self, y_pred, y_label):\n",
        "      # Step 1: compute nugative log of predictions\n",
        "      neg_log_pred = -np.log(y_pred)\n",
        "\n",
        "      # Step 2: Multiply the negative logarithms by the true labels element-wise\n",
        "      weighted_loss = neg_log_pred * y_label\n",
        "\n",
        "      # Step 3: Sum along the rows\n",
        "      sum_loss_per_sample = weighted_loss.sum(axis=1)\n",
        "\n",
        "      # Step 4: Compute the average loss\n",
        "      average_loss = sum_loss_per_sample.mean()\n",
        "\n",
        "      return average_loss\n",
        "\n",
        "\n",
        "    # This function computes the classification accuracy of predictions\n",
        "    def accuracy_evaluation(self, y_pred, y_label):\n",
        "        return np.all(y_pred==y_label,axis=1).mean()\n",
        "\n",
        "\n",
        "    def evaluate(self,input_feature,output_label):\n",
        "      # Evaluate the performance (accuracy) predicting on X with true labels Y\n",
        "      prediction = self.predict(input_feature)\n",
        "      return self.accuracy_evaluation(prediction,output_label)\n",
        "\n",
        "\n",
        "    # <===== Model evaluation functions =======> ENDS HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <===== Model initialization functions ======> STARTS HERE\n",
        "\n",
        "\n",
        "    # Function sets up the initial weights for the neural network based on the specified layer sizes.\n",
        "    def weight_init(self):\n",
        "\n",
        "      # Step 1: initialize an empty list that will hold the weight matrices for each layer of the network\n",
        "      self.weights = list()\n",
        "\n",
        "      # Step 2: loop over layers\n",
        "      # If there are n layers (including input and output layers), then there will be n-1 sets of weights connecting them\n",
        "      for i in range(self.layer_sizes.shape[0]-1):\n",
        "\n",
        "        # Step 3: initialize the weights for each individual layer => RANDOM INITIALIZATION METHOD USED HERE\n",
        "          # The dimensions of this matrix depend on:\n",
        "            # the number of nodes in the current layer (self.layer_sizes[i])\n",
        "            # the number of nodes in the next layer (self.layer_sizes[i+1])\n",
        "        self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
        "\n",
        "      # Step 4: Convert list to array and store to class attribute\n",
        "      self.weights = np.asarray(self.weights)\n",
        "\n",
        "\n",
        "\n",
        "    # FUNCTIONS FOR WEIGHT INITIALIZATION\n",
        "\n",
        "    # 1) All-zero weight initialization\n",
        "    def weight_init_all_zero(self):\n",
        "      self.weights = list()\n",
        "\n",
        "      for i in range(self.layer_sizes.shape[0]-1):\n",
        "        self.weights.append(np.zeros((self.layer_sizes[i], self.layer_sizes[i+1])))\n",
        "      self.weights = np.asarray(self.weights)\n",
        "\n",
        "\n",
        "    # 2) Uniform [-1, 1] weight initialization\n",
        "    def weight_init_uniform(self):\n",
        "      self.weights = list()\n",
        "\n",
        "      for i in range(self.layer_sizes.shape[0]-1):\n",
        "        self.weights.append(np.random.uniform(-1, 1, size=[self.layer_sizes[i], self.layer_sizes[i+1]]))\n",
        "      self.weights = np.asarray(self.weights)\n",
        "\n",
        "\n",
        "    # 3) Gaussian N(0,1) weight initialization\n",
        "    def weight_init_gaussian(self):\n",
        "      self.weights = list()\n",
        "\n",
        "      for i in range(self.layer_sizes.shape[0]-1):\n",
        "        # initialize the weights using a Gaussian (normal) distribution with mean 0 and standard deviation 1\n",
        "        self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "        #np.random.randn generates samples from a standard normal distribution (also called the Z-distribution)\n",
        "      self.weights = np.asarray(self.weights)\n",
        "\n",
        "\n",
        "    # 4) Xavier weight initialization\n",
        "\n",
        "    def weight_init_xavier(self):\n",
        "      self.weights = list()\n",
        "\n",
        "      for i in range(self.layer_sizes.shape[0]-1):\n",
        "        # Xavier initialization\n",
        "        n_in = self.layer_sizes[i]\n",
        "        n_out = self.layer_sizes[i+1]\n",
        "        limit = np.sqrt(6 / (n_in + n_out))\n",
        "        self.weights.append(np.random.uniform(-limit, limit, size=[n_in, n_out]))\n",
        "\n",
        "      self.weights = np.asarray(self.weights)\n",
        "\n",
        "\n",
        "    # 5) Kaiming weight initialization\n",
        "    def weight_init_kaiming(self):\n",
        "      self.weights = list()\n",
        "\n",
        "      for i in range(self.layer_sizes.shape[0]-1):\n",
        "        # Kaiming initialization\n",
        "        n_in = self.layer_sizes[i]\n",
        "        self.weights.append(np.random.randn(n_in, self.layer_sizes[i+1]) * np.sqrt(2 / n_in))\n",
        "\n",
        "      self.weights = np.asarray(self.weights)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Function initializes the activations for each layer in the neural network for a given batch of input data.\n",
        "    # Parameter:\n",
        "      # batch_size: represents the number of data samples that will be processed simultaneously in a single forward and backward pass during training\n",
        "    def layers_init(self, batch_size):\n",
        "\n",
        "      # Step 1: initialize an empty list that will hold the activations for each layer\n",
        "      layer_activations = []\n",
        "\n",
        "      # Step 2: loop through all layer sizes stored previously\n",
        "      for curr_layer_size in self.layer_sizes:\n",
        "\n",
        "        # Step 3: create an empty array for the current layer\n",
        "        activation_array = np.empty((batch_size, curr_layer_size))\n",
        "\n",
        "        # Step 4: append the array to the activation list\n",
        "        layer_activations.append(activation_array)\n",
        "\n",
        "      # Step 5: store in class attribute\n",
        "      self.layer_activations = layer_activations\n",
        "\n",
        "\n",
        "    # <======== Model initialization functions =========> ENDS HERE\n",
        "\n",
        "\n",
        "\n",
        "    # <======== FORWARDPROP, BACKPROP, PREDICT ==========> STARTS HERE\n",
        "\n",
        "    # function implements the forward propagation step for a neural network\n",
        "    # Parameter:\n",
        "      # batch: a batch of input data\n",
        "    def forward_prop(self, batch):\n",
        "\n",
        "      # Step 1: initialize the first activation with input data\n",
        "        # This input data becomes the activation of the first layer (input layer).\n",
        "      activation_values = batch\n",
        "      self.layer_activations[0] = activation_values\n",
        "\n",
        "\n",
        "\n",
        "      # Step 2: loop through weights\n",
        "      for curr_layer_num, curr_weight in enumerate(self.weights):\n",
        "\n",
        "        # Step 3: calculating using SIGMOID -> MODIFY LATER\n",
        "          # First, activation from previous layer is dot product with current layer's weight\n",
        "          # Then, we pass it through the activation function to produce next layer's activation\n",
        "          # Lastly, this value is stored again in the a_l\n",
        "        activation_values = self.sigmoid_forward(activation_values.dot(curr_weight))\n",
        "\n",
        "        # Step 4: store the activation for the current layer\n",
        "        self.layer_activations[curr_layer_num+1] = activation_values\n",
        "\n",
        "      # Step 5: compute output with softmax activation and store in class attribute\n",
        "      self.output = self.softmax(self.layer_activations[-1])\n",
        "\n",
        "\n",
        "\n",
        "    # function computing the gradient of the loss with respect to the network's weights,\n",
        "    # and then using this gradient to update the weights to reduce the error.\n",
        "\n",
        "    # Parameter:\n",
        "      # batch_y: a batch of target values\n",
        "    def backprop(self, batch_y):\n",
        "\n",
        "\n",
        "      # Step 1: Compute output layer error\n",
        "        # Mini-step 1. Calculate the difference between predicted outputs and actual targets.\n",
        "      output_error = self.output - batch_y\n",
        "\n",
        "        # Mini-step 2: Compute the derivative of the sigmoid function for the activations of the last hidden layer.\n",
        "      sigmoid_derivatives = self.sigmoid_derivative(self.layer_activations[-1])\n",
        "\n",
        "        # Mini-step 3: Element-wise multiplication of the output error and the sigmoid derivative to get the error for the output layer.\n",
        "      delta_t = output_error * sigmoid_derivatives\n",
        "\n",
        "\n",
        "      # Step 2: Iterate over network weights in reverse\n",
        "      for i in range(1,len(self.weights)+1):\n",
        "\n",
        "        # Step 3: Update the weights for the current layer\n",
        "          # Mini-step 1. Transpose the activations from the previous layer\n",
        "        transposed_activations = self.layer_activations[-i-1].T\n",
        "\n",
        "          # Mini-step 2. Calculate the gradient for the current layer's weights.\n",
        "            # This is done by taking the dot product of the transposed activations and the error (`delta_t`)\n",
        "        gradient = transposed_activations.dot(delta_t)\n",
        "\n",
        "          # Mini-step 3. Average the gradient over the entire batch.\n",
        "        averaged_gradient = gradient / self.batch_size\n",
        "\n",
        "          # Mini-step 4. Scale the averaged gradient by the learning rate\n",
        "        lr_scaled_gradient = self.lr * averaged_gradient\n",
        "\n",
        "          # Mini-step 5. Update the weights of the current layer by subtracting the scaled gradient\n",
        "        regularized_term_l1 = self.l1_lambda * np.sign(self.weights[-i])  # L1 regularization term\n",
        "        regularized_term_l2 = 2 * self.l2_lambda * self.weights[-i]  # L2 regularization term\n",
        "        self.weights[-i] = (self.weights[-i] - lr_scaled_gradient) - regularized_term_l1 - regularized_term_l2\n",
        "\n",
        "\n",
        "\n",
        "        # Step 4: Compute error for the previous layer\n",
        "          # Mini-step 1. Compute the derivative of the sigmoid function for the activations of the current hidden layer.\n",
        "        sigmoid_derivatives = self.sigmoid_derivative(self.layer_activations[-i-1])\n",
        "\n",
        "          # Mini-step 2. Transpose the weights of the current layer.\n",
        "        transposed_weights = self.weights[-i].T\n",
        "\n",
        "          # Mini-step 3. Calculate the product of the error (`delta_t`) and the transposed weights.\n",
        "        error_weight_product = delta_t.dot(transposed_weights)\n",
        "\n",
        "          # Mini-step 4. Element-wise multiplication of the sigmoid derivative and the result from step 3 to compute the error for the current layer.\n",
        "        delta_t = sigmoid_derivatives * error_weight_product\n",
        "\n",
        "\n",
        "\n",
        "    # function produces predictions given input data\n",
        "    # Parameter:\n",
        "      # input_feature: input data for which we want an output to\n",
        "    def predict(self, input_feature):\n",
        "\n",
        "      # Step 1: add a column of bias 1\n",
        "      input_feature = np.concatenate((input_feature,np.ones((input_feature.shape[0],1))),axis=1)\n",
        "\n",
        "      # Step 2: initialize layer\n",
        "        #  initializes arrays for the hidden layer activations, preparing them for the subsequent feed-forward step.\n",
        "      self.layers_init(input_feature.shape[0])\n",
        "\n",
        "      # Step 3: perform fowrad propogation to get output predictions\n",
        "      self.forward_prop(input_feature)\n",
        "\n",
        "      # Step 4: convert raw output to one-hot encoded format\n",
        "      return self.categorical(self.output)\n",
        "\n",
        "    # <======== FORWARDPROP, BACKPROP, PREDICT ==========> ENDS HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <========== TRAIN =========> STARTS HERE\n",
        "\n",
        "    def train(self, batch_size = 8, epochs = 25, lr = 1.0, early_stopping_patience=3):\n",
        "\n",
        "      # Initialize hyperparameters\n",
        "      self.lr = lr\n",
        "      self.batch_size=batch_size\n",
        "\n",
        "      best_val_acc = 0  # Track the best validation accuracy\n",
        "      consecutive_no_improvement = 0  # Track consecutive epochs without improvement\n",
        "\n",
        "      # Loop through epochs\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "        # Initialize the layer activations\n",
        "        self.layers_init(self.batch_size)\n",
        "\n",
        "        # Shuffle data to prevent the model from \"memorizing\" the data.\n",
        "        shuffle = np.random.permutation(self.num_samples)\n",
        "\n",
        "        # Initialize train loss and accuracy\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "\n",
        "        # Split data into batches\n",
        "        X_batches = np.array_split(self.training_features[shuffle],self.num_samples/self.batch_size)\n",
        "        Y_batches = np.array_split(self.training_label[shuffle],self.num_samples/self.batch_size)\n",
        "\n",
        "        # Batch-wise pairing\n",
        "        for batch_x,batch_y in zip(X_batches,Y_batches):\n",
        "          # Forward prop\n",
        "          self.forward_prop(batch_x)\n",
        "          # Compute loss and accuracy\n",
        "          train_loss += self.loss_function(self.output,batch_y)\n",
        "          train_acc += self.accuracy_evaluation(self.categorical(self.output),batch_y)\n",
        "          # Backprop\n",
        "          self.backprop(batch_y)\n",
        "\n",
        "        # Computes and store average training loss and accuracy over all batches.\n",
        "        train_loss = (train_loss/len(X_batches))\n",
        "        train_acc = (train_acc/len(X_batches))\n",
        "        self.train_loss.append(train_loss)\n",
        "        self.train_acc.append(train_acc)\n",
        "\n",
        "\n",
        "        # After training on the batches, the model's performance is evaluated on the validation set\n",
        "        # This provides an indication of how well the model is generalizing to unseen data.\n",
        "        self.layers_init(self.validation_features.shape[0])\n",
        "        self.forward_prop(self.validation_features)\n",
        "        val_loss = self.loss_function(self.output,self.validation_label)\n",
        "        val_acc = self.accuracy_evaluation(self.categorical(self.output),self.validation_label)\n",
        "        self.val_loss.append(val_loss)\n",
        "        self.val_acc.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} |\")\n",
        "\n",
        "        # Check for early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            consecutive_no_improvement = 0\n",
        "        else:\n",
        "            consecutive_no_improvement +=1\n",
        "\n",
        "        if consecutive_no_improvement >= early_stopping_patience:\n",
        "            print(f\"Early stopping after {early_stopping_patience} epochs without improvement.\")\n",
        "            break\n",
        "\n",
        "      return best_val_acc\n"
      ],
      "metadata": {
        "id": "tojDhed1NeHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the best batch_size and learning rate."
      ],
      "metadata": {
        "id": "14nu-a5Hli3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_batch_lr(data, targets, validation_data, validation_targets, num_hidden_layers, num_neurons, batch_sizes, learning_rates):\n",
        "    best_accuracy = 0\n",
        "    best_batch_size = 0\n",
        "    best_lr = 0\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for lr in learning_rates:\n",
        "            print(f\"~~~~~~~~~~~~~~~~~~Batch size: {batch_size}~~~~~~~~~~~~~~~~~~\")\n",
        "            print(f\"~~~~~~~~~~~~~~~~~~Learning rate: {lr}~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "            # Create a new MLP model\n",
        "            model = MLP(data, targets, validation_data, validation_targets, num_hidden_layers, num_neurons)\n",
        "            accuracy = model.train(batch_size=batch_size, epochs=25, lr=lr)\n",
        "\n",
        "            # Evaluate the model on the validation set\n",
        "            # accuracy = model.evaluate(model.categorical(validation_data), validation_targets)\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_batch_size = batch_size\n",
        "                best_lr = lr\n",
        "\n",
        "    print(f\"Best batch size: {best_batch_size}\")\n",
        "    print(f\"Best learning rate: {best_lr}\")"
      ],
      "metadata": {
        "id": "onQUEciMiKdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [4, 8, 16, 32, 64, 128]\n",
        "learning_rates = [0.001, 0.01, 0.1, 1, 10]"
      ],
      "metadata": {
        "id": "O3Sfh8RCiPlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_best_batch_lr(FMNIST_training_set.data.view(-1,28*28).numpy(),\n",
        "            FMNIST_training_set.targets.numpy(),\n",
        "            FMNIST_test_set.data.view(-1,28*28).numpy(),\n",
        "            FMNIST_test_set.targets.numpy(),\n",
        "            num_hidden_layers=2,num_neurons=128,\n",
        "            batch_sizes=batch_sizes, learning_rates=learning_rates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1teBAdhil2P",
        "outputId": "fce19a91-58cb-4e6c-8c73-ac12737a139f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~Batch size: 4~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.001~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-a2556287c5b3>:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-15-a2556287c5b3>:233: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 2.212 | acc = 0.103 | val_loss = 2.124 | val_acc = 0.103 |\n",
            "Epoch 2: loss = 2.072 | acc = 0.103 | val_loss = 2.028 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 1.985 | acc = 0.1 | val_loss = 1.952 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 1.925 | acc = 0.101 | val_loss = 1.907 | val_acc = 0.101 |\n",
            "Epoch 5: loss = 1.89 | acc = 0.105 | val_loss = 1.88 | val_acc = 0.109 |\n",
            "Epoch 6: loss = 1.868 | acc = 0.13 | val_loss = 1.862 | val_acc = 0.155 |\n",
            "Epoch 7: loss = 1.851 | acc = 0.245 | val_loss = 1.842 | val_acc = 0.412 |\n",
            "Epoch 8: loss = 1.792 | acc = 0.616 | val_loss = 1.775 | val_acc = 0.648 |\n",
            "Epoch 9: loss = 1.762 | acc = 0.676 | val_loss = 1.758 | val_acc = 0.677 |\n",
            "Epoch 10: loss = 1.747 | acc = 0.694 | val_loss = 1.745 | val_acc = 0.691 |\n",
            "Epoch 11: loss = 1.735 | acc = 0.706 | val_loss = 1.734 | val_acc = 0.703 |\n",
            "Epoch 12: loss = 1.725 | acc = 0.716 | val_loss = 1.726 | val_acc = 0.71 |\n",
            "Epoch 13: loss = 1.717 | acc = 0.722 | val_loss = 1.718 | val_acc = 0.717 |\n",
            "Epoch 14: loss = 1.71 | acc = 0.728 | val_loss = 1.712 | val_acc = 0.721 |\n",
            "Epoch 15: loss = 1.704 | acc = 0.732 | val_loss = 1.707 | val_acc = 0.725 |\n",
            "Epoch 16: loss = 1.699 | acc = 0.735 | val_loss = 1.702 | val_acc = 0.728 |\n",
            "Epoch 17: loss = 1.694 | acc = 0.738 | val_loss = 1.697 | val_acc = 0.733 |\n",
            "Epoch 18: loss = 1.69 | acc = 0.741 | val_loss = 1.693 | val_acc = 0.736 |\n",
            "Epoch 19: loss = 1.686 | acc = 0.742 | val_loss = 1.689 | val_acc = 0.738 |\n",
            "Epoch 20: loss = 1.682 | acc = 0.745 | val_loss = 1.686 | val_acc = 0.74 |\n",
            "Epoch 21: loss = 1.679 | acc = 0.747 | val_loss = 1.683 | val_acc = 0.742 |\n",
            "Epoch 22: loss = 1.676 | acc = 0.749 | val_loss = 1.68 | val_acc = 0.744 |\n",
            "Epoch 23: loss = 1.673 | acc = 0.751 | val_loss = 1.677 | val_acc = 0.746 |\n",
            "Epoch 24: loss = 1.671 | acc = 0.753 | val_loss = 1.675 | val_acc = 0.748 |\n",
            "Epoch 25: loss = 1.668 | acc = 0.755 | val_loss = 1.672 | val_acc = 0.749 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 4~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.989 | acc = 0.193 | val_loss = 1.838 | val_acc = 0.538 |\n",
            "Epoch 2: loss = 1.786 | acc = 0.623 | val_loss = 1.762 | val_acc = 0.643 |\n",
            "Epoch 3: loss = 1.719 | acc = 0.695 | val_loss = 1.676 | val_acc = 0.743 |\n",
            "Epoch 4: loss = 1.659 | acc = 0.756 | val_loss = 1.657 | val_acc = 0.752 |\n",
            "Epoch 5: loss = 1.645 | acc = 0.766 | val_loss = 1.646 | val_acc = 0.764 |\n",
            "Epoch 6: loss = 1.635 | acc = 0.772 | val_loss = 1.638 | val_acc = 0.768 |\n",
            "Epoch 7: loss = 1.628 | acc = 0.776 | val_loss = 1.632 | val_acc = 0.769 |\n",
            "Epoch 8: loss = 1.622 | acc = 0.781 | val_loss = 1.627 | val_acc = 0.774 |\n",
            "Epoch 9: loss = 1.617 | acc = 0.786 | val_loss = 1.623 | val_acc = 0.781 |\n",
            "Epoch 10: loss = 1.612 | acc = 0.789 | val_loss = 1.619 | val_acc = 0.788 |\n",
            "Epoch 11: loss = 1.608 | acc = 0.793 | val_loss = 1.615 | val_acc = 0.788 |\n",
            "Epoch 12: loss = 1.605 | acc = 0.798 | val_loss = 1.612 | val_acc = 0.792 |\n",
            "Epoch 13: loss = 1.602 | acc = 0.803 | val_loss = 1.611 | val_acc = 0.793 |\n",
            "Epoch 14: loss = 1.599 | acc = 0.806 | val_loss = 1.608 | val_acc = 0.8 |\n",
            "Epoch 15: loss = 1.597 | acc = 0.809 | val_loss = 1.606 | val_acc = 0.799 |\n",
            "Epoch 16: loss = 1.594 | acc = 0.812 | val_loss = 1.604 | val_acc = 0.806 |\n",
            "Epoch 17: loss = 1.592 | acc = 0.816 | val_loss = 1.602 | val_acc = 0.807 |\n",
            "Epoch 18: loss = 1.59 | acc = 0.818 | val_loss = 1.6 | val_acc = 0.808 |\n",
            "Epoch 19: loss = 1.588 | acc = 0.821 | val_loss = 1.599 | val_acc = 0.81 |\n",
            "Epoch 20: loss = 1.587 | acc = 0.823 | val_loss = 1.597 | val_acc = 0.813 |\n",
            "Epoch 21: loss = 1.585 | acc = 0.824 | val_loss = 1.596 | val_acc = 0.812 |\n",
            "Epoch 22: loss = 1.584 | acc = 0.827 | val_loss = 1.594 | val_acc = 0.816 |\n",
            "Epoch 23: loss = 1.582 | acc = 0.829 | val_loss = 1.593 | val_acc = 0.817 |\n",
            "Epoch 24: loss = 1.581 | acc = 0.83 | val_loss = 1.592 | val_acc = 0.819 |\n",
            "Epoch 25: loss = 1.579 | acc = 0.832 | val_loss = 1.591 | val_acc = 0.82 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 4~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.677 | acc = 0.724 | val_loss = 1.627 | val_acc = 0.752 |\n",
            "Epoch 2: loss = 1.607 | acc = 0.784 | val_loss = 1.608 | val_acc = 0.789 |\n",
            "Epoch 3: loss = 1.589 | acc = 0.815 | val_loss = 1.593 | val_acc = 0.809 |\n",
            "Epoch 4: loss = 1.578 | acc = 0.83 | val_loss = 1.585 | val_acc = 0.817 |\n",
            "Epoch 5: loss = 1.571 | acc = 0.838 | val_loss = 1.581 | val_acc = 0.832 |\n",
            "Epoch 6: loss = 1.565 | acc = 0.845 | val_loss = 1.576 | val_acc = 0.836 |\n",
            "Epoch 7: loss = 1.561 | acc = 0.849 | val_loss = 1.572 | val_acc = 0.843 |\n",
            "Epoch 8: loss = 1.557 | acc = 0.854 | val_loss = 1.572 | val_acc = 0.846 |\n",
            "Epoch 9: loss = 1.555 | acc = 0.858 | val_loss = 1.569 | val_acc = 0.843 |\n",
            "Epoch 10: loss = 1.552 | acc = 0.86 | val_loss = 1.567 | val_acc = 0.848 |\n",
            "Epoch 11: loss = 1.55 | acc = 0.862 | val_loss = 1.567 | val_acc = 0.845 |\n",
            "Epoch 12: loss = 1.548 | acc = 0.864 | val_loss = 1.565 | val_acc = 0.849 |\n",
            "Epoch 13: loss = 1.546 | acc = 0.868 | val_loss = 1.564 | val_acc = 0.852 |\n",
            "Epoch 14: loss = 1.544 | acc = 0.868 | val_loss = 1.563 | val_acc = 0.849 |\n",
            "Epoch 15: loss = 1.543 | acc = 0.87 | val_loss = 1.564 | val_acc = 0.852 |\n",
            "Epoch 16: loss = 1.541 | acc = 0.872 | val_loss = 1.561 | val_acc = 0.855 |\n",
            "Epoch 17: loss = 1.54 | acc = 0.873 | val_loss = 1.565 | val_acc = 0.852 |\n",
            "Epoch 18: loss = 1.539 | acc = 0.874 | val_loss = 1.56 | val_acc = 0.856 |\n",
            "Epoch 19: loss = 1.538 | acc = 0.877 | val_loss = 1.561 | val_acc = 0.855 |\n",
            "Epoch 20: loss = 1.537 | acc = 0.877 | val_loss = 1.559 | val_acc = 0.856 |\n",
            "Epoch 21: loss = 1.535 | acc = 0.879 | val_loss = 1.558 | val_acc = 0.856 |\n",
            "Epoch 22: loss = 1.535 | acc = 0.88 | val_loss = 1.558 | val_acc = 0.856 |\n",
            "Epoch 23: loss = 1.534 | acc = 0.882 | val_loss = 1.559 | val_acc = 0.858 |\n",
            "Epoch 24: loss = 1.533 | acc = 0.882 | val_loss = 1.559 | val_acc = 0.862 |\n",
            "Epoch 25: loss = 1.531 | acc = 0.884 | val_loss = 1.557 | val_acc = 0.858 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 4~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.798 | acc = 0.638 | val_loss = 1.78 | val_acc = 0.654 |\n",
            "Epoch 2: loss = 1.778 | acc = 0.659 | val_loss = 1.773 | val_acc = 0.67 |\n",
            "Epoch 3: loss = 1.774 | acc = 0.667 | val_loss = 1.773 | val_acc = 0.667 |\n",
            "Epoch 4: loss = 1.772 | acc = 0.669 | val_loss = 1.773 | val_acc = 0.67 |\n",
            "Epoch 5: loss = 1.771 | acc = 0.67 | val_loss = 1.771 | val_acc = 0.675 |\n",
            "Epoch 6: loss = 1.77 | acc = 0.672 | val_loss = 1.774 | val_acc = 0.672 |\n",
            "Epoch 7: loss = 1.77 | acc = 0.678 | val_loss = 1.773 | val_acc = 0.663 |\n",
            "Epoch 8: loss = 1.769 | acc = 0.679 | val_loss = 1.783 | val_acc = 0.651 |\n",
            "Epoch 9: loss = 1.769 | acc = 0.682 | val_loss = 1.778 | val_acc = 0.682 |\n",
            "Epoch 10: loss = 1.769 | acc = 0.693 | val_loss = 1.776 | val_acc = 0.721 |\n",
            "Epoch 11: loss = 1.768 | acc = 0.685 | val_loss = 1.778 | val_acc = 0.672 |\n",
            "Epoch 12: loss = 1.767 | acc = 0.689 | val_loss = 1.77 | val_acc = 0.668 |\n",
            "Epoch 13: loss = 1.766 | acc = 0.686 | val_loss = 1.786 | val_acc = 0.65 |\n",
            "Epoch 14: loss = 1.768 | acc = 0.678 | val_loss = 1.773 | val_acc = 0.663 |\n",
            "Epoch 15: loss = 1.766 | acc = 0.673 | val_loss = 1.774 | val_acc = 0.677 |\n",
            "Epoch 16: loss = 1.768 | acc = 0.679 | val_loss = 1.777 | val_acc = 0.686 |\n",
            "Epoch 17: loss = 1.769 | acc = 0.678 | val_loss = 1.769 | val_acc = 0.682 |\n",
            "Epoch 18: loss = 1.769 | acc = 0.686 | val_loss = 1.775 | val_acc = 0.679 |\n",
            "Epoch 19: loss = 1.768 | acc = 0.676 | val_loss = 1.773 | val_acc = 0.669 |\n",
            "Epoch 20: loss = 1.767 | acc = 0.683 | val_loss = 1.771 | val_acc = 0.723 |\n",
            "Epoch 21: loss = 1.766 | acc = 0.689 | val_loss = 1.773 | val_acc = 0.67 |\n",
            "Epoch 22: loss = 1.766 | acc = 0.696 | val_loss = 1.775 | val_acc = 0.683 |\n",
            "Epoch 23: loss = 1.765 | acc = 0.691 | val_loss = 1.777 | val_acc = 0.67 |\n",
            "Epoch 24: loss = 1.766 | acc = 0.699 | val_loss = 1.769 | val_acc = 0.67 |\n",
            "Epoch 25: loss = 1.766 | acc = 0.69 | val_loss = 1.768 | val_acc = 0.702 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 4~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.415 | acc = 0.149 | val_loss = 2.392 | val_acc = 0.216 |\n",
            "Epoch 2: loss = 2.392 | acc = 0.22 | val_loss = 2.392 | val_acc = 0.217 |\n",
            "Epoch 3: loss = 2.377 | acc = 0.194 | val_loss = 2.367 | val_acc = 0.174 |\n",
            "Epoch 4: loss = 2.368 | acc = 0.137 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 5: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 6: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 7: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 8: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 9: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 10: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 11: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 12: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 13: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 14: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 15: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 16: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 17: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 18: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 19: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 20: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 21: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 22: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 23: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 24: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.108 |\n",
            "Epoch 25: loss = 2.367 | acc = 0.11 | val_loss = 2.367 | val_acc = 0.107 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 8~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.295 | acc = 0.125 | val_loss = 2.243 | val_acc = 0.126 |\n",
            "Epoch 2: loss = 2.207 | acc = 0.126 | val_loss = 2.174 | val_acc = 0.127 |\n",
            "Epoch 3: loss = 2.15 | acc = 0.127 | val_loss = 2.131 | val_acc = 0.126 |\n",
            "Epoch 4: loss = 2.115 | acc = 0.128 | val_loss = 2.102 | val_acc = 0.128 |\n",
            "Epoch 5: loss = 2.09 | acc = 0.13 | val_loss = 2.08 | val_acc = 0.13 |\n",
            "Epoch 6: loss = 2.067 | acc = 0.13 | val_loss = 2.054 | val_acc = 0.126 |\n",
            "Epoch 7: loss = 2.027 | acc = 0.126 | val_loss = 2.002 | val_acc = 0.127 |\n",
            "Epoch 8: loss = 1.98 | acc = 0.155 | val_loss = 1.964 | val_acc = 0.199 |\n",
            "Epoch 9: loss = 1.94 | acc = 0.335 | val_loss = 1.915 | val_acc = 0.495 |\n",
            "Epoch 10: loss = 1.896 | acc = 0.555 | val_loss = 1.885 | val_acc = 0.562 |\n",
            "Epoch 11: loss = 1.866 | acc = 0.578 | val_loss = 1.846 | val_acc = 0.572 |\n",
            "Epoch 12: loss = 1.824 | acc = 0.584 | val_loss = 1.811 | val_acc = 0.581 |\n",
            "Epoch 13: loss = 1.799 | acc = 0.6 | val_loss = 1.792 | val_acc = 0.6 |\n",
            "Epoch 14: loss = 1.784 | acc = 0.616 | val_loss = 1.779 | val_acc = 0.615 |\n",
            "Epoch 15: loss = 1.772 | acc = 0.627 | val_loss = 1.769 | val_acc = 0.626 |\n",
            "Epoch 16: loss = 1.763 | acc = 0.637 | val_loss = 1.761 | val_acc = 0.633 |\n",
            "Epoch 17: loss = 1.755 | acc = 0.644 | val_loss = 1.754 | val_acc = 0.64 |\n",
            "Epoch 18: loss = 1.749 | acc = 0.651 | val_loss = 1.748 | val_acc = 0.647 |\n",
            "Epoch 19: loss = 1.743 | acc = 0.658 | val_loss = 1.743 | val_acc = 0.651 |\n",
            "Epoch 20: loss = 1.737 | acc = 0.663 | val_loss = 1.738 | val_acc = 0.658 |\n",
            "Epoch 21: loss = 1.733 | acc = 0.669 | val_loss = 1.733 | val_acc = 0.663 |\n",
            "Epoch 22: loss = 1.728 | acc = 0.673 | val_loss = 1.729 | val_acc = 0.668 |\n",
            "Epoch 23: loss = 1.724 | acc = 0.678 | val_loss = 1.726 | val_acc = 0.671 |\n",
            "Epoch 24: loss = 1.721 | acc = 0.682 | val_loss = 1.722 | val_acc = 0.676 |\n",
            "Epoch 25: loss = 1.717 | acc = 0.686 | val_loss = 1.719 | val_acc = 0.679 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 8~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.033 | acc = 0.112 | val_loss = 1.898 | val_acc = 0.164 |\n",
            "Epoch 2: loss = 1.847 | acc = 0.479 | val_loss = 1.799 | val_acc = 0.64 |\n",
            "Epoch 3: loss = 1.774 | acc = 0.663 | val_loss = 1.766 | val_acc = 0.662 |\n",
            "Epoch 4: loss = 1.751 | acc = 0.674 | val_loss = 1.749 | val_acc = 0.67 |\n",
            "Epoch 5: loss = 1.738 | acc = 0.68 | val_loss = 1.739 | val_acc = 0.671 |\n",
            "Epoch 6: loss = 1.729 | acc = 0.684 | val_loss = 1.732 | val_acc = 0.676 |\n",
            "Epoch 7: loss = 1.722 | acc = 0.687 | val_loss = 1.726 | val_acc = 0.679 |\n",
            "Epoch 8: loss = 1.717 | acc = 0.69 | val_loss = 1.721 | val_acc = 0.678 |\n",
            "Epoch 9: loss = 1.712 | acc = 0.692 | val_loss = 1.717 | val_acc = 0.682 |\n",
            "Epoch 10: loss = 1.708 | acc = 0.694 | val_loss = 1.714 | val_acc = 0.684 |\n",
            "Epoch 11: loss = 1.705 | acc = 0.698 | val_loss = 1.711 | val_acc = 0.689 |\n",
            "Epoch 12: loss = 1.701 | acc = 0.699 | val_loss = 1.708 | val_acc = 0.693 |\n",
            "Epoch 13: loss = 1.699 | acc = 0.702 | val_loss = 1.705 | val_acc = 0.694 |\n",
            "Epoch 14: loss = 1.696 | acc = 0.705 | val_loss = 1.703 | val_acc = 0.698 |\n",
            "Epoch 15: loss = 1.694 | acc = 0.708 | val_loss = 1.701 | val_acc = 0.699 |\n",
            "Epoch 16: loss = 1.692 | acc = 0.711 | val_loss = 1.699 | val_acc = 0.702 |\n",
            "Epoch 17: loss = 1.689 | acc = 0.716 | val_loss = 1.696 | val_acc = 0.712 |\n",
            "Epoch 18: loss = 1.654 | acc = 0.777 | val_loss = 1.648 | val_acc = 0.778 |\n",
            "Epoch 19: loss = 1.633 | acc = 0.793 | val_loss = 1.639 | val_acc = 0.781 |\n",
            "Epoch 20: loss = 1.626 | acc = 0.795 | val_loss = 1.633 | val_acc = 0.782 |\n",
            "Epoch 21: loss = 1.621 | acc = 0.797 | val_loss = 1.629 | val_acc = 0.783 |\n",
            "Epoch 22: loss = 1.617 | acc = 0.798 | val_loss = 1.625 | val_acc = 0.786 |\n",
            "Epoch 23: loss = 1.613 | acc = 0.801 | val_loss = 1.622 | val_acc = 0.792 |\n",
            "Epoch 24: loss = 1.611 | acc = 0.803 | val_loss = 1.62 | val_acc = 0.79 |\n",
            "Epoch 25: loss = 1.608 | acc = 0.805 | val_loss = 1.618 | val_acc = 0.791 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 8~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.693 | acc = 0.713 | val_loss = 1.636 | val_acc = 0.75 |\n",
            "Epoch 2: loss = 1.617 | acc = 0.774 | val_loss = 1.614 | val_acc = 0.779 |\n",
            "Epoch 3: loss = 1.601 | acc = 0.793 | val_loss = 1.604 | val_acc = 0.791 |\n",
            "Epoch 4: loss = 1.591 | acc = 0.807 | val_loss = 1.597 | val_acc = 0.802 |\n",
            "Epoch 5: loss = 1.584 | acc = 0.819 | val_loss = 1.592 | val_acc = 0.812 |\n",
            "Epoch 6: loss = 1.578 | acc = 0.827 | val_loss = 1.588 | val_acc = 0.818 |\n",
            "Epoch 7: loss = 1.573 | acc = 0.834 | val_loss = 1.585 | val_acc = 0.822 |\n",
            "Epoch 8: loss = 1.569 | acc = 0.838 | val_loss = 1.582 | val_acc = 0.828 |\n",
            "Epoch 9: loss = 1.566 | acc = 0.842 | val_loss = 1.579 | val_acc = 0.831 |\n",
            "Epoch 10: loss = 1.563 | acc = 0.846 | val_loss = 1.577 | val_acc = 0.833 |\n",
            "Epoch 11: loss = 1.561 | acc = 0.849 | val_loss = 1.576 | val_acc = 0.833 |\n",
            "Epoch 12: loss = 1.559 | acc = 0.851 | val_loss = 1.575 | val_acc = 0.836 |\n",
            "Epoch 13: loss = 1.557 | acc = 0.852 | val_loss = 1.573 | val_acc = 0.838 |\n",
            "Epoch 14: loss = 1.555 | acc = 0.855 | val_loss = 1.572 | val_acc = 0.841 |\n",
            "Epoch 15: loss = 1.553 | acc = 0.856 | val_loss = 1.572 | val_acc = 0.841 |\n",
            "Epoch 16: loss = 1.552 | acc = 0.857 | val_loss = 1.571 | val_acc = 0.84 |\n",
            "Epoch 17: loss = 1.551 | acc = 0.859 | val_loss = 1.57 | val_acc = 0.842 |\n",
            "Epoch 18: loss = 1.55 | acc = 0.86 | val_loss = 1.568 | val_acc = 0.844 |\n",
            "Epoch 19: loss = 1.548 | acc = 0.861 | val_loss = 1.569 | val_acc = 0.846 |\n",
            "Epoch 20: loss = 1.547 | acc = 0.862 | val_loss = 1.573 | val_acc = 0.837 |\n",
            "Epoch 21: loss = 1.546 | acc = 0.864 | val_loss = 1.567 | val_acc = 0.848 |\n",
            "Epoch 22: loss = 1.545 | acc = 0.866 | val_loss = 1.566 | val_acc = 0.849 |\n",
            "Epoch 23: loss = 1.544 | acc = 0.866 | val_loss = 1.565 | val_acc = 0.85 |\n",
            "Epoch 24: loss = 1.543 | acc = 0.866 | val_loss = 1.565 | val_acc = 0.85 |\n",
            "Epoch 25: loss = 1.542 | acc = 0.867 | val_loss = 1.564 | val_acc = 0.852 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 8~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.629 | acc = 0.733 | val_loss = 1.606 | val_acc = 0.782 |\n",
            "Epoch 2: loss = 1.587 | acc = 0.791 | val_loss = 1.589 | val_acc = 0.797 |\n",
            "Epoch 3: loss = 1.577 | acc = 0.819 | val_loss = 1.591 | val_acc = 0.817 |\n",
            "Epoch 4: loss = 1.568 | acc = 0.832 | val_loss = 1.578 | val_acc = 0.827 |\n",
            "Epoch 5: loss = 1.564 | acc = 0.841 | val_loss = 1.575 | val_acc = 0.834 |\n",
            "Epoch 6: loss = 1.562 | acc = 0.845 | val_loss = 1.574 | val_acc = 0.839 |\n",
            "Epoch 7: loss = 1.559 | acc = 0.849 | val_loss = 1.567 | val_acc = 0.84 |\n",
            "Epoch 8: loss = 1.557 | acc = 0.852 | val_loss = 1.568 | val_acc = 0.845 |\n",
            "Epoch 9: loss = 1.555 | acc = 0.855 | val_loss = 1.567 | val_acc = 0.843 |\n",
            "Epoch 10: loss = 1.553 | acc = 0.856 | val_loss = 1.578 | val_acc = 0.827 |\n",
            "Epoch 11: loss = 1.554 | acc = 0.855 | val_loss = 1.565 | val_acc = 0.841 |\n",
            "Epoch 12: loss = 1.553 | acc = 0.855 | val_loss = 1.567 | val_acc = 0.835 |\n",
            "Epoch 13: loss = 1.55 | acc = 0.859 | val_loss = 1.564 | val_acc = 0.848 |\n",
            "Epoch 14: loss = 1.552 | acc = 0.858 | val_loss = 1.567 | val_acc = 0.84 |\n",
            "Epoch 15: loss = 1.554 | acc = 0.854 | val_loss = 1.567 | val_acc = 0.843 |\n",
            "Epoch 16: loss = 1.552 | acc = 0.859 | val_loss = 1.565 | val_acc = 0.842 |\n",
            "Epoch 17: loss = 1.549 | acc = 0.862 | val_loss = 1.564 | val_acc = 0.852 |\n",
            "Epoch 18: loss = 1.549 | acc = 0.86 | val_loss = 1.564 | val_acc = 0.839 |\n",
            "Epoch 19: loss = 1.548 | acc = 0.861 | val_loss = 1.565 | val_acc = 0.832 |\n",
            "Epoch 20: loss = 1.548 | acc = 0.862 | val_loss = 1.564 | val_acc = 0.841 |\n",
            "Epoch 21: loss = 1.549 | acc = 0.859 | val_loss = 1.572 | val_acc = 0.831 |\n",
            "Epoch 22: loss = 1.549 | acc = 0.86 | val_loss = 1.565 | val_acc = 0.846 |\n",
            "Epoch 23: loss = 1.549 | acc = 0.861 | val_loss = 1.572 | val_acc = 0.838 |\n",
            "Epoch 24: loss = 1.547 | acc = 0.863 | val_loss = 1.567 | val_acc = 0.839 |\n",
            "Epoch 25: loss = 1.547 | acc = 0.863 | val_loss = 1.565 | val_acc = 0.844 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 8~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.247 | acc = 0.14 | val_loss = 2.131 | val_acc = 0.182 |\n",
            "Epoch 2: loss = 2.138 | acc = 0.12 | val_loss = 2.166 | val_acc = 0.191 |\n",
            "Epoch 3: loss = 2.134 | acc = 0.138 | val_loss = 2.133 | val_acc = 0.11 |\n",
            "Epoch 4: loss = 2.131 | acc = 0.16 | val_loss = 2.121 | val_acc = 0.124 |\n",
            "Epoch 5: loss = 2.128 | acc = 0.176 | val_loss = 2.119 | val_acc = 0.153 |\n",
            "Epoch 6: loss = 2.126 | acc = 0.152 | val_loss = 2.123 | val_acc = 0.168 |\n",
            "Epoch 7: loss = 2.127 | acc = 0.168 | val_loss = 2.129 | val_acc = 0.185 |\n",
            "Epoch 8: loss = 2.131 | acc = 0.158 | val_loss = 2.126 | val_acc = 0.177 |\n",
            "Epoch 9: loss = 2.13 | acc = 0.165 | val_loss = 2.129 | val_acc = 0.192 |\n",
            "Epoch 10: loss = 2.126 | acc = 0.149 | val_loss = 2.12 | val_acc = 0.102 |\n",
            "Epoch 11: loss = 2.141 | acc = 0.157 | val_loss = 2.128 | val_acc = 0.149 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-a2556287c5b3>:101: RuntimeWarning: overflow encountered in exp\n",
            "  return 1./(1. + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: loss = 2.151 | acc = 0.189 | val_loss = 2.149 | val_acc = 0.161 |\n",
            "Epoch 13: loss = 2.142 | acc = 0.142 | val_loss = 2.17 | val_acc = 0.163 |\n",
            "Epoch 14: loss = 2.141 | acc = 0.152 | val_loss = 2.157 | val_acc = 0.124 |\n",
            "Epoch 15: loss = 2.145 | acc = 0.15 | val_loss = 2.174 | val_acc = 0.167 |\n",
            "Epoch 16: loss = 2.142 | acc = 0.144 | val_loss = 2.145 | val_acc = 0.153 |\n",
            "Epoch 17: loss = 2.15 | acc = 0.157 | val_loss = 2.203 | val_acc = 0.282 |\n",
            "Epoch 18: loss = 2.149 | acc = 0.155 | val_loss = 2.134 | val_acc = 0.109 |\n",
            "Epoch 19: loss = 2.15 | acc = 0.137 | val_loss = 2.174 | val_acc = 0.153 |\n",
            "Epoch 20: loss = 2.164 | acc = 0.146 | val_loss = 2.136 | val_acc = 0.188 |\n",
            "Epoch 21: loss = 2.167 | acc = 0.15 | val_loss = 2.173 | val_acc = 0.131 |\n",
            "Epoch 22: loss = 2.169 | acc = 0.152 | val_loss = 2.189 | val_acc = 0.111 |\n",
            "Epoch 23: loss = 2.167 | acc = 0.141 | val_loss = 2.134 | val_acc = 0.174 |\n",
            "Epoch 24: loss = 2.149 | acc = 0.13 | val_loss = 2.13 | val_acc = 0.113 |\n",
            "Epoch 25: loss = 2.149 | acc = 0.105 | val_loss = 2.136 | val_acc = 0.1 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.401 | acc = 0.095 | val_loss = 2.386 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.37 | acc = 0.099 | val_loss = 2.353 | val_acc = 0.104 |\n",
            "Epoch 3: loss = 2.341 | acc = 0.101 | val_loss = 2.329 | val_acc = 0.105 |\n",
            "Epoch 4: loss = 2.32 | acc = 0.103 | val_loss = 2.31 | val_acc = 0.105 |\n",
            "Epoch 5: loss = 2.304 | acc = 0.103 | val_loss = 2.295 | val_acc = 0.104 |\n",
            "Epoch 6: loss = 2.288 | acc = 0.103 | val_loss = 2.276 | val_acc = 0.103 |\n",
            "Epoch 7: loss = 2.257 | acc = 0.102 | val_loss = 2.233 | val_acc = 0.101 |\n",
            "Epoch 8: loss = 2.221 | acc = 0.101 | val_loss = 2.206 | val_acc = 0.102 |\n",
            "Epoch 9: loss = 2.197 | acc = 0.102 | val_loss = 2.186 | val_acc = 0.102 |\n",
            "Epoch 10: loss = 2.18 | acc = 0.102 | val_loss = 2.172 | val_acc = 0.103 |\n",
            "Epoch 11: loss = 2.166 | acc = 0.103 | val_loss = 2.16 | val_acc = 0.105 |\n",
            "Epoch 12: loss = 2.155 | acc = 0.105 | val_loss = 2.15 | val_acc = 0.106 |\n",
            "Epoch 13: loss = 2.146 | acc = 0.107 | val_loss = 2.142 | val_acc = 0.108 |\n",
            "Epoch 14: loss = 2.138 | acc = 0.109 | val_loss = 2.135 | val_acc = 0.11 |\n",
            "Epoch 15: loss = 2.131 | acc = 0.112 | val_loss = 2.128 | val_acc = 0.114 |\n",
            "Epoch 16: loss = 2.124 | acc = 0.116 | val_loss = 2.122 | val_acc = 0.119 |\n",
            "Epoch 17: loss = 2.117 | acc = 0.121 | val_loss = 2.115 | val_acc = 0.126 |\n",
            "Epoch 18: loss = 2.108 | acc = 0.128 | val_loss = 2.103 | val_acc = 0.134 |\n",
            "Epoch 19: loss = 2.093 | acc = 0.136 | val_loss = 2.088 | val_acc = 0.142 |\n",
            "Epoch 20: loss = 2.081 | acc = 0.148 | val_loss = 2.081 | val_acc = 0.156 |\n",
            "Epoch 21: loss = 2.074 | acc = 0.164 | val_loss = 2.074 | val_acc = 0.175 |\n",
            "Epoch 22: loss = 2.068 | acc = 0.188 | val_loss = 2.068 | val_acc = 0.202 |\n",
            "Epoch 23: loss = 2.062 | acc = 0.225 | val_loss = 2.061 | val_acc = 0.25 |\n",
            "Epoch 24: loss = 2.052 | acc = 0.292 | val_loss = 2.048 | val_acc = 0.338 |\n",
            "Epoch 25: loss = 2.033 | acc = 0.392 | val_loss = 2.024 | val_acc = 0.428 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.142 | acc = 0.226 | val_loss = 1.949 | val_acc = 0.527 |\n",
            "Epoch 2: loss = 1.879 | acc = 0.596 | val_loss = 1.84 | val_acc = 0.619 |\n",
            "Epoch 3: loss = 1.813 | acc = 0.635 | val_loss = 1.8 | val_acc = 0.636 |\n",
            "Epoch 4: loss = 1.783 | acc = 0.646 | val_loss = 1.777 | val_acc = 0.642 |\n",
            "Epoch 5: loss = 1.765 | acc = 0.652 | val_loss = 1.763 | val_acc = 0.648 |\n",
            "Epoch 6: loss = 1.753 | acc = 0.655 | val_loss = 1.753 | val_acc = 0.651 |\n",
            "Epoch 7: loss = 1.744 | acc = 0.659 | val_loss = 1.745 | val_acc = 0.655 |\n",
            "Epoch 8: loss = 1.723 | acc = 0.685 | val_loss = 1.705 | val_acc = 0.707 |\n",
            "Epoch 9: loss = 1.69 | acc = 0.721 | val_loss = 1.689 | val_acc = 0.718 |\n",
            "Epoch 10: loss = 1.678 | acc = 0.728 | val_loss = 1.679 | val_acc = 0.723 |\n",
            "Epoch 11: loss = 1.67 | acc = 0.734 | val_loss = 1.671 | val_acc = 0.726 |\n",
            "Epoch 12: loss = 1.663 | acc = 0.737 | val_loss = 1.665 | val_acc = 0.731 |\n",
            "Epoch 13: loss = 1.658 | acc = 0.74 | val_loss = 1.661 | val_acc = 0.731 |\n",
            "Epoch 14: loss = 1.653 | acc = 0.742 | val_loss = 1.656 | val_acc = 0.734 |\n",
            "Epoch 15: loss = 1.649 | acc = 0.744 | val_loss = 1.653 | val_acc = 0.736 |\n",
            "Epoch 16: loss = 1.646 | acc = 0.746 | val_loss = 1.65 | val_acc = 0.736 |\n",
            "Epoch 17: loss = 1.643 | acc = 0.747 | val_loss = 1.647 | val_acc = 0.738 |\n",
            "Epoch 18: loss = 1.64 | acc = 0.749 | val_loss = 1.644 | val_acc = 0.74 |\n",
            "Epoch 19: loss = 1.637 | acc = 0.751 | val_loss = 1.642 | val_acc = 0.741 |\n",
            "Epoch 20: loss = 1.635 | acc = 0.752 | val_loss = 1.639 | val_acc = 0.741 |\n",
            "Epoch 21: loss = 1.633 | acc = 0.753 | val_loss = 1.637 | val_acc = 0.744 |\n",
            "Epoch 22: loss = 1.631 | acc = 0.754 | val_loss = 1.635 | val_acc = 0.745 |\n",
            "Epoch 23: loss = 1.629 | acc = 0.755 | val_loss = 1.633 | val_acc = 0.746 |\n",
            "Epoch 24: loss = 1.627 | acc = 0.756 | val_loss = 1.632 | val_acc = 0.748 |\n",
            "Epoch 25: loss = 1.625 | acc = 0.757 | val_loss = 1.63 | val_acc = 0.75 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.802 | acc = 0.622 | val_loss = 1.732 | val_acc = 0.658 |\n",
            "Epoch 2: loss = 1.712 | acc = 0.678 | val_loss = 1.709 | val_acc = 0.67 |\n",
            "Epoch 3: loss = 1.695 | acc = 0.686 | val_loss = 1.697 | val_acc = 0.677 |\n",
            "Epoch 4: loss = 1.685 | acc = 0.695 | val_loss = 1.69 | val_acc = 0.685 |\n",
            "Epoch 5: loss = 1.679 | acc = 0.703 | val_loss = 1.685 | val_acc = 0.693 |\n",
            "Epoch 6: loss = 1.674 | acc = 0.711 | val_loss = 1.681 | val_acc = 0.7 |\n",
            "Epoch 7: loss = 1.67 | acc = 0.717 | val_loss = 1.678 | val_acc = 0.706 |\n",
            "Epoch 8: loss = 1.667 | acc = 0.728 | val_loss = 1.676 | val_acc = 0.715 |\n",
            "Epoch 9: loss = 1.65 | acc = 0.76 | val_loss = 1.614 | val_acc = 0.808 |\n",
            "Epoch 10: loss = 1.595 | acc = 0.825 | val_loss = 1.6 | val_acc = 0.815 |\n",
            "Epoch 11: loss = 1.586 | acc = 0.831 | val_loss = 1.595 | val_acc = 0.816 |\n",
            "Epoch 12: loss = 1.581 | acc = 0.833 | val_loss = 1.591 | val_acc = 0.82 |\n",
            "Epoch 13: loss = 1.577 | acc = 0.836 | val_loss = 1.588 | val_acc = 0.823 |\n",
            "Epoch 14: loss = 1.574 | acc = 0.839 | val_loss = 1.586 | val_acc = 0.827 |\n",
            "Epoch 15: loss = 1.571 | acc = 0.84 | val_loss = 1.583 | val_acc = 0.828 |\n",
            "Epoch 16: loss = 1.569 | acc = 0.841 | val_loss = 1.583 | val_acc = 0.828 |\n",
            "Epoch 17: loss = 1.567 | acc = 0.844 | val_loss = 1.581 | val_acc = 0.831 |\n",
            "Epoch 18: loss = 1.566 | acc = 0.846 | val_loss = 1.58 | val_acc = 0.832 |\n",
            "Epoch 19: loss = 1.564 | acc = 0.847 | val_loss = 1.578 | val_acc = 0.835 |\n",
            "Epoch 20: loss = 1.562 | acc = 0.848 | val_loss = 1.577 | val_acc = 0.835 |\n",
            "Epoch 21: loss = 1.561 | acc = 0.849 | val_loss = 1.576 | val_acc = 0.837 |\n",
            "Epoch 22: loss = 1.56 | acc = 0.851 | val_loss = 1.575 | val_acc = 0.836 |\n",
            "Epoch 23: loss = 1.559 | acc = 0.852 | val_loss = 1.574 | val_acc = 0.838 |\n",
            "Epoch 24: loss = 1.558 | acc = 0.853 | val_loss = 1.574 | val_acc = 0.839 |\n",
            "Epoch 25: loss = 1.557 | acc = 0.854 | val_loss = 1.574 | val_acc = 0.838 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.719 | acc = 0.647 | val_loss = 1.68 | val_acc = 0.699 |\n",
            "Epoch 2: loss = 1.664 | acc = 0.731 | val_loss = 1.664 | val_acc = 0.731 |\n",
            "Epoch 3: loss = 1.651 | acc = 0.746 | val_loss = 1.66 | val_acc = 0.744 |\n",
            "Epoch 4: loss = 1.644 | acc = 0.755 | val_loss = 1.651 | val_acc = 0.752 |\n",
            "Epoch 5: loss = 1.638 | acc = 0.761 | val_loss = 1.653 | val_acc = 0.748 |\n",
            "Epoch 6: loss = 1.634 | acc = 0.765 | val_loss = 1.647 | val_acc = 0.754 |\n",
            "Epoch 7: loss = 1.631 | acc = 0.768 | val_loss = 1.646 | val_acc = 0.756 |\n",
            "Epoch 8: loss = 1.628 | acc = 0.773 | val_loss = 1.649 | val_acc = 0.748 |\n",
            "Epoch 9: loss = 1.626 | acc = 0.775 | val_loss = 1.641 | val_acc = 0.76 |\n",
            "Epoch 10: loss = 1.624 | acc = 0.777 | val_loss = 1.647 | val_acc = 0.754 |\n",
            "Epoch 11: loss = 1.622 | acc = 0.778 | val_loss = 1.642 | val_acc = 0.766 |\n",
            "Epoch 12: loss = 1.621 | acc = 0.78 | val_loss = 1.639 | val_acc = 0.76 |\n",
            "Epoch 13: loss = 1.619 | acc = 0.782 | val_loss = 1.64 | val_acc = 0.757 |\n",
            "Epoch 14: loss = 1.618 | acc = 0.784 | val_loss = 1.637 | val_acc = 0.767 |\n",
            "Epoch 15: loss = 1.617 | acc = 0.786 | val_loss = 1.637 | val_acc = 0.765 |\n",
            "Epoch 16: loss = 1.616 | acc = 0.787 | val_loss = 1.639 | val_acc = 0.762 |\n",
            "Epoch 17: loss = 1.615 | acc = 0.788 | val_loss = 1.636 | val_acc = 0.768 |\n",
            "Epoch 18: loss = 1.613 | acc = 0.79 | val_loss = 1.637 | val_acc = 0.766 |\n",
            "Epoch 19: loss = 1.613 | acc = 0.791 | val_loss = 1.635 | val_acc = 0.772 |\n",
            "Epoch 20: loss = 1.612 | acc = 0.793 | val_loss = 1.634 | val_acc = 0.771 |\n",
            "Epoch 21: loss = 1.611 | acc = 0.792 | val_loss = 1.634 | val_acc = 0.767 |\n",
            "Epoch 22: loss = 1.61 | acc = 0.794 | val_loss = 1.633 | val_acc = 0.775 |\n",
            "Epoch 23: loss = 1.61 | acc = 0.796 | val_loss = 1.636 | val_acc = 0.766 |\n",
            "Epoch 24: loss = 1.609 | acc = 0.797 | val_loss = 1.635 | val_acc = 0.773 |\n",
            "Epoch 25: loss = 1.608 | acc = 0.799 | val_loss = 1.634 | val_acc = 0.775 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.192 | acc = 0.234 | val_loss = 2.146 | val_acc = 0.28 |\n",
            "Epoch 2: loss = 2.149 | acc = 0.266 | val_loss = 2.149 | val_acc = 0.27 |\n",
            "Epoch 3: loss = 2.125 | acc = 0.294 | val_loss = 2.08 | val_acc = 0.298 |\n",
            "Epoch 4: loss = 2.079 | acc = 0.343 | val_loss = 2.074 | val_acc = 0.376 |\n",
            "Epoch 5: loss = 2.078 | acc = 0.371 | val_loss = 2.069 | val_acc = 0.385 |\n",
            "Epoch 6: loss = 2.075 | acc = 0.393 | val_loss = 2.074 | val_acc = 0.412 |\n",
            "Epoch 7: loss = 2.076 | acc = 0.37 | val_loss = 2.09 | val_acc = 0.379 |\n",
            "Epoch 8: loss = 2.075 | acc = 0.375 | val_loss = 2.073 | val_acc = 0.382 |\n",
            "Epoch 9: loss = 2.05 | acc = 0.39 | val_loss = 2.044 | val_acc = 0.384 |\n",
            "Epoch 10: loss = 2.066 | acc = 0.361 | val_loss = 2.032 | val_acc = 0.386 |\n",
            "Epoch 11: loss = 2.029 | acc = 0.404 | val_loss = 2.017 | val_acc = 0.431 |\n",
            "Epoch 12: loss = 2.025 | acc = 0.44 | val_loss = 2.032 | val_acc = 0.438 |\n",
            "Epoch 13: loss = 2.004 | acc = 0.46 | val_loss = 1.961 | val_acc = 0.471 |\n",
            "Epoch 14: loss = 1.965 | acc = 0.47 | val_loss = 1.958 | val_acc = 0.47 |\n",
            "Epoch 15: loss = 1.964 | acc = 0.469 | val_loss = 1.968 | val_acc = 0.473 |\n",
            "Epoch 16: loss = 1.964 | acc = 0.485 | val_loss = 1.988 | val_acc = 0.526 |\n",
            "Epoch 17: loss = 1.918 | acc = 0.535 | val_loss = 1.891 | val_acc = 0.556 |\n",
            "Epoch 18: loss = 1.888 | acc = 0.558 | val_loss = 1.899 | val_acc = 0.554 |\n",
            "Epoch 19: loss = 1.884 | acc = 0.56 | val_loss = 1.882 | val_acc = 0.569 |\n",
            "Epoch 20: loss = 1.885 | acc = 0.559 | val_loss = 1.889 | val_acc = 0.565 |\n",
            "Epoch 21: loss = 1.891 | acc = 0.549 | val_loss = 1.944 | val_acc = 0.554 |\n",
            "Epoch 22: loss = 1.897 | acc = 0.547 | val_loss = 1.915 | val_acc = 0.489 |\n",
            "Epoch 23: loss = 1.902 | acc = 0.539 | val_loss = 1.893 | val_acc = 0.488 |\n",
            "Epoch 24: loss = 1.907 | acc = 0.487 | val_loss = 1.919 | val_acc = 0.164 |\n",
            "Epoch 25: loss = 1.93 | acc = 0.19 | val_loss = 1.944 | val_acc = 0.199 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.337 | acc = 0.127 | val_loss = 2.31 | val_acc = 0.129 |\n",
            "Epoch 2: loss = 2.289 | acc = 0.128 | val_loss = 2.27 | val_acc = 0.134 |\n",
            "Epoch 3: loss = 2.254 | acc = 0.138 | val_loss = 2.238 | val_acc = 0.148 |\n",
            "Epoch 4: loss = 2.223 | acc = 0.155 | val_loss = 2.209 | val_acc = 0.168 |\n",
            "Epoch 5: loss = 2.195 | acc = 0.179 | val_loss = 2.182 | val_acc = 0.199 |\n",
            "Epoch 6: loss = 2.167 | acc = 0.215 | val_loss = 2.152 | val_acc = 0.236 |\n",
            "Epoch 7: loss = 2.134 | acc = 0.256 | val_loss = 2.118 | val_acc = 0.277 |\n",
            "Epoch 8: loss = 2.103 | acc = 0.299 | val_loss = 2.089 | val_acc = 0.334 |\n",
            "Epoch 9: loss = 2.076 | acc = 0.36 | val_loss = 2.063 | val_acc = 0.392 |\n",
            "Epoch 10: loss = 2.052 | acc = 0.413 | val_loss = 2.041 | val_acc = 0.438 |\n",
            "Epoch 11: loss = 2.032 | acc = 0.453 | val_loss = 2.023 | val_acc = 0.468 |\n",
            "Epoch 12: loss = 2.014 | acc = 0.481 | val_loss = 2.006 | val_acc = 0.494 |\n",
            "Epoch 13: loss = 1.998 | acc = 0.503 | val_loss = 1.992 | val_acc = 0.513 |\n",
            "Epoch 14: loss = 1.984 | acc = 0.521 | val_loss = 1.979 | val_acc = 0.529 |\n",
            "Epoch 15: loss = 1.972 | acc = 0.536 | val_loss = 1.967 | val_acc = 0.543 |\n",
            "Epoch 16: loss = 1.96 | acc = 0.55 | val_loss = 1.955 | val_acc = 0.556 |\n",
            "Epoch 17: loss = 1.949 | acc = 0.562 | val_loss = 1.944 | val_acc = 0.566 |\n",
            "Epoch 18: loss = 1.937 | acc = 0.574 | val_loss = 1.932 | val_acc = 0.578 |\n",
            "Epoch 19: loss = 1.925 | acc = 0.588 | val_loss = 1.92 | val_acc = 0.59 |\n",
            "Epoch 20: loss = 1.914 | acc = 0.599 | val_loss = 1.911 | val_acc = 0.601 |\n",
            "Epoch 21: loss = 1.905 | acc = 0.606 | val_loss = 1.902 | val_acc = 0.609 |\n",
            "Epoch 22: loss = 1.897 | acc = 0.613 | val_loss = 1.895 | val_acc = 0.615 |\n",
            "Epoch 23: loss = 1.89 | acc = 0.619 | val_loss = 1.888 | val_acc = 0.619 |\n",
            "Epoch 24: loss = 1.883 | acc = 0.624 | val_loss = 1.882 | val_acc = 0.624 |\n",
            "Epoch 25: loss = 1.877 | acc = 0.629 | val_loss = 1.876 | val_acc = 0.629 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.225 | acc = 0.2 | val_loss = 2.116 | val_acc = 0.305 |\n",
            "Epoch 2: loss = 2.059 | acc = 0.402 | val_loss = 2.025 | val_acc = 0.462 |\n",
            "Epoch 3: loss = 2.0 | acc = 0.491 | val_loss = 1.987 | val_acc = 0.499 |\n",
            "Epoch 4: loss = 1.971 | acc = 0.517 | val_loss = 1.965 | val_acc = 0.515 |\n",
            "Epoch 5: loss = 1.953 | acc = 0.531 | val_loss = 1.95 | val_acc = 0.53 |\n",
            "Epoch 6: loss = 1.94 | acc = 0.54 | val_loss = 1.939 | val_acc = 0.54 |\n",
            "Epoch 7: loss = 1.93 | acc = 0.549 | val_loss = 1.93 | val_acc = 0.546 |\n",
            "Epoch 8: loss = 1.922 | acc = 0.555 | val_loss = 1.923 | val_acc = 0.553 |\n",
            "Epoch 9: loss = 1.915 | acc = 0.56 | val_loss = 1.916 | val_acc = 0.557 |\n",
            "Epoch 10: loss = 1.908 | acc = 0.564 | val_loss = 1.911 | val_acc = 0.559 |\n",
            "Epoch 11: loss = 1.903 | acc = 0.568 | val_loss = 1.906 | val_acc = 0.563 |\n",
            "Epoch 12: loss = 1.898 | acc = 0.571 | val_loss = 1.901 | val_acc = 0.565 |\n",
            "Epoch 13: loss = 1.894 | acc = 0.573 | val_loss = 1.897 | val_acc = 0.57 |\n",
            "Epoch 14: loss = 1.89 | acc = 0.576 | val_loss = 1.893 | val_acc = 0.57 |\n",
            "Epoch 15: loss = 1.886 | acc = 0.578 | val_loss = 1.887 | val_acc = 0.573 |\n",
            "Epoch 16: loss = 1.861 | acc = 0.626 | val_loss = 1.844 | val_acc = 0.648 |\n",
            "Epoch 17: loss = 1.83 | acc = 0.656 | val_loss = 1.829 | val_acc = 0.652 |\n",
            "Epoch 18: loss = 1.819 | acc = 0.661 | val_loss = 1.821 | val_acc = 0.652 |\n",
            "Epoch 19: loss = 1.813 | acc = 0.663 | val_loss = 1.815 | val_acc = 0.656 |\n",
            "Epoch 20: loss = 1.808 | acc = 0.665 | val_loss = 1.811 | val_acc = 0.657 |\n",
            "Epoch 21: loss = 1.803 | acc = 0.666 | val_loss = 1.807 | val_acc = 0.658 |\n",
            "Epoch 22: loss = 1.8 | acc = 0.667 | val_loss = 1.804 | val_acc = 0.662 |\n",
            "Epoch 23: loss = 1.797 | acc = 0.669 | val_loss = 1.801 | val_acc = 0.663 |\n",
            "Epoch 24: loss = 1.794 | acc = 0.67 | val_loss = 1.798 | val_acc = 0.665 |\n",
            "Epoch 25: loss = 1.791 | acc = 0.671 | val_loss = 1.796 | val_acc = 0.666 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.864 | acc = 0.533 | val_loss = 1.728 | val_acc = 0.613 |\n",
            "Epoch 2: loss = 1.698 | acc = 0.653 | val_loss = 1.686 | val_acc = 0.664 |\n",
            "Epoch 3: loss = 1.669 | acc = 0.695 | val_loss = 1.666 | val_acc = 0.708 |\n",
            "Epoch 4: loss = 1.649 | acc = 0.736 | val_loss = 1.647 | val_acc = 0.736 |\n",
            "Epoch 5: loss = 1.633 | acc = 0.751 | val_loss = 1.636 | val_acc = 0.745 |\n",
            "Epoch 6: loss = 1.624 | acc = 0.761 | val_loss = 1.629 | val_acc = 0.756 |\n",
            "Epoch 7: loss = 1.617 | acc = 0.768 | val_loss = 1.623 | val_acc = 0.763 |\n",
            "Epoch 8: loss = 1.612 | acc = 0.775 | val_loss = 1.619 | val_acc = 0.768 |\n",
            "Epoch 9: loss = 1.608 | acc = 0.78 | val_loss = 1.615 | val_acc = 0.77 |\n",
            "Epoch 10: loss = 1.604 | acc = 0.785 | val_loss = 1.612 | val_acc = 0.778 |\n",
            "Epoch 11: loss = 1.601 | acc = 0.79 | val_loss = 1.609 | val_acc = 0.781 |\n",
            "Epoch 12: loss = 1.598 | acc = 0.795 | val_loss = 1.607 | val_acc = 0.787 |\n",
            "Epoch 13: loss = 1.595 | acc = 0.799 | val_loss = 1.605 | val_acc = 0.794 |\n",
            "Epoch 14: loss = 1.593 | acc = 0.804 | val_loss = 1.603 | val_acc = 0.798 |\n",
            "Epoch 15: loss = 1.591 | acc = 0.81 | val_loss = 1.601 | val_acc = 0.801 |\n",
            "Epoch 16: loss = 1.589 | acc = 0.815 | val_loss = 1.599 | val_acc = 0.806 |\n",
            "Epoch 17: loss = 1.587 | acc = 0.82 | val_loss = 1.597 | val_acc = 0.809 |\n",
            "Epoch 18: loss = 1.585 | acc = 0.823 | val_loss = 1.595 | val_acc = 0.812 |\n",
            "Epoch 19: loss = 1.583 | acc = 0.827 | val_loss = 1.594 | val_acc = 0.814 |\n",
            "Epoch 20: loss = 1.581 | acc = 0.83 | val_loss = 1.592 | val_acc = 0.816 |\n",
            "Epoch 21: loss = 1.579 | acc = 0.832 | val_loss = 1.591 | val_acc = 0.817 |\n",
            "Epoch 22: loss = 1.577 | acc = 0.834 | val_loss = 1.59 | val_acc = 0.818 |\n",
            "Epoch 23: loss = 1.576 | acc = 0.834 | val_loss = 1.588 | val_acc = 0.822 |\n",
            "Epoch 24: loss = 1.574 | acc = 0.837 | val_loss = 1.587 | val_acc = 0.824 |\n",
            "Epoch 25: loss = 1.573 | acc = 0.837 | val_loss = 1.585 | val_acc = 0.825 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.653 | acc = 0.742 | val_loss = 1.614 | val_acc = 0.767 |\n",
            "Epoch 2: loss = 1.595 | acc = 0.803 | val_loss = 1.598 | val_acc = 0.805 |\n",
            "Epoch 3: loss = 1.58 | acc = 0.829 | val_loss = 1.59 | val_acc = 0.814 |\n",
            "Epoch 4: loss = 1.571 | acc = 0.839 | val_loss = 1.583 | val_acc = 0.828 |\n",
            "Epoch 5: loss = 1.565 | acc = 0.847 | val_loss = 1.577 | val_acc = 0.83 |\n",
            "Epoch 6: loss = 1.56 | acc = 0.853 | val_loss = 1.573 | val_acc = 0.835 |\n",
            "Epoch 7: loss = 1.556 | acc = 0.856 | val_loss = 1.57 | val_acc = 0.845 |\n",
            "Epoch 8: loss = 1.553 | acc = 0.859 | val_loss = 1.566 | val_acc = 0.844 |\n",
            "Epoch 9: loss = 1.55 | acc = 0.862 | val_loss = 1.569 | val_acc = 0.842 |\n",
            "Epoch 10: loss = 1.548 | acc = 0.865 | val_loss = 1.563 | val_acc = 0.849 |\n",
            "Epoch 11: loss = 1.546 | acc = 0.867 | val_loss = 1.564 | val_acc = 0.85 |\n",
            "Epoch 12: loss = 1.544 | acc = 0.87 | val_loss = 1.564 | val_acc = 0.853 |\n",
            "Epoch 13: loss = 1.542 | acc = 0.87 | val_loss = 1.562 | val_acc = 0.854 |\n",
            "Epoch 14: loss = 1.541 | acc = 0.873 | val_loss = 1.559 | val_acc = 0.854 |\n",
            "Epoch 15: loss = 1.539 | acc = 0.874 | val_loss = 1.561 | val_acc = 0.853 |\n",
            "Epoch 16: loss = 1.538 | acc = 0.876 | val_loss = 1.564 | val_acc = 0.85 |\n",
            "Epoch 17: loss = 1.537 | acc = 0.877 | val_loss = 1.558 | val_acc = 0.859 |\n",
            "Epoch 18: loss = 1.536 | acc = 0.879 | val_loss = 1.559 | val_acc = 0.86 |\n",
            "Epoch 19: loss = 1.534 | acc = 0.88 | val_loss = 1.56 | val_acc = 0.855 |\n",
            "Epoch 20: loss = 1.533 | acc = 0.881 | val_loss = 1.558 | val_acc = 0.861 |\n",
            "Epoch 21: loss = 1.532 | acc = 0.882 | val_loss = 1.558 | val_acc = 0.86 |\n",
            "Epoch 22: loss = 1.531 | acc = 0.883 | val_loss = 1.555 | val_acc = 0.865 |\n",
            "Epoch 23: loss = 1.53 | acc = 0.885 | val_loss = 1.557 | val_acc = 0.863 |\n",
            "Epoch 24: loss = 1.529 | acc = 0.886 | val_loss = 1.561 | val_acc = 0.863 |\n",
            "Epoch 25: loss = 1.528 | acc = 0.888 | val_loss = 1.557 | val_acc = 0.866 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.893 | acc = 0.342 | val_loss = 1.723 | val_acc = 0.58 |\n",
            "Epoch 2: loss = 1.717 | acc = 0.599 | val_loss = 1.716 | val_acc = 0.565 |\n",
            "Epoch 3: loss = 1.711 | acc = 0.603 | val_loss = 1.704 | val_acc = 0.665 |\n",
            "Epoch 4: loss = 1.708 | acc = 0.632 | val_loss = 1.709 | val_acc = 0.532 |\n",
            "Epoch 5: loss = 1.707 | acc = 0.642 | val_loss = 1.722 | val_acc = 0.621 |\n",
            "Epoch 6: loss = 1.704 | acc = 0.625 | val_loss = 1.703 | val_acc = 0.628 |\n",
            "Epoch 7: loss = 1.701 | acc = 0.619 | val_loss = 1.71 | val_acc = 0.646 |\n",
            "Epoch 8: loss = 1.642 | acc = 0.654 | val_loss = 1.636 | val_acc = 0.658 |\n",
            "Epoch 9: loss = 1.622 | acc = 0.689 | val_loss = 1.618 | val_acc = 0.728 |\n",
            "Epoch 10: loss = 1.602 | acc = 0.753 | val_loss = 1.605 | val_acc = 0.701 |\n",
            "Epoch 11: loss = 1.599 | acc = 0.766 | val_loss = 1.604 | val_acc = 0.734 |\n",
            "Epoch 12: loss = 1.596 | acc = 0.774 | val_loss = 1.596 | val_acc = 0.773 |\n",
            "Epoch 13: loss = 1.595 | acc = 0.783 | val_loss = 1.595 | val_acc = 0.786 |\n",
            "Epoch 14: loss = 1.597 | acc = 0.766 | val_loss = 1.604 | val_acc = 0.743 |\n",
            "Epoch 15: loss = 1.596 | acc = 0.767 | val_loss = 1.595 | val_acc = 0.776 |\n",
            "Epoch 16: loss = 1.597 | acc = 0.769 | val_loss = 1.622 | val_acc = 0.748 |\n",
            "Epoch 17: loss = 1.601 | acc = 0.756 | val_loss = 1.601 | val_acc = 0.779 |\n",
            "Epoch 18: loss = 1.604 | acc = 0.755 | val_loss = 1.622 | val_acc = 0.723 |\n",
            "Epoch 19: loss = 1.602 | acc = 0.76 | val_loss = 1.616 | val_acc = 0.777 |\n",
            "Epoch 20: loss = 1.599 | acc = 0.762 | val_loss = 1.599 | val_acc = 0.758 |\n",
            "Epoch 21: loss = 1.599 | acc = 0.757 | val_loss = 1.596 | val_acc = 0.743 |\n",
            "Epoch 22: loss = 1.6 | acc = 0.754 | val_loss = 1.606 | val_acc = 0.769 |\n",
            "Epoch 23: loss = 1.6 | acc = 0.764 | val_loss = 1.607 | val_acc = 0.783 |\n",
            "Epoch 24: loss = 1.607 | acc = 0.759 | val_loss = 1.618 | val_acc = 0.752 |\n",
            "Epoch 25: loss = 1.607 | acc = 0.761 | val_loss = 1.615 | val_acc = 0.775 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.115 | val_loss = 2.299 | val_acc = 0.114 |\n",
            "Epoch 2: loss = 2.284 | acc = 0.114 | val_loss = 2.275 | val_acc = 0.113 |\n",
            "Epoch 3: loss = 2.264 | acc = 0.113 | val_loss = 2.259 | val_acc = 0.113 |\n",
            "Epoch 4: loss = 2.25 | acc = 0.113 | val_loss = 2.247 | val_acc = 0.112 |\n",
            "Epoch 5: loss = 2.239 | acc = 0.113 | val_loss = 2.236 | val_acc = 0.112 |\n",
            "Epoch 6: loss = 2.228 | acc = 0.112 | val_loss = 2.226 | val_acc = 0.112 |\n",
            "Epoch 7: loss = 2.217 | acc = 0.112 | val_loss = 2.216 | val_acc = 0.112 |\n",
            "Epoch 8: loss = 2.207 | acc = 0.112 | val_loss = 2.206 | val_acc = 0.112 |\n",
            "Epoch 9: loss = 2.197 | acc = 0.112 | val_loss = 2.195 | val_acc = 0.111 |\n",
            "Epoch 10: loss = 2.186 | acc = 0.111 | val_loss = 2.185 | val_acc = 0.111 |\n",
            "Epoch 11: loss = 2.175 | acc = 0.111 | val_loss = 2.174 | val_acc = 0.111 |\n",
            "Epoch 12: loss = 2.164 | acc = 0.111 | val_loss = 2.162 | val_acc = 0.111 |\n",
            "Epoch 13: loss = 2.151 | acc = 0.111 | val_loss = 2.149 | val_acc = 0.111 |\n",
            "Epoch 14: loss = 2.138 | acc = 0.111 | val_loss = 2.137 | val_acc = 0.11 |\n",
            "Epoch 15: loss = 2.126 | acc = 0.111 | val_loss = 2.125 | val_acc = 0.11 |\n",
            "Epoch 16: loss = 2.115 | acc = 0.11 | val_loss = 2.115 | val_acc = 0.11 |\n",
            "Epoch 17: loss = 2.105 | acc = 0.11 | val_loss = 2.105 | val_acc = 0.11 |\n",
            "Epoch 18: loss = 2.096 | acc = 0.11 | val_loss = 2.097 | val_acc = 0.11 |\n",
            "Epoch 19: loss = 2.087 | acc = 0.11 | val_loss = 2.089 | val_acc = 0.109 |\n",
            "Epoch 20: loss = 2.08 | acc = 0.109 | val_loss = 2.081 | val_acc = 0.108 |\n",
            "Epoch 21: loss = 2.072 | acc = 0.109 | val_loss = 2.074 | val_acc = 0.108 |\n",
            "Epoch 22: loss = 2.065 | acc = 0.109 | val_loss = 2.067 | val_acc = 0.108 |\n",
            "Epoch 23: loss = 2.059 | acc = 0.109 | val_loss = 2.061 | val_acc = 0.108 |\n",
            "Epoch 24: loss = 2.053 | acc = 0.108 | val_loss = 2.055 | val_acc = 0.107 |\n",
            "Epoch 25: loss = 2.047 | acc = 0.108 | val_loss = 2.049 | val_acc = 0.107 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.097 | val_loss = 2.259 | val_acc = 0.091 |\n",
            "Epoch 2: loss = 2.209 | acc = 0.092 | val_loss = 2.153 | val_acc = 0.09 |\n",
            "Epoch 3: loss = 2.111 | acc = 0.092 | val_loss = 2.065 | val_acc = 0.091 |\n",
            "Epoch 4: loss = 2.024 | acc = 0.09 | val_loss = 2.0 | val_acc = 0.09 |\n",
            "Epoch 5: loss = 1.982 | acc = 0.088 | val_loss = 1.97 | val_acc = 0.087 |\n",
            "Epoch 6: loss = 1.955 | acc = 0.097 | val_loss = 1.945 | val_acc = 0.106 |\n",
            "Epoch 7: loss = 1.929 | acc = 0.109 | val_loss = 1.922 | val_acc = 0.109 |\n",
            "Epoch 8: loss = 1.908 | acc = 0.11 | val_loss = 1.902 | val_acc = 0.109 |\n",
            "Epoch 9: loss = 1.887 | acc = 0.11 | val_loss = 1.879 | val_acc = 0.11 |\n",
            "Epoch 10: loss = 1.865 | acc = 0.11 | val_loss = 1.86 | val_acc = 0.11 |\n",
            "Epoch 11: loss = 1.849 | acc = 0.111 | val_loss = 1.848 | val_acc = 0.111 |\n",
            "Epoch 12: loss = 1.838 | acc = 0.112 | val_loss = 1.839 | val_acc = 0.111 |\n",
            "Epoch 13: loss = 1.83 | acc = 0.113 | val_loss = 1.831 | val_acc = 0.112 |\n",
            "Epoch 14: loss = 1.823 | acc = 0.115 | val_loss = 1.825 | val_acc = 0.114 |\n",
            "Epoch 15: loss = 1.817 | acc = 0.116 | val_loss = 1.82 | val_acc = 0.116 |\n",
            "Epoch 16: loss = 1.812 | acc = 0.118 | val_loss = 1.815 | val_acc = 0.118 |\n",
            "Epoch 17: loss = 1.807 | acc = 0.12 | val_loss = 1.81 | val_acc = 0.119 |\n",
            "Epoch 18: loss = 1.803 | acc = 0.122 | val_loss = 1.807 | val_acc = 0.122 |\n",
            "Epoch 19: loss = 1.8 | acc = 0.125 | val_loss = 1.803 | val_acc = 0.125 |\n",
            "Epoch 20: loss = 1.796 | acc = 0.127 | val_loss = 1.8 | val_acc = 0.128 |\n",
            "Epoch 21: loss = 1.793 | acc = 0.13 | val_loss = 1.797 | val_acc = 0.132 |\n",
            "Epoch 22: loss = 1.79 | acc = 0.134 | val_loss = 1.794 | val_acc = 0.135 |\n",
            "Epoch 23: loss = 1.787 | acc = 0.138 | val_loss = 1.791 | val_acc = 0.14 |\n",
            "Epoch 24: loss = 1.785 | acc = 0.143 | val_loss = 1.789 | val_acc = 0.146 |\n",
            "Epoch 25: loss = 1.782 | acc = 0.15 | val_loss = 1.787 | val_acc = 0.152 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.981 | acc = 0.446 | val_loss = 1.839 | val_acc = 0.615 |\n",
            "Epoch 2: loss = 1.802 | acc = 0.643 | val_loss = 1.783 | val_acc = 0.647 |\n",
            "Epoch 3: loss = 1.764 | acc = 0.662 | val_loss = 1.758 | val_acc = 0.663 |\n",
            "Epoch 4: loss = 1.744 | acc = 0.672 | val_loss = 1.743 | val_acc = 0.67 |\n",
            "Epoch 5: loss = 1.732 | acc = 0.68 | val_loss = 1.733 | val_acc = 0.676 |\n",
            "Epoch 6: loss = 1.723 | acc = 0.685 | val_loss = 1.726 | val_acc = 0.682 |\n",
            "Epoch 7: loss = 1.716 | acc = 0.69 | val_loss = 1.72 | val_acc = 0.684 |\n",
            "Epoch 8: loss = 1.71 | acc = 0.694 | val_loss = 1.715 | val_acc = 0.686 |\n",
            "Epoch 9: loss = 1.705 | acc = 0.698 | val_loss = 1.71 | val_acc = 0.693 |\n",
            "Epoch 10: loss = 1.701 | acc = 0.703 | val_loss = 1.706 | val_acc = 0.697 |\n",
            "Epoch 11: loss = 1.697 | acc = 0.707 | val_loss = 1.703 | val_acc = 0.699 |\n",
            "Epoch 12: loss = 1.694 | acc = 0.71 | val_loss = 1.701 | val_acc = 0.704 |\n",
            "Epoch 13: loss = 1.691 | acc = 0.713 | val_loss = 1.698 | val_acc = 0.709 |\n",
            "Epoch 14: loss = 1.689 | acc = 0.717 | val_loss = 1.696 | val_acc = 0.711 |\n",
            "Epoch 15: loss = 1.687 | acc = 0.72 | val_loss = 1.694 | val_acc = 0.712 |\n",
            "Epoch 16: loss = 1.685 | acc = 0.723 | val_loss = 1.692 | val_acc = 0.715 |\n",
            "Epoch 17: loss = 1.683 | acc = 0.725 | val_loss = 1.691 | val_acc = 0.716 |\n",
            "Epoch 18: loss = 1.681 | acc = 0.727 | val_loss = 1.689 | val_acc = 0.719 |\n",
            "Epoch 19: loss = 1.679 | acc = 0.729 | val_loss = 1.687 | val_acc = 0.721 |\n",
            "Epoch 20: loss = 1.677 | acc = 0.731 | val_loss = 1.686 | val_acc = 0.724 |\n",
            "Epoch 21: loss = 1.676 | acc = 0.734 | val_loss = 1.685 | val_acc = 0.725 |\n",
            "Epoch 22: loss = 1.674 | acc = 0.735 | val_loss = 1.683 | val_acc = 0.726 |\n",
            "Epoch 23: loss = 1.673 | acc = 0.736 | val_loss = 1.682 | val_acc = 0.727 |\n",
            "Epoch 24: loss = 1.672 | acc = 0.737 | val_loss = 1.681 | val_acc = 0.73 |\n",
            "Epoch 25: loss = 1.67 | acc = 0.739 | val_loss = 1.68 | val_acc = 0.731 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.688 | acc = 0.725 | val_loss = 1.635 | val_acc = 0.748 |\n",
            "Epoch 2: loss = 1.614 | acc = 0.794 | val_loss = 1.612 | val_acc = 0.793 |\n",
            "Epoch 3: loss = 1.598 | acc = 0.813 | val_loss = 1.601 | val_acc = 0.807 |\n",
            "Epoch 4: loss = 1.588 | acc = 0.825 | val_loss = 1.592 | val_acc = 0.812 |\n",
            "Epoch 5: loss = 1.579 | acc = 0.832 | val_loss = 1.586 | val_acc = 0.822 |\n",
            "Epoch 6: loss = 1.573 | acc = 0.838 | val_loss = 1.583 | val_acc = 0.822 |\n",
            "Epoch 7: loss = 1.568 | acc = 0.841 | val_loss = 1.581 | val_acc = 0.831 |\n",
            "Epoch 8: loss = 1.564 | acc = 0.846 | val_loss = 1.577 | val_acc = 0.833 |\n",
            "Epoch 9: loss = 1.561 | acc = 0.849 | val_loss = 1.574 | val_acc = 0.836 |\n",
            "Epoch 10: loss = 1.559 | acc = 0.852 | val_loss = 1.578 | val_acc = 0.834 |\n",
            "Epoch 11: loss = 1.556 | acc = 0.854 | val_loss = 1.572 | val_acc = 0.841 |\n",
            "Epoch 12: loss = 1.554 | acc = 0.856 | val_loss = 1.574 | val_acc = 0.839 |\n",
            "Epoch 13: loss = 1.553 | acc = 0.859 | val_loss = 1.569 | val_acc = 0.843 |\n",
            "Epoch 14: loss = 1.551 | acc = 0.86 | val_loss = 1.569 | val_acc = 0.845 |\n",
            "Epoch 15: loss = 1.549 | acc = 0.863 | val_loss = 1.567 | val_acc = 0.848 |\n",
            "Epoch 16: loss = 1.548 | acc = 0.865 | val_loss = 1.566 | val_acc = 0.846 |\n",
            "Epoch 17: loss = 1.547 | acc = 0.866 | val_loss = 1.565 | val_acc = 0.849 |\n",
            "Epoch 18: loss = 1.545 | acc = 0.867 | val_loss = 1.565 | val_acc = 0.852 |\n",
            "Epoch 19: loss = 1.544 | acc = 0.869 | val_loss = 1.564 | val_acc = 0.851 |\n",
            "Epoch 20: loss = 1.543 | acc = 0.869 | val_loss = 1.566 | val_acc = 0.85 |\n",
            "Epoch 21: loss = 1.542 | acc = 0.872 | val_loss = 1.563 | val_acc = 0.854 |\n",
            "Epoch 22: loss = 1.541 | acc = 0.872 | val_loss = 1.564 | val_acc = 0.856 |\n",
            "Epoch 23: loss = 1.54 | acc = 0.873 | val_loss = 1.562 | val_acc = 0.856 |\n",
            "Epoch 24: loss = 1.539 | acc = 0.875 | val_loss = 1.563 | val_acc = 0.853 |\n",
            "Epoch 25: loss = 1.538 | acc = 0.875 | val_loss = 1.562 | val_acc = 0.856 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.772 | acc = 0.2 | val_loss = 1.733 | val_acc = 0.241 |\n",
            "Epoch 2: loss = 1.725 | acc = 0.34 | val_loss = 1.727 | val_acc = 0.443 |\n",
            "Epoch 3: loss = 1.72 | acc = 0.404 | val_loss = 1.724 | val_acc = 0.421 |\n",
            "Epoch 4: loss = 1.715 | acc = 0.44 | val_loss = 1.719 | val_acc = 0.458 |\n",
            "Epoch 5: loss = 1.711 | acc = 0.435 | val_loss = 1.721 | val_acc = 0.423 |\n",
            "Epoch 6: loss = 1.711 | acc = 0.496 | val_loss = 1.723 | val_acc = 0.453 |\n",
            "Epoch 7: loss = 1.71 | acc = 0.551 | val_loss = 1.719 | val_acc = 0.634 |\n",
            "Epoch 8: loss = 1.641 | acc = 0.679 | val_loss = 1.605 | val_acc = 0.727 |\n",
            "Epoch 9: loss = 1.596 | acc = 0.726 | val_loss = 1.603 | val_acc = 0.714 |\n",
            "Epoch 10: loss = 1.593 | acc = 0.763 | val_loss = 1.605 | val_acc = 0.735 |\n",
            "Epoch 11: loss = 1.591 | acc = 0.768 | val_loss = 1.597 | val_acc = 0.707 |\n",
            "Epoch 12: loss = 1.589 | acc = 0.762 | val_loss = 1.599 | val_acc = 0.729 |\n",
            "Epoch 13: loss = 1.588 | acc = 0.753 | val_loss = 1.598 | val_acc = 0.812 |\n",
            "Epoch 14: loss = 1.589 | acc = 0.798 | val_loss = 1.599 | val_acc = 0.832 |\n",
            "Epoch 15: loss = 1.588 | acc = 0.808 | val_loss = 1.596 | val_acc = 0.805 |\n",
            "Epoch 16: loss = 1.587 | acc = 0.804 | val_loss = 1.599 | val_acc = 0.748 |\n",
            "Epoch 17: loss = 1.585 | acc = 0.802 | val_loss = 1.598 | val_acc = 0.826 |\n",
            "Epoch 18: loss = 1.585 | acc = 0.813 | val_loss = 1.591 | val_acc = 0.821 |\n",
            "Epoch 19: loss = 1.583 | acc = 0.827 | val_loss = 1.597 | val_acc = 0.824 |\n",
            "Epoch 20: loss = 1.582 | acc = 0.812 | val_loss = 1.603 | val_acc = 0.809 |\n",
            "Epoch 21: loss = 1.583 | acc = 0.815 | val_loss = 1.597 | val_acc = 0.792 |\n",
            "Epoch 22: loss = 1.582 | acc = 0.814 | val_loss = 1.612 | val_acc = 0.726 |\n",
            "Epoch 23: loss = 1.581 | acc = 0.826 | val_loss = 1.6 | val_acc = 0.787 |\n",
            "Epoch 24: loss = 1.585 | acc = 0.82 | val_loss = 1.58 | val_acc = 0.825 |\n",
            "Epoch 25: loss = 1.564 | acc = 0.841 | val_loss = 1.573 | val_acc = 0.82 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.335 | acc = 0.092 | val_loss = 2.33 | val_acc = 0.089 |\n",
            "Epoch 2: loss = 2.327 | acc = 0.091 | val_loss = 2.323 | val_acc = 0.089 |\n",
            "Epoch 3: loss = 2.32 | acc = 0.09 | val_loss = 2.317 | val_acc = 0.088 |\n",
            "Epoch 4: loss = 2.314 | acc = 0.089 | val_loss = 2.311 | val_acc = 0.087 |\n",
            "Epoch 5: loss = 2.309 | acc = 0.088 | val_loss = 2.307 | val_acc = 0.087 |\n",
            "Epoch 6: loss = 2.305 | acc = 0.087 | val_loss = 2.303 | val_acc = 0.086 |\n",
            "Epoch 7: loss = 2.301 | acc = 0.086 | val_loss = 2.299 | val_acc = 0.086 |\n",
            "Epoch 8: loss = 2.297 | acc = 0.086 | val_loss = 2.295 | val_acc = 0.085 |\n",
            "Epoch 9: loss = 2.294 | acc = 0.085 | val_loss = 2.292 | val_acc = 0.085 |\n",
            "Epoch 10: loss = 2.291 | acc = 0.084 | val_loss = 2.289 | val_acc = 0.084 |\n",
            "Epoch 11: loss = 2.288 | acc = 0.083 | val_loss = 2.286 | val_acc = 0.083 |\n",
            "Epoch 12: loss = 2.285 | acc = 0.083 | val_loss = 2.283 | val_acc = 0.083 |\n",
            "Epoch 13: loss = 2.282 | acc = 0.082 | val_loss = 2.28 | val_acc = 0.082 |\n",
            "Epoch 14: loss = 2.279 | acc = 0.081 | val_loss = 2.277 | val_acc = 0.081 |\n",
            "Epoch 15: loss = 2.276 | acc = 0.081 | val_loss = 2.274 | val_acc = 0.08 |\n",
            "Epoch 16: loss = 2.273 | acc = 0.08 | val_loss = 2.271 | val_acc = 0.08 |\n",
            "Epoch 17: loss = 2.27 | acc = 0.08 | val_loss = 2.268 | val_acc = 0.079 |\n",
            "Epoch 18: loss = 2.267 | acc = 0.079 | val_loss = 2.265 | val_acc = 0.079 |\n",
            "Epoch 19: loss = 2.264 | acc = 0.079 | val_loss = 2.262 | val_acc = 0.079 |\n",
            "Epoch 20: loss = 2.261 | acc = 0.079 | val_loss = 2.259 | val_acc = 0.078 |\n",
            "Epoch 21: loss = 2.257 | acc = 0.078 | val_loss = 2.256 | val_acc = 0.078 |\n",
            "Epoch 22: loss = 2.254 | acc = 0.078 | val_loss = 2.253 | val_acc = 0.077 |\n",
            "Epoch 23: loss = 2.251 | acc = 0.078 | val_loss = 2.249 | val_acc = 0.077 |\n",
            "Epoch 24: loss = 2.247 | acc = 0.078 | val_loss = 2.246 | val_acc = 0.077 |\n",
            "Epoch 25: loss = 2.244 | acc = 0.077 | val_loss = 2.242 | val_acc = 0.076 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.1 | val_loss = 2.258 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.227 | acc = 0.1 | val_loss = 2.206 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.185 | acc = 0.1 | val_loss = 2.172 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.157 | acc = 0.1 | val_loss = 2.148 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.136 | acc = 0.1 | val_loss = 2.13 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.12 | acc = 0.1 | val_loss = 2.116 | val_acc = 0.1 |\n",
            "Epoch 7: loss = 2.107 | acc = 0.1 | val_loss = 2.104 | val_acc = 0.1 |\n",
            "Epoch 8: loss = 2.095 | acc = 0.1 | val_loss = 2.094 | val_acc = 0.101 |\n",
            "Epoch 9: loss = 2.085 | acc = 0.1 | val_loss = 2.083 | val_acc = 0.101 |\n",
            "Epoch 10: loss = 2.068 | acc = 0.1 | val_loss = 2.055 | val_acc = 0.101 |\n",
            "Epoch 11: loss = 2.04 | acc = 0.101 | val_loss = 2.038 | val_acc = 0.101 |\n",
            "Epoch 12: loss = 2.029 | acc = 0.101 | val_loss = 2.029 | val_acc = 0.101 |\n",
            "Epoch 13: loss = 2.021 | acc = 0.101 | val_loss = 2.021 | val_acc = 0.102 |\n",
            "Epoch 14: loss = 2.014 | acc = 0.101 | val_loss = 2.015 | val_acc = 0.102 |\n",
            "Epoch 15: loss = 2.008 | acc = 0.101 | val_loss = 2.009 | val_acc = 0.102 |\n",
            "Epoch 16: loss = 2.003 | acc = 0.101 | val_loss = 2.004 | val_acc = 0.102 |\n",
            "Epoch 17: loss = 1.998 | acc = 0.101 | val_loss = 1.999 | val_acc = 0.102 |\n",
            "Epoch 18: loss = 1.993 | acc = 0.101 | val_loss = 1.994 | val_acc = 0.102 |\n",
            "Epoch 19: loss = 1.989 | acc = 0.101 | val_loss = 1.99 | val_acc = 0.102 |\n",
            "Epoch 20: loss = 1.985 | acc = 0.102 | val_loss = 1.987 | val_acc = 0.102 |\n",
            "Epoch 21: loss = 1.981 | acc = 0.102 | val_loss = 1.983 | val_acc = 0.102 |\n",
            "Epoch 22: loss = 1.978 | acc = 0.102 | val_loss = 1.979 | val_acc = 0.102 |\n",
            "Epoch 23: loss = 1.974 | acc = 0.102 | val_loss = 1.975 | val_acc = 0.102 |\n",
            "Epoch 24: loss = 1.964 | acc = 0.102 | val_loss = 1.953 | val_acc = 0.103 |\n",
            "Epoch 25: loss = 1.939 | acc = 0.102 | val_loss = 1.935 | val_acc = 0.102 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.157 | acc = 0.305 | val_loss = 2.017 | val_acc = 0.458 |\n",
            "Epoch 2: loss = 1.967 | acc = 0.51 | val_loss = 1.942 | val_acc = 0.531 |\n",
            "Epoch 3: loss = 1.923 | acc = 0.55 | val_loss = 1.914 | val_acc = 0.559 |\n",
            "Epoch 4: loss = 1.902 | acc = 0.568 | val_loss = 1.898 | val_acc = 0.57 |\n",
            "Epoch 5: loss = 1.889 | acc = 0.578 | val_loss = 1.887 | val_acc = 0.582 |\n",
            "Epoch 6: loss = 1.879 | acc = 0.584 | val_loss = 1.88 | val_acc = 0.586 |\n",
            "Epoch 7: loss = 1.872 | acc = 0.588 | val_loss = 1.873 | val_acc = 0.59 |\n",
            "Epoch 8: loss = 1.867 | acc = 0.592 | val_loss = 1.868 | val_acc = 0.592 |\n",
            "Epoch 9: loss = 1.862 | acc = 0.594 | val_loss = 1.863 | val_acc = 0.593 |\n",
            "Epoch 10: loss = 1.857 | acc = 0.595 | val_loss = 1.859 | val_acc = 0.596 |\n",
            "Epoch 11: loss = 1.853 | acc = 0.597 | val_loss = 1.855 | val_acc = 0.596 |\n",
            "Epoch 12: loss = 1.849 | acc = 0.598 | val_loss = 1.852 | val_acc = 0.599 |\n",
            "Epoch 13: loss = 1.846 | acc = 0.6 | val_loss = 1.849 | val_acc = 0.599 |\n",
            "Epoch 14: loss = 1.844 | acc = 0.602 | val_loss = 1.847 | val_acc = 0.599 |\n",
            "Epoch 15: loss = 1.842 | acc = 0.603 | val_loss = 1.845 | val_acc = 0.602 |\n",
            "Epoch 16: loss = 1.839 | acc = 0.604 | val_loss = 1.843 | val_acc = 0.602 |\n",
            "Epoch 17: loss = 1.838 | acc = 0.605 | val_loss = 1.841 | val_acc = 0.604 |\n",
            "Epoch 18: loss = 1.836 | acc = 0.607 | val_loss = 1.839 | val_acc = 0.604 |\n",
            "Epoch 19: loss = 1.834 | acc = 0.608 | val_loss = 1.838 | val_acc = 0.606 |\n",
            "Epoch 20: loss = 1.833 | acc = 0.609 | val_loss = 1.836 | val_acc = 0.606 |\n",
            "Epoch 21: loss = 1.831 | acc = 0.61 | val_loss = 1.835 | val_acc = 0.609 |\n",
            "Epoch 22: loss = 1.83 | acc = 0.612 | val_loss = 1.834 | val_acc = 0.609 |\n",
            "Epoch 23: loss = 1.829 | acc = 0.612 | val_loss = 1.833 | val_acc = 0.61 |\n",
            "Epoch 24: loss = 1.828 | acc = 0.614 | val_loss = 1.831 | val_acc = 0.611 |\n",
            "Epoch 25: loss = 1.826 | acc = 0.614 | val_loss = 1.831 | val_acc = 0.612 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.822 | acc = 0.498 | val_loss = 1.74 | val_acc = 0.649 |\n",
            "Epoch 2: loss = 1.716 | acc = 0.672 | val_loss = 1.711 | val_acc = 0.672 |\n",
            "Epoch 3: loss = 1.681 | acc = 0.713 | val_loss = 1.632 | val_acc = 0.772 |\n",
            "Epoch 4: loss = 1.612 | acc = 0.79 | val_loss = 1.615 | val_acc = 0.784 |\n",
            "Epoch 5: loss = 1.6 | acc = 0.798 | val_loss = 1.606 | val_acc = 0.79 |\n",
            "Epoch 6: loss = 1.593 | acc = 0.805 | val_loss = 1.601 | val_acc = 0.798 |\n",
            "Epoch 7: loss = 1.587 | acc = 0.811 | val_loss = 1.597 | val_acc = 0.799 |\n",
            "Epoch 8: loss = 1.583 | acc = 0.815 | val_loss = 1.593 | val_acc = 0.803 |\n",
            "Epoch 9: loss = 1.579 | acc = 0.818 | val_loss = 1.59 | val_acc = 0.81 |\n",
            "Epoch 10: loss = 1.576 | acc = 0.822 | val_loss = 1.588 | val_acc = 0.811 |\n",
            "Epoch 11: loss = 1.573 | acc = 0.826 | val_loss = 1.586 | val_acc = 0.814 |\n",
            "Epoch 12: loss = 1.571 | acc = 0.829 | val_loss = 1.584 | val_acc = 0.818 |\n",
            "Epoch 13: loss = 1.569 | acc = 0.832 | val_loss = 1.583 | val_acc = 0.818 |\n",
            "Epoch 14: loss = 1.567 | acc = 0.835 | val_loss = 1.582 | val_acc = 0.817 |\n",
            "Epoch 15: loss = 1.565 | acc = 0.837 | val_loss = 1.581 | val_acc = 0.822 |\n",
            "Epoch 16: loss = 1.563 | acc = 0.839 | val_loss = 1.579 | val_acc = 0.824 |\n",
            "Epoch 17: loss = 1.562 | acc = 0.842 | val_loss = 1.578 | val_acc = 0.83 |\n",
            "Epoch 18: loss = 1.56 | acc = 0.843 | val_loss = 1.577 | val_acc = 0.828 |\n",
            "Epoch 19: loss = 1.559 | acc = 0.846 | val_loss = 1.577 | val_acc = 0.833 |\n",
            "Epoch 20: loss = 1.558 | acc = 0.848 | val_loss = 1.576 | val_acc = 0.833 |\n",
            "Epoch 21: loss = 1.556 | acc = 0.852 | val_loss = 1.574 | val_acc = 0.837 |\n",
            "Epoch 22: loss = 1.555 | acc = 0.853 | val_loss = 1.574 | val_acc = 0.839 |\n",
            "Epoch 23: loss = 1.554 | acc = 0.856 | val_loss = 1.573 | val_acc = 0.841 |\n",
            "Epoch 24: loss = 1.553 | acc = 0.858 | val_loss = 1.571 | val_acc = 0.843 |\n",
            "Epoch 25: loss = 1.551 | acc = 0.86 | val_loss = 1.571 | val_acc = 0.843 |\n",
            "~~~~~~~~~~~~~~~~~~Batch size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.7 | acc = 0.643 | val_loss = 1.608 | val_acc = 0.779 |\n",
            "Epoch 2: loss = 1.597 | acc = 0.788 | val_loss = 1.59 | val_acc = 0.806 |\n",
            "Epoch 3: loss = 1.581 | acc = 0.818 | val_loss = 1.598 | val_acc = 0.79 |\n",
            "Epoch 4: loss = 1.572 | acc = 0.831 | val_loss = 1.59 | val_acc = 0.818 |\n",
            "Epoch 5: loss = 1.566 | acc = 0.838 | val_loss = 1.575 | val_acc = 0.833 |\n",
            "Epoch 6: loss = 1.56 | acc = 0.847 | val_loss = 1.573 | val_acc = 0.839 |\n",
            "Epoch 7: loss = 1.556 | acc = 0.854 | val_loss = 1.568 | val_acc = 0.842 |\n",
            "Epoch 8: loss = 1.554 | acc = 0.857 | val_loss = 1.567 | val_acc = 0.843 |\n",
            "Epoch 9: loss = 1.551 | acc = 0.86 | val_loss = 1.567 | val_acc = 0.842 |\n",
            "Epoch 10: loss = 1.548 | acc = 0.862 | val_loss = 1.566 | val_acc = 0.845 |\n",
            "Epoch 11: loss = 1.547 | acc = 0.866 | val_loss = 1.562 | val_acc = 0.852 |\n",
            "Epoch 12: loss = 1.545 | acc = 0.867 | val_loss = 1.561 | val_acc = 0.847 |\n",
            "Epoch 13: loss = 1.545 | acc = 0.867 | val_loss = 1.563 | val_acc = 0.856 |\n",
            "Epoch 14: loss = 1.543 | acc = 0.87 | val_loss = 1.563 | val_acc = 0.846 |\n",
            "Epoch 15: loss = 1.542 | acc = 0.87 | val_loss = 1.561 | val_acc = 0.855 |\n",
            "Epoch 16: loss = 1.541 | acc = 0.872 | val_loss = 1.557 | val_acc = 0.858 |\n",
            "Epoch 17: loss = 1.54 | acc = 0.873 | val_loss = 1.559 | val_acc = 0.857 |\n",
            "Epoch 18: loss = 1.539 | acc = 0.875 | val_loss = 1.558 | val_acc = 0.861 |\n",
            "Epoch 19: loss = 1.538 | acc = 0.874 | val_loss = 1.56 | val_acc = 0.861 |\n",
            "Epoch 20: loss = 1.537 | acc = 0.877 | val_loss = 1.556 | val_acc = 0.856 |\n",
            "Epoch 21: loss = 1.536 | acc = 0.877 | val_loss = 1.554 | val_acc = 0.86 |\n",
            "Epoch 22: loss = 1.537 | acc = 0.878 | val_loss = 1.559 | val_acc = 0.846 |\n",
            "Epoch 23: loss = 1.536 | acc = 0.878 | val_loss = 1.556 | val_acc = 0.857 |\n",
            "Epoch 24: loss = 1.535 | acc = 0.881 | val_loss = 1.556 | val_acc = 0.857 |\n",
            "Epoch 25: loss = 1.534 | acc = 0.88 | val_loss = 1.56 | val_acc = 0.856 |\n",
            "Best batch size: 32\n",
            "Best learning rate: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(FMNIST_training_set.data.view(-1,28*28).numpy() ,\n",
        "            FMNIST_training_set.targets.numpy(),\n",
        "            FMNIST_test_set.data.view(-1,28*28).numpy(),\n",
        "            FMNIST_test_set.targets.numpy(),\n",
        "            num_hidden_layers=2,num_neurons=128)\n",
        "\n"
      ],
      "metadata": {
        "id": "-LzSs_CSYc-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc879e85-dc85-4c61-f0bb-ffb54cf06e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-03b14244862a>:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-11-03b14244862a>:233: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 1.945 | acc = 0.434 | val_loss = 1.78 | val_acc = 0.658 |\n",
            "Epoch 2: loss = 1.737 | acc = 0.688 | val_loss = 1.715 | val_acc = 0.7 |\n",
            "Epoch 3: loss = 1.696 | acc = 0.715 | val_loss = 1.688 | val_acc = 0.718 |\n",
            "Epoch 4: loss = 1.676 | acc = 0.728 | val_loss = 1.673 | val_acc = 0.723 |\n",
            "Epoch 5: loss = 1.663 | acc = 0.735 | val_loss = 1.662 | val_acc = 0.731 |\n",
            "Epoch 6: loss = 1.653 | acc = 0.743 | val_loss = 1.654 | val_acc = 0.735 |\n",
            "Epoch 7: loss = 1.646 | acc = 0.747 | val_loss = 1.648 | val_acc = 0.74 |\n",
            "Epoch 8: loss = 1.64 | acc = 0.752 | val_loss = 1.643 | val_acc = 0.751 |\n",
            "Epoch 9: loss = 1.635 | acc = 0.757 | val_loss = 1.639 | val_acc = 0.75 |\n",
            "Epoch 10: loss = 1.63 | acc = 0.761 | val_loss = 1.635 | val_acc = 0.752 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Run Experiments"
      ],
      "metadata": {
        "id": "t4-2TSaM9RUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Initializing your model weights in a few different ways\n",
        "\n",
        "Create several MLPs with a single hidden layer having 128 units, initializing the weights as\n",
        "- (1) all zeros\n",
        "- (2) Uniform [-1, 1]\n",
        "- (3) Gaussian N(0,1)\n",
        "- (4) Xavier\n",
        "- (5) Kaiming\n",
        "\n",
        "After training these models, compare the effect of weight initialization on the training curves and test accuracy on the Fashion MNIST dataset."
      ],
      "metadata": {
        "id": "uliUNzbX9a5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Initializing the weights of a neural network to zero isn't always a good practice. If all weights are initialized to zero, each neuron in a layer will learn the same features during training, which is not ideal. The zero initialization is more appropriate for biases in certain circumstances, but not typically for weights.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "The Xavier initialization (also known as Glorot initialization) is especially designed for the sigmoid and hyperbolic tangent (tanh) activation functions.\n",
        "\n",
        "This initialization method helps to prevent the gradients from becoming too small or too large at the beginning of the training, which can aid in achieving faster convergence and better training of deeper models.\n",
        "\n",
        "----\n",
        "Kaiming initialization, also known as He initialization, is designed specifically for ReLU (rectified linear unit) and its variants. It sets the weights of each neuron such that the variance of its outputs, over all possible inputs, should remain the same.\n",
        "\n",
        "Kaiming initialization helps to prevent the \"dying ReLU\" problem, where neurons can sometimes get stuck and remain inactive for all inputs. This method helps ensure that the variance of the outputs of each layer remains roughly the same, which can aid in achieving faster convergence and better training of deeper models when using ReLU activations.\n"
      ],
      "metadata": {
        "id": "I1XkSKKG_Zm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: Different number of hidden layers\n",
        "\n",
        "Create three different models:\n",
        "- (1) an MLP with no hidden layers, i.e., it directly maps the inputs to outputs\n",
        "- (2) an MLP with a single hidden layer having 128 units and ReLU activation\n",
        "- (3) an MLP with 2 hidden layers each having 128 units with ReLU activations\n",
        "\n",
        "It should be noted that since we want to perform classification, all of these models should have a softmax layer at the end.\n",
        "\n",
        "After training, compare the test accuracy of these three models on the Fashion MNIST dataset. Comment on how non-linearity and network depth affects the accuracy. Are the results that you obtain expected?"
      ],
      "metadata": {
        "id": "guGMrMJZCA_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3: Different activation functions\n",
        "\n",
        "Take the last model above, the one with 2 hidden layers, and create two different copies of it in which you pick two activations of your choice (except ReLU) from the course slides.\n",
        "\n",
        "After training these two models on Fashion MNIST compare their test accuracies with the model with ReLU activations.\n",
        "\n",
        "Comment on the performances of these models: which one is better and why? Are certain activations better than others?\n",
        "\n",
        "If the results are not as you expected, what could be the reason?\n"
      ],
      "metadata": {
        "id": "8g390z7NDUea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 4: add L1 and L2 regularizations.\n",
        "\n",
        "Create an MLP with 2 hidden layers each having 128 units with ReLU activations as above. However, this\n",
        "time, independently add L1 and L2 regularization to the network and train the MLP in this way. How do these regularizations affect the accuracy? (This proportion can be varied as a tunable hyperparameter that can be\n",
        "explored as part of other project requirements)"
      ],
      "metadata": {
        "id": "IaM3DGWTl1FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_l2 = MLP(FMNIST_training_set.data.view(-1,28*28).numpy() ,\n",
        "            FMNIST_training_set.targets.numpy(),\n",
        "            FMNIST_test_set.data.view(-1,28*28).numpy(),\n",
        "            FMNIST_test_set.targets.numpy(),\n",
        "            num_hidden_layers=2,num_neurons=128)\n",
        "\n",
        "model_no_l2.train(batch_size=4,epochs=25,lr=1.0)"
      ],
      "metadata": {
        "id": "nA-W0ZI7eozp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bd088e-be03-4fb9-facf-f811739a4803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-6d692885109f>:75: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-13-6d692885109f>:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 1.637 | acc = 0.704 | val_loss = 1.607 | val_acc = 0.758 |\n",
            "Epoch 2: loss = 1.601 | acc = 0.761 | val_loss = 1.601 | val_acc = 0.757 |\n",
            "Epoch 3: loss = 1.594 | acc = 0.775 | val_loss = 1.596 | val_acc = 0.781 |\n",
            "Epoch 4: loss = 1.591 | acc = 0.781 | val_loss = 1.608 | val_acc = 0.757 |\n",
            "Epoch 5: loss = 1.59 | acc = 0.792 | val_loss = 1.6 | val_acc = 0.804 |\n",
            "Epoch 6: loss = 1.587 | acc = 0.801 | val_loss = 1.607 | val_acc = 0.784 |\n",
            "Epoch 7: loss = 1.584 | acc = 0.805 | val_loss = 1.603 | val_acc = 0.769 |\n",
            "Epoch 8: loss = 1.582 | acc = 0.801 | val_loss = 1.589 | val_acc = 0.799 |\n",
            "Epoch 9: loss = 1.583 | acc = 0.802 | val_loss = 1.599 | val_acc = 0.786 |\n",
            "Epoch 10: loss = 1.581 | acc = 0.807 | val_loss = 1.593 | val_acc = 0.796 |\n",
            "Epoch 11: loss = 1.582 | acc = 0.807 | val_loss = 1.583 | val_acc = 0.819 |\n",
            "Epoch 12: loss = 1.583 | acc = 0.801 | val_loss = 1.597 | val_acc = 0.79 |\n",
            "Epoch 13: loss = 1.581 | acc = 0.803 | val_loss = 1.594 | val_acc = 0.803 |\n",
            "Epoch 14: loss = 1.58 | acc = 0.819 | val_loss = 1.585 | val_acc = 0.806 |\n",
            "Epoch 15: loss = 1.578 | acc = 0.819 | val_loss = 1.585 | val_acc = 0.814 |\n",
            "Epoch 16: loss = 1.579 | acc = 0.814 | val_loss = 1.591 | val_acc = 0.797 |\n",
            "Epoch 17: loss = 1.578 | acc = 0.816 | val_loss = 1.591 | val_acc = 0.815 |\n",
            "Epoch 18: loss = 1.578 | acc = 0.817 | val_loss = 1.615 | val_acc = 0.776 |\n",
            "Epoch 19: loss = 1.577 | acc = 0.819 | val_loss = 1.594 | val_acc = 0.79 |\n",
            "Epoch 20: loss = 1.575 | acc = 0.821 | val_loss = 1.588 | val_acc = 0.815 |\n",
            "Epoch 21: loss = 1.576 | acc = 0.82 | val_loss = 1.584 | val_acc = 0.813 |\n",
            "Epoch 22: loss = 1.575 | acc = 0.823 | val_loss = 1.586 | val_acc = 0.82 |\n",
            "Epoch 23: loss = 1.573 | acc = 0.831 | val_loss = 1.591 | val_acc = 0.807 |\n",
            "Epoch 24: loss = 1.576 | acc = 0.824 | val_loss = 1.585 | val_acc = 0.822 |\n",
            "Epoch 25: loss = 1.574 | acc = 0.825 | val_loss = 1.589 | val_acc = 0.803 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2 = MLP(FMNIST_training_set.data.view(-1,28*28).numpy() ,\n",
        "            FMNIST_training_set.targets.numpy(),\n",
        "            FMNIST_test_set.data.view(-1,28*28).numpy(),\n",
        "            FMNIST_test_set.targets.numpy(),\n",
        "            num_hidden_layers=2,num_neurons=128, l2_lambda= 0.000001)\n",
        "\n",
        "model_l2.train(batch_size=32,epochs=25,lr=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m35pkthteyQ",
        "outputId": "c64f40c3-b7af-4fe4-bc70-3dec036c86c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-141793cbf3e5>:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-17-141793cbf3e5>:233: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 1.665 | acc = 0.722 | val_loss = 1.614 | val_acc = 0.791 |\n",
            "Epoch 2: loss = 1.593 | acc = 0.815 | val_loss = 1.593 | val_acc = 0.816 |\n",
            "Epoch 3: loss = 1.579 | acc = 0.832 | val_loss = 1.584 | val_acc = 0.827 |\n",
            "Epoch 4: loss = 1.57 | acc = 0.84 | val_loss = 1.58 | val_acc = 0.829 |\n",
            "Epoch 5: loss = 1.565 | acc = 0.847 | val_loss = 1.578 | val_acc = 0.831 |\n",
            "Epoch 6: loss = 1.56 | acc = 0.851 | val_loss = 1.575 | val_acc = 0.841 |\n",
            "Epoch 7: loss = 1.556 | acc = 0.855 | val_loss = 1.57 | val_acc = 0.845 |\n",
            "Epoch 8: loss = 1.553 | acc = 0.858 | val_loss = 1.577 | val_acc = 0.833 |\n",
            "Epoch 9: loss = 1.55 | acc = 0.86 | val_loss = 1.568 | val_acc = 0.844 |\n",
            "Epoch 10: loss = 1.548 | acc = 0.862 | val_loss = 1.566 | val_acc = 0.85 |\n",
            "Epoch 11: loss = 1.546 | acc = 0.866 | val_loss = 1.565 | val_acc = 0.85 |\n",
            "Epoch 12: loss = 1.544 | acc = 0.868 | val_loss = 1.565 | val_acc = 0.849 |\n",
            "Epoch 13: loss = 1.542 | acc = 0.87 | val_loss = 1.565 | val_acc = 0.855 |\n",
            "Epoch 14: loss = 1.541 | acc = 0.872 | val_loss = 1.563 | val_acc = 0.856 |\n",
            "Epoch 15: loss = 1.539 | acc = 0.873 | val_loss = 1.561 | val_acc = 0.859 |\n",
            "Epoch 16: loss = 1.539 | acc = 0.875 | val_loss = 1.564 | val_acc = 0.85 |\n",
            "Epoch 17: loss = 1.537 | acc = 0.878 | val_loss = 1.559 | val_acc = 0.859 |\n",
            "Epoch 18: loss = 1.536 | acc = 0.877 | val_loss = 1.559 | val_acc = 0.859 |\n",
            "Epoch 19: loss = 1.535 | acc = 0.878 | val_loss = 1.562 | val_acc = 0.858 |\n",
            "Epoch 20: loss = 1.534 | acc = 0.88 | val_loss = 1.56 | val_acc = 0.861 |\n",
            "Epoch 21: loss = 1.533 | acc = 0.882 | val_loss = 1.56 | val_acc = 0.857 |\n",
            "Epoch 22: loss = 1.532 | acc = 0.883 | val_loss = 1.557 | val_acc = 0.863 |\n",
            "Epoch 23: loss = 1.531 | acc = 0.883 | val_loss = 1.558 | val_acc = 0.863 |\n",
            "Epoch 24: loss = 1.53 | acc = 0.884 | val_loss = 1.557 | val_acc = 0.864 |\n",
            "Epoch 25: loss = 1.529 | acc = 0.886 | val_loss = 1.557 | val_acc = 0.863 |\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8629"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_l1_l2 = MLP(fashion_mnist_x_train ,\n",
        "            fashion_mnist_y_train,\n",
        "            fashion_mnist_x_test,\n",
        "            fashion_mnist_y_test,\n",
        "            num_hidden_layers=2,num_neurons=128, l1_lambda = 0.000001, l2_lambda= 0.000001)\n",
        "\n",
        "model_l1_l2.train(batch_size=32,epochs=25,lr=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "2rJY85EozPhC",
        "outputId": "ad8b72cc-8703-4951-df2d-39cffa483687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f8b3dad1a73e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_l1_l2 = MLP(fashion_mnist_x_train ,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mfashion_mnist_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mfashion_mnist_x_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mfashion_mnist_y_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             num_hidden_layers=2,num_neurons=128, l1_lambda = 0.000001, l2_lambda= 0.000001)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FMNIST_training_set.data.view(-1,28*28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykVTOH2pSDI5",
        "outputId": "df4279b7-ccb1-46b0-8081-7cb712494f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_10_training_set.data = np.array(CIFAR_10_training_set.data)"
      ],
      "metadata": {
        "id": "8q5eZUipTnMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_10_training_set.data.view(-1,32*32*3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "4lEjA9YNScDn",
        "outputId": "87d65d53-4432-4033-9495-3f7f119c5df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-23e4de5609a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCIFAR_10_training_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Type must be a sub-type of ndarray type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR_10_training_set.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMuBkLPQTBd9",
        "outputId": "96b9adfc-415f-49b0-97dd-27ab129584fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 5: Normalization vs Non-Normalization.\n",
        "\n",
        "Create an MLP with 2 hidden layers each having 128 units with ReLU activations as above. However, this time,\n",
        "train it with unnormalized images. How does this affect the accuracy?"
      ],
      "metadata": {
        "id": "7fVC2w4N1PUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_CIFAR_10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "    # Numbers from normalization comes from this link:\n",
        "    # https://www.kaggle.com/code/fanbyprinciple/cifar10-explanation-with-pytorch\n",
        "])\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "CIFAR_10_training_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_CIFAR_10)\n",
        "CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "\n",
        "CIFAR_10_testing_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_CIFAR_10)\n",
        "CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_testing_set,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l79KsPgPPuKN",
        "outputId": "ef222265-0833-49cd-e9e7-3dd4b6df3ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:08<00:00, 20903579.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "id": "K73jB6O-WCnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cifar_10 = MLP(np.asarray(CIFAR_10_training_set.data.reshape(-1,32*32*3)) ,\n",
        "            np.asarray(CIFAR_10_training_set.targets),\n",
        "            np.asarray(CIFAR_10_testing_set.data.reshape(-1,32*32*3)),\n",
        "                     np.asarray(CIFAR_10_testing_set.targets),\n",
        "            num_hidden_layers=2,num_neurons=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4r_ILMNRcDm",
        "outputId": "2c14efe6-5114-4fe2-9317-0a56b180bfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e3b905e1d7b4>:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-3-e3b905e1d7b4>:233: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cifar_10.train(batch_size=16,epochs=25,lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DenHeivQXGZ2",
        "outputId": "0f8c4a28-6a65-4069-a22f-f7d4be003d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e3b905e1d7b4>:101: RuntimeWarning: overflow encountered in exp\n",
            "  return 1./(1. + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 2.369 | acc = 0.101 | val_loss = 2.354 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.35 | acc = 0.1 | val_loss = 2.343 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.34 | acc = 0.1 | val_loss = 2.334 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.331 | acc = 0.1 | val_loss = 2.325 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1003"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def find_best_parameters(data, targets, validation_data, validation_targets, num_hidden_layers, num_neurons, l1_lambdas, l2_lambdas, batch_sizes, learning_rates, epochs=25):\n",
        "    best_accuracy = 0\n",
        "    best_l1_lambda = 0\n",
        "    best_l2_lambda = 0\n",
        "    best_batch_size = 0\n",
        "    best_lr = 0\n",
        "\n",
        "    for l1_lambda, l2_lambda, batch_size, lr in itertools.product(l1_lambdas, l2_lambdas, batch_sizes, learning_rates):\n",
        "        print(f\"~~~~~~~~~~~~~~~~~~L1 Lambda: {l1_lambda}~~~~~~~~~~~~~~~~~~\")\n",
        "        print(f\"~~~~~~~~~~~~~~~~~~L2 Lambda: {l2_lambda}~~~~~~~~~~~~~~~~~~\")\n",
        "        print(f\"~~~~~~~~~~~~~~~~~~Batch Size: {batch_size}~~~~~~~~~~~~~~~~~~\")\n",
        "        print(f\"~~~~~~~~~~~~~~~~~~Learning Rate: {lr}~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "        # Create a new MLP model with the specified hyperparameters\n",
        "        model = MLP(data, targets, validation_data, validation_targets, num_hidden_layers, num_neurons, l1_lambda, l2_lambda)\n",
        "        accuracy = model.train(batch_size, epochs, lr)\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_l1_lambda = l1_lambda\n",
        "            best_l2_lambda = l2_lambda\n",
        "            best_batch_size = batch_size\n",
        "            best_lr = lr\n",
        "\n",
        "    print(f\"Best L1 Lambda: {best_l1_lambda}\")\n",
        "    print(f\"Best L2 Lambda: {best_l2_lambda}\")\n",
        "    print(f\"Best Batch Size: {best_batch_size}\")\n",
        "    print(f\"Best Learning Rate: {best_lr}\")"
      ],
      "metadata": {
        "id": "R1E2nyLXYT0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FashionMNIST\n",
        "l1_lambdas = [0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "l2_lambdas = [0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "learning_rates = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "FMNIST_training_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                            download=True,\n",
        "                                            train=True,\n",
        "                                            transform=transform_FMNIST)\n",
        "\n",
        "FMINIST_trainloader = torch.utils.data.DataLoader(FMNIST_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "FMNIST_test_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                        download=True,\n",
        "                                        train=False,\n",
        "                                        transform=transform_FMNIST)\n",
        "\n",
        "FMINIST_testloader = torch.utils.data.DataLoader(FMNIST_test_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "find_best_parameters(np.asarray(FMNIST_training_set.data.reshape(-1,28*28)) ,\n",
        "            np.asarray(FMNIST_training_set.targets),\n",
        "            np.asarray(FMNIST_test_set.data.reshape(-1,28*28)),\n",
        "                     np.asarray(FMNIST_test_set.targets),\n",
        "            num_hidden_layers=2,num_neurons=128, l1_lambdas=l1_lambdas, l2_lambdas=l2_lambdas, batch_sizes=batch_sizes, learning_rates=learning_rates, epochs=20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u06oGBBX9Tvh",
        "outputId": "0ac6583f-244e-4a90-d98d-30f2313e3b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-e3b905e1d7b4>:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-3-e3b905e1d7b4>:233: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n",
            "<ipython-input-3-e3b905e1d7b4>:101: RuntimeWarning: overflow encountered in exp\n",
            "  return 1./(1. + np.exp(-x))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: loss = 2.369 | acc = 0.081 | val_loss = 2.344 | val_acc = 0.081 |\n",
            "Epoch 2: loss = 2.329 | acc = 0.082 | val_loss = 2.312 | val_acc = 0.082 |\n",
            "Epoch 3: loss = 2.299 | acc = 0.083 | val_loss = 2.281 | val_acc = 0.083 |\n",
            "Epoch 4: loss = 2.264 | acc = 0.084 | val_loss = 2.242 | val_acc = 0.083 |\n",
            "Epoch 5: loss = 2.225 | acc = 0.085 | val_loss = 2.206 | val_acc = 0.084 |\n",
            "Epoch 6: loss = 2.192 | acc = 0.087 | val_loss = 2.176 | val_acc = 0.087 |\n",
            "Epoch 7: loss = 2.164 | acc = 0.09 | val_loss = 2.151 | val_acc = 0.09 |\n",
            "Epoch 8: loss = 2.139 | acc = 0.094 | val_loss = 2.128 | val_acc = 0.095 |\n",
            "Epoch 9: loss = 2.118 | acc = 0.098 | val_loss = 2.108 | val_acc = 0.099 |\n",
            "Epoch 10: loss = 2.099 | acc = 0.104 | val_loss = 2.089 | val_acc = 0.104 |\n",
            "Epoch 11: loss = 2.079 | acc = 0.108 | val_loss = 2.069 | val_acc = 0.108 |\n",
            "Epoch 12: loss = 2.059 | acc = 0.11 | val_loss = 2.048 | val_acc = 0.11 |\n",
            "Epoch 13: loss = 2.04 | acc = 0.112 | val_loss = 2.032 | val_acc = 0.113 |\n",
            "Epoch 14: loss = 2.024 | acc = 0.113 | val_loss = 2.017 | val_acc = 0.115 |\n",
            "Epoch 15: loss = 2.01 | acc = 0.115 | val_loss = 2.004 | val_acc = 0.117 |\n",
            "Epoch 16: loss = 1.999 | acc = 0.117 | val_loss = 1.994 | val_acc = 0.12 |\n",
            "Epoch 17: loss = 1.988 | acc = 0.119 | val_loss = 1.985 | val_acc = 0.122 |\n",
            "Epoch 18: loss = 1.979 | acc = 0.121 | val_loss = 1.977 | val_acc = 0.122 |\n",
            "Epoch 19: loss = 1.972 | acc = 0.122 | val_loss = 1.97 | val_acc = 0.123 |\n",
            "Epoch 20: loss = 1.964 | acc = 0.122 | val_loss = 1.964 | val_acc = 0.125 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.073 | acc = 0.341 | val_loss = 1.934 | val_acc = 0.494 |\n",
            "Epoch 2: loss = 1.886 | acc = 0.533 | val_loss = 1.857 | val_acc = 0.55 |\n",
            "Epoch 3: loss = 1.838 | acc = 0.572 | val_loss = 1.826 | val_acc = 0.582 |\n",
            "Epoch 4: loss = 1.81 | acc = 0.593 | val_loss = 1.804 | val_acc = 0.598 |\n",
            "Epoch 5: loss = 1.791 | acc = 0.611 | val_loss = 1.79 | val_acc = 0.615 |\n",
            "Epoch 6: loss = 1.779 | acc = 0.623 | val_loss = 1.778 | val_acc = 0.616 |\n",
            "Epoch 7: loss = 1.768 | acc = 0.63 | val_loss = 1.771 | val_acc = 0.628 |\n",
            "Epoch 8: loss = 1.761 | acc = 0.637 | val_loss = 1.762 | val_acc = 0.641 |\n",
            "Epoch 9: loss = 1.755 | acc = 0.643 | val_loss = 1.756 | val_acc = 0.642 |\n",
            "Epoch 10: loss = 1.749 | acc = 0.647 | val_loss = 1.753 | val_acc = 0.645 |\n",
            "Epoch 11: loss = 1.745 | acc = 0.652 | val_loss = 1.75 | val_acc = 0.644 |\n",
            "Epoch 12: loss = 1.742 | acc = 0.654 | val_loss = 1.747 | val_acc = 0.649 |\n",
            "Epoch 13: loss = 1.739 | acc = 0.659 | val_loss = 1.742 | val_acc = 0.652 |\n",
            "Epoch 14: loss = 1.736 | acc = 0.659 | val_loss = 1.74 | val_acc = 0.65 |\n",
            "Epoch 15: loss = 1.733 | acc = 0.657 | val_loss = 1.739 | val_acc = 0.65 |\n",
            "Epoch 16: loss = 1.73 | acc = 0.663 | val_loss = 1.735 | val_acc = 0.66 |\n",
            "Epoch 17: loss = 1.728 | acc = 0.67 | val_loss = 1.733 | val_acc = 0.662 |\n",
            "Epoch 18: loss = 1.727 | acc = 0.671 | val_loss = 1.734 | val_acc = 0.665 |\n",
            "Epoch 19: loss = 1.727 | acc = 0.673 | val_loss = 1.731 | val_acc = 0.668 |\n",
            "Epoch 20: loss = 1.723 | acc = 0.679 | val_loss = 1.727 | val_acc = 0.673 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.838 | acc = 0.562 | val_loss = 1.75 | val_acc = 0.627 |\n",
            "Epoch 2: loss = 1.738 | acc = 0.644 | val_loss = 1.742 | val_acc = 0.626 |\n",
            "Epoch 3: loss = 1.723 | acc = 0.651 | val_loss = 1.719 | val_acc = 0.644 |\n",
            "Epoch 4: loss = 1.709 | acc = 0.667 | val_loss = 1.696 | val_acc = 0.661 |\n",
            "Epoch 5: loss = 1.708 | acc = 0.663 | val_loss = 1.699 | val_acc = 0.652 |\n",
            "Epoch 6: loss = 1.701 | acc = 0.679 | val_loss = 1.703 | val_acc = 0.657 |\n",
            "Epoch 7: loss = 1.721 | acc = 0.639 | val_loss = 1.729 | val_acc = 0.63 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.999 | acc = 0.28 | val_loss = 2.083 | val_acc = 0.232 |\n",
            "Epoch 2: loss = 2.054 | acc = 0.202 | val_loss = 2.217 | val_acc = 0.19 |\n",
            "Epoch 3: loss = 2.147 | acc = 0.158 | val_loss = 2.251 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.195 | acc = 0.159 | val_loss = 2.142 | val_acc = 0.152 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.31 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.3 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.13 | val_loss = 2.303 | val_acc = 0.142 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.14 | val_loss = 2.303 | val_acc = 0.114 |\n",
            "Epoch 5: loss = 2.302 | acc = 0.109 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.318 | acc = 0.138 | val_loss = 2.296 | val_acc = 0.136 |\n",
            "Epoch 2: loss = 2.282 | acc = 0.137 | val_loss = 2.259 | val_acc = 0.136 |\n",
            "Epoch 3: loss = 2.247 | acc = 0.135 | val_loss = 2.227 | val_acc = 0.135 |\n",
            "Epoch 4: loss = 2.216 | acc = 0.133 | val_loss = 2.199 | val_acc = 0.132 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.287 | acc = 0.102 | val_loss = 2.16 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 2.083 | acc = 0.144 | val_loss = 2.026 | val_acc = 0.262 |\n",
            "Epoch 3: loss = 1.971 | acc = 0.418 | val_loss = 1.935 | val_acc = 0.472 |\n",
            "Epoch 4: loss = 1.899 | acc = 0.505 | val_loss = 1.859 | val_acc = 0.556 |\n",
            "Epoch 5: loss = 1.823 | acc = 0.589 | val_loss = 1.804 | val_acc = 0.601 |\n",
            "Epoch 6: loss = 1.785 | acc = 0.615 | val_loss = 1.779 | val_acc = 0.606 |\n",
            "Epoch 7: loss = 1.764 | acc = 0.629 | val_loss = 1.76 | val_acc = 0.632 |\n",
            "Epoch 8: loss = 1.749 | acc = 0.646 | val_loss = 1.749 | val_acc = 0.644 |\n",
            "Epoch 9: loss = 1.738 | acc = 0.656 | val_loss = 1.739 | val_acc = 0.648 |\n",
            "Epoch 10: loss = 1.729 | acc = 0.659 | val_loss = 1.731 | val_acc = 0.657 |\n",
            "Epoch 11: loss = 1.722 | acc = 0.666 | val_loss = 1.724 | val_acc = 0.659 |\n",
            "Epoch 12: loss = 1.715 | acc = 0.673 | val_loss = 1.717 | val_acc = 0.664 |\n",
            "Epoch 13: loss = 1.709 | acc = 0.677 | val_loss = 1.713 | val_acc = 0.665 |\n",
            "Epoch 14: loss = 1.704 | acc = 0.677 | val_loss = 1.708 | val_acc = 0.663 |\n",
            "Epoch 15: loss = 1.699 | acc = 0.679 | val_loss = 1.703 | val_acc = 0.674 |\n",
            "Epoch 16: loss = 1.695 | acc = 0.683 | val_loss = 1.699 | val_acc = 0.674 |\n",
            "Epoch 17: loss = 1.691 | acc = 0.688 | val_loss = 1.695 | val_acc = 0.679 |\n",
            "Epoch 18: loss = 1.688 | acc = 0.69 | val_loss = 1.693 | val_acc = 0.687 |\n",
            "Epoch 19: loss = 1.686 | acc = 0.698 | val_loss = 1.69 | val_acc = 0.691 |\n",
            "Epoch 20: loss = 1.683 | acc = 0.702 | val_loss = 1.688 | val_acc = 0.687 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.949 | acc = 0.476 | val_loss = 1.833 | val_acc = 0.593 |\n",
            "Epoch 2: loss = 1.797 | acc = 0.614 | val_loss = 1.794 | val_acc = 0.61 |\n",
            "Epoch 3: loss = 1.716 | acc = 0.684 | val_loss = 1.698 | val_acc = 0.69 |\n",
            "Epoch 4: loss = 1.687 | acc = 0.692 | val_loss = 1.686 | val_acc = 0.696 |\n",
            "Epoch 5: loss = 1.678 | acc = 0.709 | val_loss = 1.683 | val_acc = 0.7 |\n",
            "Epoch 6: loss = 1.673 | acc = 0.715 | val_loss = 1.687 | val_acc = 0.691 |\n",
            "Epoch 7: loss = 1.679 | acc = 0.707 | val_loss = 1.682 | val_acc = 0.697 |\n",
            "Epoch 8: loss = 1.671 | acc = 0.708 | val_loss = 1.669 | val_acc = 0.702 |\n",
            "Epoch 9: loss = 1.664 | acc = 0.711 | val_loss = 1.669 | val_acc = 0.698 |\n",
            "Epoch 10: loss = 1.658 | acc = 0.726 | val_loss = 1.663 | val_acc = 0.729 |\n",
            "Epoch 11: loss = 1.652 | acc = 0.725 | val_loss = 1.66 | val_acc = 0.714 |\n",
            "Epoch 12: loss = 1.657 | acc = 0.716 | val_loss = 1.657 | val_acc = 0.716 |\n",
            "Epoch 13: loss = 1.647 | acc = 0.724 | val_loss = 1.654 | val_acc = 0.723 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.97 | acc = 0.106 | val_loss = 1.981 | val_acc = 0.111 |\n",
            "Epoch 2: loss = 1.974 | acc = 0.119 | val_loss = 2.088 | val_acc = 0.117 |\n",
            "Epoch 3: loss = 1.994 | acc = 0.212 | val_loss = 1.978 | val_acc = 0.206 |\n",
            "Epoch 4: loss = 2.009 | acc = 0.231 | val_loss = 2.032 | val_acc = 0.203 |\n",
            "Epoch 5: loss = 1.992 | acc = 0.173 | val_loss = 2.006 | val_acc = 0.148 |\n",
            "Epoch 6: loss = 1.98 | acc = 0.208 | val_loss = 1.986 | val_acc = 0.223 |\n",
            "Epoch 7: loss = 1.972 | acc = 0.208 | val_loss = 1.986 | val_acc = 0.2 |\n",
            "Epoch 8: loss = 1.927 | acc = 0.23 | val_loss = 1.867 | val_acc = 0.275 |\n",
            "Epoch 9: loss = 1.932 | acc = 0.268 | val_loss = 1.962 | val_acc = 0.269 |\n",
            "Epoch 10: loss = 2.049 | acc = 0.185 | val_loss = 2.037 | val_acc = 0.147 |\n",
            "Epoch 11: loss = 2.102 | acc = 0.196 | val_loss = 2.29 | val_acc = 0.149 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.344 | acc = 0.144 | val_loss = 2.337 | val_acc = 0.169 |\n",
            "Epoch 2: loss = 2.334 | acc = 0.14 | val_loss = 2.337 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 2.334 | acc = 0.11 | val_loss = 2.337 | val_acc = 0.102 |\n",
            "Epoch 4: loss = 2.31 | acc = 0.161 | val_loss = 2.337 | val_acc = 0.192 |\n",
            "Epoch 5: loss = 2.337 | acc = 0.191 | val_loss = 2.337 | val_acc = 0.191 |\n",
            "Epoch 6: loss = 2.313 | acc = 0.152 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 7: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.36 | acc = 0.152 | val_loss = 2.347 | val_acc = 0.154 |\n",
            "Epoch 2: loss = 2.337 | acc = 0.155 | val_loss = 2.323 | val_acc = 0.157 |\n",
            "Epoch 3: loss = 2.312 | acc = 0.156 | val_loss = 2.299 | val_acc = 0.158 |\n",
            "Epoch 4: loss = 2.287 | acc = 0.156 | val_loss = 2.273 | val_acc = 0.157 |\n",
            "Epoch 5: loss = 2.262 | acc = 0.154 | val_loss = 2.25 | val_acc = 0.156 |\n",
            "Epoch 6: loss = 2.241 | acc = 0.153 | val_loss = 2.231 | val_acc = 0.156 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.209 | acc = 0.124 | val_loss = 2.124 | val_acc = 0.105 |\n",
            "Epoch 2: loss = 2.075 | acc = 0.108 | val_loss = 2.039 | val_acc = 0.108 |\n",
            "Epoch 3: loss = 2.011 | acc = 0.115 | val_loss = 1.991 | val_acc = 0.12 |\n",
            "Epoch 4: loss = 1.972 | acc = 0.129 | val_loss = 1.96 | val_acc = 0.138 |\n",
            "Epoch 5: loss = 1.946 | acc = 0.149 | val_loss = 1.94 | val_acc = 0.161 |\n",
            "Epoch 6: loss = 1.928 | acc = 0.182 | val_loss = 1.924 | val_acc = 0.201 |\n",
            "Epoch 7: loss = 1.912 | acc = 0.235 | val_loss = 1.909 | val_acc = 0.285 |\n",
            "Epoch 8: loss = 1.898 | acc = 0.345 | val_loss = 1.896 | val_acc = 0.419 |\n",
            "Epoch 9: loss = 1.882 | acc = 0.506 | val_loss = 1.871 | val_acc = 0.578 |\n",
            "Epoch 10: loss = 1.854 | acc = 0.607 | val_loss = 1.849 | val_acc = 0.614 |\n",
            "Epoch 11: loss = 1.837 | acc = 0.62 | val_loss = 1.835 | val_acc = 0.62 |\n",
            "Epoch 12: loss = 1.824 | acc = 0.628 | val_loss = 1.826 | val_acc = 0.629 |\n",
            "Epoch 13: loss = 1.815 | acc = 0.634 | val_loss = 1.817 | val_acc = 0.637 |\n",
            "Epoch 14: loss = 1.808 | acc = 0.639 | val_loss = 1.81 | val_acc = 0.638 |\n",
            "Epoch 15: loss = 1.801 | acc = 0.643 | val_loss = 1.804 | val_acc = 0.64 |\n",
            "Epoch 16: loss = 1.795 | acc = 0.649 | val_loss = 1.798 | val_acc = 0.646 |\n",
            "Epoch 17: loss = 1.79 | acc = 0.652 | val_loss = 1.794 | val_acc = 0.649 |\n",
            "Epoch 18: loss = 1.785 | acc = 0.655 | val_loss = 1.791 | val_acc = 0.649 |\n",
            "Epoch 19: loss = 1.781 | acc = 0.658 | val_loss = 1.786 | val_acc = 0.653 |\n",
            "Epoch 20: loss = 1.778 | acc = 0.662 | val_loss = 1.783 | val_acc = 0.657 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.031 | acc = 0.101 | val_loss = 1.889 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 1.847 | acc = 0.1 | val_loss = 1.826 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 1.807 | acc = 0.1 | val_loss = 1.797 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 1.787 | acc = 0.1 | val_loss = 1.788 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 1.777 | acc = 0.104 | val_loss = 1.775 | val_acc = 0.104 |\n",
            "Epoch 6: loss = 1.77 | acc = 0.105 | val_loss = 1.771 | val_acc = 0.106 |\n",
            "Epoch 7: loss = 1.764 | acc = 0.105 | val_loss = 1.767 | val_acc = 0.104 |\n",
            "Epoch 8: loss = 1.761 | acc = 0.106 | val_loss = 1.765 | val_acc = 0.11 |\n",
            "Epoch 9: loss = 1.755 | acc = 0.121 | val_loss = 1.76 | val_acc = 0.115 |\n",
            "Epoch 10: loss = 1.752 | acc = 0.116 | val_loss = 1.758 | val_acc = 0.12 |\n",
            "Epoch 11: loss = 1.753 | acc = 0.111 | val_loss = 1.761 | val_acc = 0.104 |\n",
            "Epoch 12: loss = 1.75 | acc = 0.115 | val_loss = 1.752 | val_acc = 0.119 |\n",
            "Epoch 13: loss = 1.745 | acc = 0.118 | val_loss = 1.755 | val_acc = 0.126 |\n",
            "Epoch 14: loss = 1.746 | acc = 0.128 | val_loss = 1.749 | val_acc = 0.113 |\n",
            "Epoch 15: loss = 1.742 | acc = 0.121 | val_loss = 1.748 | val_acc = 0.137 |\n",
            "Epoch 16: loss = 1.74 | acc = 0.118 | val_loss = 1.745 | val_acc = 0.125 |\n",
            "Epoch 17: loss = 1.739 | acc = 0.117 | val_loss = 1.747 | val_acc = 0.115 |\n",
            "Epoch 18: loss = 1.739 | acc = 0.123 | val_loss = 1.748 | val_acc = 0.114 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.018 | acc = 0.412 | val_loss = 1.956 | val_acc = 0.469 |\n",
            "Epoch 2: loss = 1.97 | acc = 0.434 | val_loss = 1.953 | val_acc = 0.416 |\n",
            "Epoch 3: loss = 1.872 | acc = 0.429 | val_loss = 1.847 | val_acc = 0.424 |\n",
            "Epoch 4: loss = 1.852 | acc = 0.398 | val_loss = 1.879 | val_acc = 0.3 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.246 | acc = 0.14 | val_loss = 2.303 | val_acc = 0.103 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.126 | val_loss = 2.303 | val_acc = 0.203 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.213 | val_loss = 2.302 | val_acc = 0.203 |\n",
            "Epoch 4: loss = 2.285 | acc = 0.181 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.309 | acc = 0.106 | val_loss = 2.303 | val_acc = 0.11 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.362 | acc = 0.11 | val_loss = 2.353 | val_acc = 0.11 |\n",
            "Epoch 2: loss = 2.35 | acc = 0.11 | val_loss = 2.341 | val_acc = 0.11 |\n",
            "Epoch 3: loss = 2.338 | acc = 0.11 | val_loss = 2.329 | val_acc = 0.11 |\n",
            "Epoch 4: loss = 2.326 | acc = 0.11 | val_loss = 2.317 | val_acc = 0.109 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.35 | acc = 0.102 | val_loss = 2.342 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 2.331 | acc = 0.101 | val_loss = 2.323 | val_acc = 0.105 |\n",
            "Epoch 3: loss = 2.314 | acc = 0.102 | val_loss = 2.306 | val_acc = 0.104 |\n",
            "Epoch 4: loss = 2.295 | acc = 0.102 | val_loss = 2.285 | val_acc = 0.104 |\n",
            "Epoch 5: loss = 2.275 | acc = 0.101 | val_loss = 2.268 | val_acc = 0.104 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.136 | acc = 0.212 | val_loss = 1.944 | val_acc = 0.462 |\n",
            "Epoch 2: loss = 1.855 | acc = 0.53 | val_loss = 1.809 | val_acc = 0.564 |\n",
            "Epoch 3: loss = 1.782 | acc = 0.596 | val_loss = 1.766 | val_acc = 0.608 |\n",
            "Epoch 4: loss = 1.751 | acc = 0.627 | val_loss = 1.745 | val_acc = 0.631 |\n",
            "Epoch 5: loss = 1.732 | acc = 0.641 | val_loss = 1.728 | val_acc = 0.642 |\n",
            "Epoch 6: loss = 1.718 | acc = 0.655 | val_loss = 1.717 | val_acc = 0.647 |\n",
            "Epoch 7: loss = 1.708 | acc = 0.667 | val_loss = 1.709 | val_acc = 0.654 |\n",
            "Epoch 8: loss = 1.7 | acc = 0.665 | val_loss = 1.704 | val_acc = 0.651 |\n",
            "Epoch 9: loss = 1.696 | acc = 0.668 | val_loss = 1.7 | val_acc = 0.662 |\n",
            "Epoch 10: loss = 1.691 | acc = 0.676 | val_loss = 1.694 | val_acc = 0.671 |\n",
            "Epoch 11: loss = 1.686 | acc = 0.673 | val_loss = 1.691 | val_acc = 0.659 |\n",
            "Epoch 12: loss = 1.682 | acc = 0.674 | val_loss = 1.684 | val_acc = 0.671 |\n",
            "Epoch 13: loss = 1.677 | acc = 0.686 | val_loss = 1.682 | val_acc = 0.681 |\n",
            "Epoch 14: loss = 1.675 | acc = 0.689 | val_loss = 1.683 | val_acc = 0.692 |\n",
            "Epoch 15: loss = 1.674 | acc = 0.697 | val_loss = 1.681 | val_acc = 0.674 |\n",
            "Epoch 16: loss = 1.672 | acc = 0.686 | val_loss = 1.677 | val_acc = 0.678 |\n",
            "Epoch 17: loss = 1.669 | acc = 0.689 | val_loss = 1.673 | val_acc = 0.683 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.946 | acc = 0.44 | val_loss = 1.893 | val_acc = 0.413 |\n",
            "Epoch 2: loss = 1.878 | acc = 0.48 | val_loss = 1.895 | val_acc = 0.45 |\n",
            "Epoch 3: loss = 1.89 | acc = 0.465 | val_loss = 1.883 | val_acc = 0.486 |\n",
            "Epoch 4: loss = 1.876 | acc = 0.484 | val_loss = 1.875 | val_acc = 0.484 |\n",
            "Epoch 5: loss = 1.863 | acc = 0.488 | val_loss = 1.853 | val_acc = 0.488 |\n",
            "Epoch 6: loss = 1.865 | acc = 0.48 | val_loss = 1.854 | val_acc = 0.486 |\n",
            "Epoch 7: loss = 1.861 | acc = 0.479 | val_loss = 1.887 | val_acc = 0.456 |\n",
            "Epoch 8: loss = 1.855 | acc = 0.501 | val_loss = 1.845 | val_acc = 0.511 |\n",
            "Epoch 9: loss = 1.863 | acc = 0.478 | val_loss = 1.864 | val_acc = 0.457 |\n",
            "Epoch 10: loss = 1.877 | acc = 0.436 | val_loss = 1.892 | val_acc = 0.414 |\n",
            "Epoch 11: loss = 1.884 | acc = 0.388 | val_loss = 1.868 | val_acc = 0.392 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.258 | acc = 0.183 | val_loss = 2.183 | val_acc = 0.23 |\n",
            "Epoch 2: loss = 2.29 | acc = 0.126 | val_loss = 2.284 | val_acc = 0.176 |\n",
            "Epoch 3: loss = 2.301 | acc = 0.12 | val_loss = 2.278 | val_acc = 0.166 |\n",
            "Epoch 4: loss = 2.297 | acc = 0.124 | val_loss = 2.299 | val_acc = 0.101 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.315 | acc = 0.105 | val_loss = 2.265 | val_acc = 0.108 |\n",
            "Epoch 2: loss = 2.229 | acc = 0.114 | val_loss = 2.193 | val_acc = 0.122 |\n",
            "Epoch 3: loss = 2.171 | acc = 0.137 | val_loss = 2.147 | val_acc = 0.154 |\n",
            "Epoch 4: loss = 2.13 | acc = 0.176 | val_loss = 2.111 | val_acc = 0.21 |\n",
            "Epoch 5: loss = 2.092 | acc = 0.243 | val_loss = 2.07 | val_acc = 0.298 |\n",
            "Epoch 6: loss = 2.044 | acc = 0.351 | val_loss = 2.02 | val_acc = 0.408 |\n",
            "Epoch 7: loss = 2.001 | acc = 0.436 | val_loss = 1.982 | val_acc = 0.458 |\n",
            "Epoch 8: loss = 1.968 | acc = 0.474 | val_loss = 1.954 | val_acc = 0.488 |\n",
            "Epoch 9: loss = 1.942 | acc = 0.503 | val_loss = 1.93 | val_acc = 0.511 |\n",
            "Epoch 10: loss = 1.92 | acc = 0.531 | val_loss = 1.911 | val_acc = 0.54 |\n",
            "Epoch 11: loss = 1.902 | acc = 0.552 | val_loss = 1.894 | val_acc = 0.558 |\n",
            "Epoch 12: loss = 1.885 | acc = 0.573 | val_loss = 1.88 | val_acc = 0.575 |\n",
            "Epoch 13: loss = 1.872 | acc = 0.588 | val_loss = 1.868 | val_acc = 0.587 |\n",
            "Epoch 14: loss = 1.859 | acc = 0.6 | val_loss = 1.856 | val_acc = 0.6 |\n",
            "Epoch 15: loss = 1.848 | acc = 0.613 | val_loss = 1.846 | val_acc = 0.611 |\n",
            "Epoch 16: loss = 1.838 | acc = 0.622 | val_loss = 1.836 | val_acc = 0.619 |\n",
            "Epoch 17: loss = 1.83 | acc = 0.631 | val_loss = 1.829 | val_acc = 0.627 |\n",
            "Epoch 18: loss = 1.822 | acc = 0.638 | val_loss = 1.822 | val_acc = 0.634 |\n",
            "Epoch 19: loss = 1.815 | acc = 0.645 | val_loss = 1.816 | val_acc = 0.64 |\n",
            "Epoch 20: loss = 1.808 | acc = 0.651 | val_loss = 1.808 | val_acc = 0.647 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.143 | acc = 0.177 | val_loss = 2.008 | val_acc = 0.446 |\n",
            "Epoch 2: loss = 1.937 | acc = 0.523 | val_loss = 1.866 | val_acc = 0.567 |\n",
            "Epoch 3: loss = 1.831 | acc = 0.607 | val_loss = 1.81 | val_acc = 0.63 |\n",
            "Epoch 4: loss = 1.795 | acc = 0.645 | val_loss = 1.784 | val_acc = 0.656 |\n",
            "Epoch 5: loss = 1.775 | acc = 0.665 | val_loss = 1.77 | val_acc = 0.665 |\n",
            "Epoch 6: loss = 1.762 | acc = 0.677 | val_loss = 1.758 | val_acc = 0.675 |\n",
            "Epoch 7: loss = 1.752 | acc = 0.685 | val_loss = 1.751 | val_acc = 0.687 |\n",
            "Epoch 8: loss = 1.743 | acc = 0.689 | val_loss = 1.743 | val_acc = 0.687 |\n",
            "Epoch 9: loss = 1.736 | acc = 0.695 | val_loss = 1.736 | val_acc = 0.694 |\n",
            "Epoch 10: loss = 1.73 | acc = 0.704 | val_loss = 1.731 | val_acc = 0.699 |\n",
            "Epoch 11: loss = 1.725 | acc = 0.705 | val_loss = 1.727 | val_acc = 0.704 |\n",
            "Epoch 12: loss = 1.722 | acc = 0.713 | val_loss = 1.726 | val_acc = 0.71 |\n",
            "Epoch 13: loss = 1.719 | acc = 0.721 | val_loss = 1.722 | val_acc = 0.72 |\n",
            "Epoch 14: loss = 1.717 | acc = 0.723 | val_loss = 1.719 | val_acc = 0.718 |\n",
            "Epoch 15: loss = 1.713 | acc = 0.722 | val_loss = 1.714 | val_acc = 0.714 |\n",
            "Epoch 16: loss = 1.71 | acc = 0.72 | val_loss = 1.713 | val_acc = 0.712 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.861 | acc = 0.555 | val_loss = 1.797 | val_acc = 0.607 |\n",
            "Epoch 2: loss = 1.76 | acc = 0.6 | val_loss = 1.732 | val_acc = 0.592 |\n",
            "Epoch 3: loss = 1.752 | acc = 0.576 | val_loss = 1.748 | val_acc = 0.587 |\n",
            "Epoch 4: loss = 1.733 | acc = 0.592 | val_loss = 1.737 | val_acc = 0.574 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.052 | acc = 0.255 | val_loss = 2.012 | val_acc = 0.258 |\n",
            "Epoch 2: loss = 2.138 | acc = 0.211 | val_loss = 2.199 | val_acc = 0.213 |\n",
            "Epoch 3: loss = 2.133 | acc = 0.193 | val_loss = 2.383 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.216 | acc = 0.143 | val_loss = 2.229 | val_acc = 0.187 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.311 | acc = 0.179 | val_loss = 2.303 | val_acc = 0.156 |\n",
            "Epoch 2: loss = 2.304 | acc = 0.11 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.311 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.368 | acc = 0.13 | val_loss = 2.35 | val_acc = 0.122 |\n",
            "Epoch 2: loss = 2.336 | acc = 0.113 | val_loss = 2.319 | val_acc = 0.102 |\n",
            "Epoch 3: loss = 2.305 | acc = 0.087 | val_loss = 2.289 | val_acc = 0.076 |\n",
            "Epoch 4: loss = 2.277 | acc = 0.068 | val_loss = 2.262 | val_acc = 0.062 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.213 | acc = 0.063 | val_loss = 2.148 | val_acc = 0.054 |\n",
            "Epoch 2: loss = 2.11 | acc = 0.06 | val_loss = 2.084 | val_acc = 0.078 |\n",
            "Epoch 3: loss = 2.058 | acc = 0.127 | val_loss = 2.031 | val_acc = 0.207 |\n",
            "Epoch 4: loss = 1.967 | acc = 0.342 | val_loss = 1.881 | val_acc = 0.51 |\n",
            "Epoch 5: loss = 1.836 | acc = 0.555 | val_loss = 1.811 | val_acc = 0.583 |\n",
            "Epoch 6: loss = 1.794 | acc = 0.601 | val_loss = 1.783 | val_acc = 0.612 |\n",
            "Epoch 7: loss = 1.769 | acc = 0.627 | val_loss = 1.764 | val_acc = 0.626 |\n",
            "Epoch 8: loss = 1.753 | acc = 0.641 | val_loss = 1.749 | val_acc = 0.642 |\n",
            "Epoch 9: loss = 1.74 | acc = 0.653 | val_loss = 1.738 | val_acc = 0.65 |\n",
            "Epoch 10: loss = 1.731 | acc = 0.66 | val_loss = 1.731 | val_acc = 0.659 |\n",
            "Epoch 11: loss = 1.722 | acc = 0.669 | val_loss = 1.723 | val_acc = 0.661 |\n",
            "Epoch 12: loss = 1.715 | acc = 0.671 | val_loss = 1.717 | val_acc = 0.665 |\n",
            "Epoch 13: loss = 1.708 | acc = 0.678 | val_loss = 1.711 | val_acc = 0.672 |\n",
            "Epoch 14: loss = 1.703 | acc = 0.681 | val_loss = 1.708 | val_acc = 0.67 |\n",
            "Epoch 15: loss = 1.699 | acc = 0.683 | val_loss = 1.702 | val_acc = 0.677 |\n",
            "Epoch 16: loss = 1.694 | acc = 0.687 | val_loss = 1.698 | val_acc = 0.676 |\n",
            "Epoch 17: loss = 1.691 | acc = 0.69 | val_loss = 1.695 | val_acc = 0.679 |\n",
            "Epoch 18: loss = 1.688 | acc = 0.693 | val_loss = 1.692 | val_acc = 0.682 |\n",
            "Epoch 19: loss = 1.685 | acc = 0.695 | val_loss = 1.69 | val_acc = 0.686 |\n",
            "Epoch 20: loss = 1.683 | acc = 0.697 | val_loss = 1.687 | val_acc = 0.689 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.96 | acc = 0.212 | val_loss = 1.794 | val_acc = 0.578 |\n",
            "Epoch 2: loss = 1.746 | acc = 0.647 | val_loss = 1.723 | val_acc = 0.673 |\n",
            "Epoch 3: loss = 1.707 | acc = 0.676 | val_loss = 1.709 | val_acc = 0.664 |\n",
            "Epoch 4: loss = 1.699 | acc = 0.671 | val_loss = 1.701 | val_acc = 0.653 |\n",
            "Epoch 5: loss = 1.693 | acc = 0.674 | val_loss = 1.695 | val_acc = 0.669 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.979 | acc = 0.382 | val_loss = 2.002 | val_acc = 0.351 |\n",
            "Epoch 2: loss = 2.019 | acc = 0.294 | val_loss = 2.035 | val_acc = 0.292 |\n",
            "Epoch 3: loss = 1.963 | acc = 0.264 | val_loss = 1.935 | val_acc = 0.24 |\n",
            "Epoch 4: loss = 1.984 | acc = 0.209 | val_loss = 1.982 | val_acc = 0.216 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.366 | acc = 0.116 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.425 | acc = 0.103 | val_loss = 2.417 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 2.41 | acc = 0.104 | val_loss = 2.401 | val_acc = 0.103 |\n",
            "Epoch 3: loss = 2.392 | acc = 0.104 | val_loss = 2.381 | val_acc = 0.104 |\n",
            "Epoch 4: loss = 2.37 | acc = 0.105 | val_loss = 2.358 | val_acc = 0.106 |\n",
            "Epoch 5: loss = 2.347 | acc = 0.106 | val_loss = 2.336 | val_acc = 0.105 |\n",
            "Epoch 6: loss = 2.327 | acc = 0.107 | val_loss = 2.317 | val_acc = 0.107 |\n",
            "Epoch 7: loss = 2.308 | acc = 0.108 | val_loss = 2.299 | val_acc = 0.107 |\n",
            "Epoch 8: loss = 2.291 | acc = 0.109 | val_loss = 2.282 | val_acc = 0.108 |\n",
            "Epoch 9: loss = 2.275 | acc = 0.109 | val_loss = 2.266 | val_acc = 0.108 |\n",
            "Epoch 10: loss = 2.26 | acc = 0.11 | val_loss = 2.251 | val_acc = 0.108 |\n",
            "Epoch 11: loss = 2.245 | acc = 0.11 | val_loss = 2.235 | val_acc = 0.109 |\n",
            "Epoch 12: loss = 2.229 | acc = 0.11 | val_loss = 2.22 | val_acc = 0.109 |\n",
            "Epoch 13: loss = 2.214 | acc = 0.11 | val_loss = 2.205 | val_acc = 0.109 |\n",
            "Epoch 14: loss = 2.198 | acc = 0.109 | val_loss = 2.189 | val_acc = 0.109 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.234 | acc = 0.247 | val_loss = 2.145 | val_acc = 0.341 |\n",
            "Epoch 2: loss = 2.09 | acc = 0.387 | val_loss = 2.05 | val_acc = 0.417 |\n",
            "Epoch 3: loss = 2.017 | acc = 0.454 | val_loss = 1.996 | val_acc = 0.466 |\n",
            "Epoch 4: loss = 1.972 | acc = 0.491 | val_loss = 1.96 | val_acc = 0.495 |\n",
            "Epoch 5: loss = 1.942 | acc = 0.517 | val_loss = 1.934 | val_acc = 0.52 |\n",
            "Epoch 6: loss = 1.919 | acc = 0.539 | val_loss = 1.916 | val_acc = 0.533 |\n",
            "Epoch 7: loss = 1.903 | acc = 0.551 | val_loss = 1.903 | val_acc = 0.545 |\n",
            "Epoch 8: loss = 1.891 | acc = 0.56 | val_loss = 1.89 | val_acc = 0.557 |\n",
            "Epoch 9: loss = 1.879 | acc = 0.569 | val_loss = 1.88 | val_acc = 0.566 |\n",
            "Epoch 10: loss = 1.87 | acc = 0.577 | val_loss = 1.872 | val_acc = 0.568 |\n",
            "Epoch 11: loss = 1.849 | acc = 0.589 | val_loss = 1.827 | val_acc = 0.612 |\n",
            "Epoch 12: loss = 1.809 | acc = 0.645 | val_loss = 1.807 | val_acc = 0.647 |\n",
            "Epoch 13: loss = 1.796 | acc = 0.661 | val_loss = 1.797 | val_acc = 0.654 |\n",
            "Epoch 14: loss = 1.787 | acc = 0.667 | val_loss = 1.791 | val_acc = 0.66 |\n",
            "Epoch 15: loss = 1.781 | acc = 0.674 | val_loss = 1.784 | val_acc = 0.668 |\n",
            "Epoch 16: loss = 1.776 | acc = 0.679 | val_loss = 1.78 | val_acc = 0.672 |\n",
            "Epoch 17: loss = 1.771 | acc = 0.682 | val_loss = 1.775 | val_acc = 0.674 |\n",
            "Epoch 18: loss = 1.766 | acc = 0.686 | val_loss = 1.771 | val_acc = 0.678 |\n",
            "Epoch 19: loss = 1.762 | acc = 0.687 | val_loss = 1.767 | val_acc = 0.676 |\n",
            "Epoch 20: loss = 1.759 | acc = 0.688 | val_loss = 1.764 | val_acc = 0.68 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.991 | acc = 0.436 | val_loss = 1.844 | val_acc = 0.565 |\n",
            "Epoch 2: loss = 1.813 | acc = 0.599 | val_loss = 1.803 | val_acc = 0.601 |\n",
            "Epoch 3: loss = 1.78 | acc = 0.627 | val_loss = 1.777 | val_acc = 0.624 |\n",
            "Epoch 4: loss = 1.763 | acc = 0.638 | val_loss = 1.762 | val_acc = 0.639 |\n",
            "Epoch 5: loss = 1.751 | acc = 0.65 | val_loss = 1.756 | val_acc = 0.641 |\n",
            "Epoch 6: loss = 1.745 | acc = 0.653 | val_loss = 1.748 | val_acc = 0.648 |\n",
            "Epoch 7: loss = 1.738 | acc = 0.649 | val_loss = 1.744 | val_acc = 0.643 |\n",
            "Epoch 8: loss = 1.732 | acc = 0.652 | val_loss = 1.737 | val_acc = 0.655 |\n",
            "Epoch 9: loss = 1.731 | acc = 0.66 | val_loss = 1.736 | val_acc = 0.648 |\n",
            "Epoch 10: loss = 1.729 | acc = 0.653 | val_loss = 1.735 | val_acc = 0.633 |\n",
            "Epoch 11: loss = 1.726 | acc = 0.65 | val_loss = 1.73 | val_acc = 0.643 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.861 | acc = 0.46 | val_loss = 1.813 | val_acc = 0.47 |\n",
            "Epoch 2: loss = 1.812 | acc = 0.461 | val_loss = 1.801 | val_acc = 0.397 |\n",
            "Epoch 3: loss = 1.892 | acc = 0.359 | val_loss = 1.91 | val_acc = 0.39 |\n",
            "Epoch 4: loss = 1.838 | acc = 0.352 | val_loss = 1.828 | val_acc = 0.38 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.25 | acc = 0.155 | val_loss = 2.183 | val_acc = 0.199 |\n",
            "Epoch 2: loss = 2.294 | acc = 0.119 | val_loss = 2.302 | val_acc = 0.131 |\n",
            "Epoch 3: loss = 2.278 | acc = 0.134 | val_loss = 2.24 | val_acc = 0.152 |\n",
            "Epoch 4: loss = 2.286 | acc = 0.152 | val_loss = 2.303 | val_acc = 0.18 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.133 | val_loss = 2.306 | val_acc = 0.141 |\n",
            "Epoch 2: loss = 2.298 | acc = 0.136 | val_loss = 2.292 | val_acc = 0.142 |\n",
            "Epoch 3: loss = 2.284 | acc = 0.138 | val_loss = 2.279 | val_acc = 0.144 |\n",
            "Epoch 4: loss = 2.271 | acc = 0.141 | val_loss = 2.266 | val_acc = 0.146 |\n",
            "Epoch 5: loss = 2.258 | acc = 0.145 | val_loss = 2.253 | val_acc = 0.148 |\n",
            "Epoch 6: loss = 2.246 | acc = 0.148 | val_loss = 2.241 | val_acc = 0.153 |\n",
            "Epoch 7: loss = 2.234 | acc = 0.153 | val_loss = 2.23 | val_acc = 0.159 |\n",
            "Epoch 8: loss = 2.223 | acc = 0.158 | val_loss = 2.219 | val_acc = 0.165 |\n",
            "Epoch 9: loss = 2.212 | acc = 0.164 | val_loss = 2.208 | val_acc = 0.171 |\n",
            "Epoch 10: loss = 2.201 | acc = 0.172 | val_loss = 2.197 | val_acc = 0.179 |\n",
            "Epoch 11: loss = 2.19 | acc = 0.179 | val_loss = 2.186 | val_acc = 0.188 |\n",
            "Epoch 12: loss = 2.18 | acc = 0.188 | val_loss = 2.176 | val_acc = 0.194 |\n",
            "Epoch 13: loss = 2.169 | acc = 0.195 | val_loss = 2.166 | val_acc = 0.205 |\n",
            "Epoch 14: loss = 2.159 | acc = 0.203 | val_loss = 2.155 | val_acc = 0.212 |\n",
            "Epoch 15: loss = 2.149 | acc = 0.211 | val_loss = 2.145 | val_acc = 0.219 |\n",
            "Epoch 16: loss = 2.139 | acc = 0.219 | val_loss = 2.136 | val_acc = 0.226 |\n",
            "Epoch 17: loss = 2.13 | acc = 0.226 | val_loss = 2.127 | val_acc = 0.232 |\n",
            "Epoch 18: loss = 2.121 | acc = 0.234 | val_loss = 2.118 | val_acc = 0.24 |\n",
            "Epoch 19: loss = 2.113 | acc = 0.242 | val_loss = 2.11 | val_acc = 0.248 |\n",
            "Epoch 20: loss = 2.105 | acc = 0.251 | val_loss = 2.103 | val_acc = 0.254 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.345 | acc = 0.101 | val_loss = 2.313 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 2.288 | acc = 0.103 | val_loss = 2.269 | val_acc = 0.104 |\n",
            "Epoch 3: loss = 2.247 | acc = 0.112 | val_loss = 2.227 | val_acc = 0.117 |\n",
            "Epoch 4: loss = 2.2 | acc = 0.131 | val_loss = 2.171 | val_acc = 0.149 |\n",
            "Epoch 5: loss = 2.143 | acc = 0.187 | val_loss = 2.117 | val_acc = 0.236 |\n",
            "Epoch 6: loss = 2.09 | acc = 0.302 | val_loss = 2.065 | val_acc = 0.352 |\n",
            "Epoch 7: loss = 2.037 | acc = 0.385 | val_loss = 2.016 | val_acc = 0.407 |\n",
            "Epoch 8: loss = 1.996 | acc = 0.427 | val_loss = 1.982 | val_acc = 0.447 |\n",
            "Epoch 9: loss = 1.968 | acc = 0.462 | val_loss = 1.96 | val_acc = 0.474 |\n",
            "Epoch 10: loss = 1.946 | acc = 0.484 | val_loss = 1.939 | val_acc = 0.498 |\n",
            "Epoch 11: loss = 1.927 | acc = 0.507 | val_loss = 1.922 | val_acc = 0.513 |\n",
            "Epoch 12: loss = 1.912 | acc = 0.524 | val_loss = 1.908 | val_acc = 0.528 |\n",
            "Epoch 13: loss = 1.899 | acc = 0.535 | val_loss = 1.897 | val_acc = 0.541 |\n",
            "Epoch 14: loss = 1.888 | acc = 0.548 | val_loss = 1.888 | val_acc = 0.552 |\n",
            "Epoch 15: loss = 1.879 | acc = 0.558 | val_loss = 1.879 | val_acc = 0.561 |\n",
            "Epoch 16: loss = 1.871 | acc = 0.566 | val_loss = 1.871 | val_acc = 0.57 |\n",
            "Epoch 17: loss = 1.863 | acc = 0.574 | val_loss = 1.863 | val_acc = 0.577 |\n",
            "Epoch 18: loss = 1.857 | acc = 0.583 | val_loss = 1.857 | val_acc = 0.584 |\n",
            "Epoch 19: loss = 1.851 | acc = 0.589 | val_loss = 1.852 | val_acc = 0.589 |\n",
            "Epoch 20: loss = 1.845 | acc = 0.593 | val_loss = 1.847 | val_acc = 0.593 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.042 | acc = 0.15 | val_loss = 1.921 | val_acc = 0.332 |\n",
            "Epoch 2: loss = 1.841 | acc = 0.556 | val_loss = 1.803 | val_acc = 0.612 |\n",
            "Epoch 3: loss = 1.775 | acc = 0.639 | val_loss = 1.761 | val_acc = 0.643 |\n",
            "Epoch 4: loss = 1.744 | acc = 0.667 | val_loss = 1.736 | val_acc = 0.663 |\n",
            "Epoch 5: loss = 1.722 | acc = 0.683 | val_loss = 1.72 | val_acc = 0.674 |\n",
            "Epoch 6: loss = 1.708 | acc = 0.683 | val_loss = 1.707 | val_acc = 0.682 |\n",
            "Epoch 7: loss = 1.695 | acc = 0.695 | val_loss = 1.698 | val_acc = 0.689 |\n",
            "Epoch 8: loss = 1.687 | acc = 0.7 | val_loss = 1.687 | val_acc = 0.691 |\n",
            "Epoch 9: loss = 1.679 | acc = 0.709 | val_loss = 1.682 | val_acc = 0.7 |\n",
            "Epoch 10: loss = 1.673 | acc = 0.712 | val_loss = 1.675 | val_acc = 0.699 |\n",
            "Epoch 11: loss = 1.669 | acc = 0.713 | val_loss = 1.671 | val_acc = 0.71 |\n",
            "Epoch 12: loss = 1.665 | acc = 0.715 | val_loss = 1.667 | val_acc = 0.706 |\n",
            "Epoch 13: loss = 1.66 | acc = 0.715 | val_loss = 1.664 | val_acc = 0.708 |\n",
            "Epoch 14: loss = 1.657 | acc = 0.719 | val_loss = 1.661 | val_acc = 0.714 |\n",
            "Epoch 15: loss = 1.654 | acc = 0.721 | val_loss = 1.661 | val_acc = 0.711 |\n",
            "Epoch 16: loss = 1.653 | acc = 0.725 | val_loss = 1.658 | val_acc = 0.719 |\n",
            "Epoch 17: loss = 1.65 | acc = 0.729 | val_loss = 1.655 | val_acc = 0.718 |\n",
            "Epoch 18: loss = 1.648 | acc = 0.729 | val_loss = 1.655 | val_acc = 0.717 |\n",
            "Epoch 19: loss = 1.646 | acc = 0.732 | val_loss = 1.652 | val_acc = 0.726 |\n",
            "Epoch 20: loss = 1.644 | acc = 0.738 | val_loss = 1.65 | val_acc = 0.726 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.965 | acc = 0.101 | val_loss = 1.852 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 1.833 | acc = 0.102 | val_loss = 1.848 | val_acc = 0.103 |\n",
            "Epoch 3: loss = 1.836 | acc = 0.103 | val_loss = 1.838 | val_acc = 0.103 |\n",
            "Epoch 4: loss = 1.813 | acc = 0.104 | val_loss = 1.81 | val_acc = 0.106 |\n",
            "Epoch 5: loss = 1.802 | acc = 0.109 | val_loss = 1.809 | val_acc = 0.118 |\n",
            "Epoch 6: loss = 1.808 | acc = 0.112 | val_loss = 1.848 | val_acc = 0.131 |\n",
            "Epoch 7: loss = 1.83 | acc = 0.113 | val_loss = 1.833 | val_acc = 0.12 |\n",
            "Epoch 8: loss = 1.823 | acc = 0.141 | val_loss = 1.804 | val_acc = 0.126 |\n",
            "Epoch 9: loss = 1.805 | acc = 0.14 | val_loss = 1.827 | val_acc = 0.151 |\n",
            "Epoch 10: loss = 1.801 | acc = 0.16 | val_loss = 1.805 | val_acc = 0.203 |\n",
            "Epoch 11: loss = 1.798 | acc = 0.175 | val_loss = 1.791 | val_acc = 0.191 |\n",
            "Epoch 12: loss = 1.748 | acc = 0.486 | val_loss = 1.71 | val_acc = 0.554 |\n",
            "Epoch 13: loss = 1.74 | acc = 0.559 | val_loss = 1.721 | val_acc = 0.609 |\n",
            "Epoch 14: loss = 1.746 | acc = 0.57 | val_loss = 1.731 | val_acc = 0.597 |\n",
            "Epoch 15: loss = 1.741 | acc = 0.559 | val_loss = 1.736 | val_acc = 0.572 |\n",
            "Epoch 16: loss = 1.731 | acc = 0.583 | val_loss = 1.728 | val_acc = 0.588 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.139 | acc = 0.165 | val_loss = 2.124 | val_acc = 0.242 |\n",
            "Epoch 2: loss = 2.322 | acc = 0.116 | val_loss = 2.392 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.392 | acc = 0.1 | val_loss = 2.392 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.393 | acc = 0.1 | val_loss = 2.392 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.363 | acc = 0.102 | val_loss = 2.327 | val_acc = 0.105 |\n",
            "Epoch 2: loss = 2.299 | acc = 0.105 | val_loss = 2.272 | val_acc = 0.108 |\n",
            "Epoch 3: loss = 2.249 | acc = 0.109 | val_loss = 2.224 | val_acc = 0.114 |\n",
            "Epoch 4: loss = 2.2 | acc = 0.117 | val_loss = 2.173 | val_acc = 0.129 |\n",
            "Epoch 5: loss = 2.142 | acc = 0.151 | val_loss = 2.112 | val_acc = 0.187 |\n",
            "Epoch 6: loss = 2.089 | acc = 0.264 | val_loss = 2.073 | val_acc = 0.366 |\n",
            "Epoch 7: loss = 2.057 | acc = 0.449 | val_loss = 2.045 | val_acc = 0.502 |\n",
            "Epoch 8: loss = 2.033 | acc = 0.542 | val_loss = 2.026 | val_acc = 0.564 |\n",
            "Epoch 9: loss = 2.017 | acc = 0.582 | val_loss = 2.013 | val_acc = 0.591 |\n",
            "Epoch 10: loss = 2.005 | acc = 0.609 | val_loss = 2.003 | val_acc = 0.609 |\n",
            "Epoch 11: loss = 1.997 | acc = 0.628 | val_loss = 1.997 | val_acc = 0.625 |\n",
            "Epoch 12: loss = 1.993 | acc = 0.64 | val_loss = 1.994 | val_acc = 0.637 |\n",
            "Epoch 13: loss = 1.99 | acc = 0.653 | val_loss = 1.992 | val_acc = 0.644 |\n",
            "Epoch 14: loss = 1.989 | acc = 0.662 | val_loss = 1.992 | val_acc = 0.66 |\n",
            "Epoch 15: loss = 1.989 | acc = 0.673 | val_loss = 1.992 | val_acc = 0.661 |\n",
            "Epoch 16: loss = 1.989 | acc = 0.68 | val_loss = 1.993 | val_acc = 0.672 |\n",
            "Epoch 17: loss = 1.991 | acc = 0.685 | val_loss = 1.994 | val_acc = 0.675 |\n",
            "Epoch 18: loss = 1.993 | acc = 0.689 | val_loss = 1.996 | val_acc = 0.678 |\n",
            "Epoch 19: loss = 1.994 | acc = 0.696 | val_loss = 1.998 | val_acc = 0.681 |\n",
            "Epoch 20: loss = 1.996 | acc = 0.698 | val_loss = 1.999 | val_acc = 0.68 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.192 | acc = 0.1 | val_loss = 2.071 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 1.984 | acc = 0.1 | val_loss = 1.937 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 1.904 | acc = 0.1 | val_loss = 1.886 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 1.869 | acc = 0.1 | val_loss = 1.863 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 1.849 | acc = 0.165 | val_loss = 1.815 | val_acc = 0.588 |\n",
            "Epoch 6: loss = 1.769 | acc = 0.657 | val_loss = 1.758 | val_acc = 0.667 |\n",
            "Epoch 7: loss = 1.747 | acc = 0.679 | val_loss = 1.744 | val_acc = 0.678 |\n",
            "Epoch 8: loss = 1.736 | acc = 0.689 | val_loss = 1.734 | val_acc = 0.687 |\n",
            "Epoch 9: loss = 1.728 | acc = 0.705 | val_loss = 1.731 | val_acc = 0.698 |\n",
            "Epoch 10: loss = 1.723 | acc = 0.709 | val_loss = 1.723 | val_acc = 0.703 |\n",
            "Epoch 11: loss = 1.717 | acc = 0.712 | val_loss = 1.719 | val_acc = 0.7 |\n",
            "Epoch 12: loss = 1.714 | acc = 0.714 | val_loss = 1.718 | val_acc = 0.709 |\n",
            "Epoch 13: loss = 1.71 | acc = 0.719 | val_loss = 1.713 | val_acc = 0.71 |\n",
            "Epoch 14: loss = 1.708 | acc = 0.721 | val_loss = 1.712 | val_acc = 0.709 |\n",
            "Epoch 15: loss = 1.706 | acc = 0.718 | val_loss = 1.711 | val_acc = 0.707 |\n",
            "Epoch 16: loss = 1.704 | acc = 0.721 | val_loss = 1.706 | val_acc = 0.716 |\n",
            "Epoch 17: loss = 1.702 | acc = 0.724 | val_loss = 1.705 | val_acc = 0.716 |\n",
            "Epoch 18: loss = 1.7 | acc = 0.727 | val_loss = 1.703 | val_acc = 0.723 |\n",
            "Epoch 19: loss = 1.698 | acc = 0.727 | val_loss = 1.701 | val_acc = 0.72 |\n",
            "Epoch 20: loss = 1.696 | acc = 0.732 | val_loss = 1.699 | val_acc = 0.721 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.842 | acc = 0.556 | val_loss = 1.768 | val_acc = 0.61 |\n",
            "Epoch 2: loss = 1.755 | acc = 0.641 | val_loss = 1.751 | val_acc = 0.644 |\n",
            "Epoch 3: loss = 1.743 | acc = 0.657 | val_loss = 1.732 | val_acc = 0.653 |\n",
            "Epoch 4: loss = 1.737 | acc = 0.663 | val_loss = 1.74 | val_acc = 0.63 |\n",
            "Epoch 5: loss = 1.735 | acc = 0.646 | val_loss = 1.721 | val_acc = 0.673 |\n",
            "Epoch 6: loss = 1.717 | acc = 0.662 | val_loss = 1.745 | val_acc = 0.615 |\n",
            "Epoch 7: loss = 1.731 | acc = 0.656 | val_loss = 1.715 | val_acc = 0.675 |\n",
            "Epoch 8: loss = 1.728 | acc = 0.654 | val_loss = 1.729 | val_acc = 0.663 |\n",
            "Epoch 9: loss = 1.715 | acc = 0.662 | val_loss = 1.755 | val_acc = 0.622 |\n",
            "Epoch 10: loss = 1.726 | acc = 0.664 | val_loss = 1.732 | val_acc = 0.643 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.998 | acc = 0.278 | val_loss = 2.04 | val_acc = 0.228 |\n",
            "Epoch 2: loss = 2.108 | acc = 0.2 | val_loss = 2.289 | val_acc = 0.132 |\n",
            "Epoch 3: loss = 2.049 | acc = 0.228 | val_loss = 1.984 | val_acc = 0.186 |\n",
            "Epoch 4: loss = 2.057 | acc = 0.187 | val_loss = 2.123 | val_acc = 0.102 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.362 | acc = 0.12 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.339 | acc = 0.1 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.327 | acc = 0.11 | val_loss = 2.303 | val_acc = 0.125 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.122 | val_loss = 2.303 | val_acc = 0.112 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.109 | val_loss = 2.303 | val_acc = 0.107 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.106 | val_loss = 2.303 | val_acc = 0.105 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.329 | acc = 0.069 | val_loss = 2.308 | val_acc = 0.069 |\n",
            "Epoch 2: loss = 2.294 | acc = 0.072 | val_loss = 2.276 | val_acc = 0.074 |\n",
            "Epoch 3: loss = 2.262 | acc = 0.079 | val_loss = 2.246 | val_acc = 0.082 |\n",
            "Epoch 4: loss = 2.234 | acc = 0.087 | val_loss = 2.223 | val_acc = 0.09 |\n",
            "Epoch 5: loss = 2.211 | acc = 0.093 | val_loss = 2.203 | val_acc = 0.095 |\n",
            "Epoch 6: loss = 2.194 | acc = 0.097 | val_loss = 2.187 | val_acc = 0.099 |\n",
            "Epoch 7: loss = 2.179 | acc = 0.1 | val_loss = 2.173 | val_acc = 0.102 |\n",
            "Epoch 8: loss = 2.166 | acc = 0.102 | val_loss = 2.161 | val_acc = 0.103 |\n",
            "Epoch 9: loss = 2.155 | acc = 0.103 | val_loss = 2.151 | val_acc = 0.103 |\n",
            "Epoch 10: loss = 2.144 | acc = 0.103 | val_loss = 2.141 | val_acc = 0.103 |\n",
            "Epoch 11: loss = 2.136 | acc = 0.103 | val_loss = 2.134 | val_acc = 0.104 |\n",
            "Epoch 12: loss = 2.128 | acc = 0.103 | val_loss = 2.126 | val_acc = 0.104 |\n",
            "Epoch 13: loss = 2.121 | acc = 0.104 | val_loss = 2.119 | val_acc = 0.103 |\n",
            "Epoch 14: loss = 2.114 | acc = 0.104 | val_loss = 2.114 | val_acc = 0.104 |\n",
            "Epoch 15: loss = 2.109 | acc = 0.105 | val_loss = 2.109 | val_acc = 0.106 |\n",
            "Epoch 16: loss = 2.104 | acc = 0.106 | val_loss = 2.105 | val_acc = 0.108 |\n",
            "Epoch 17: loss = 2.1 | acc = 0.11 | val_loss = 2.101 | val_acc = 0.115 |\n",
            "Epoch 18: loss = 2.095 | acc = 0.128 | val_loss = 2.096 | val_acc = 0.153 |\n",
            "Epoch 19: loss = 2.089 | acc = 0.209 | val_loss = 2.089 | val_acc = 0.274 |\n",
            "Epoch 20: loss = 2.08 | acc = 0.375 | val_loss = 2.077 | val_acc = 0.456 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.141 | acc = 0.169 | val_loss = 2.025 | val_acc = 0.268 |\n",
            "Epoch 2: loss = 1.953 | acc = 0.402 | val_loss = 1.904 | val_acc = 0.518 |\n",
            "Epoch 3: loss = 1.862 | acc = 0.569 | val_loss = 1.835 | val_acc = 0.6 |\n",
            "Epoch 4: loss = 1.812 | acc = 0.617 | val_loss = 1.801 | val_acc = 0.625 |\n",
            "Epoch 5: loss = 1.785 | acc = 0.641 | val_loss = 1.782 | val_acc = 0.646 |\n",
            "Epoch 6: loss = 1.769 | acc = 0.659 | val_loss = 1.769 | val_acc = 0.665 |\n",
            "Epoch 7: loss = 1.758 | acc = 0.672 | val_loss = 1.757 | val_acc = 0.671 |\n",
            "Epoch 8: loss = 1.748 | acc = 0.686 | val_loss = 1.749 | val_acc = 0.684 |\n",
            "Epoch 9: loss = 1.74 | acc = 0.695 | val_loss = 1.743 | val_acc = 0.694 |\n",
            "Epoch 10: loss = 1.734 | acc = 0.699 | val_loss = 1.737 | val_acc = 0.694 |\n",
            "Epoch 11: loss = 1.729 | acc = 0.703 | val_loss = 1.733 | val_acc = 0.701 |\n",
            "Epoch 12: loss = 1.725 | acc = 0.709 | val_loss = 1.728 | val_acc = 0.703 |\n",
            "Epoch 13: loss = 1.721 | acc = 0.716 | val_loss = 1.726 | val_acc = 0.706 |\n",
            "Epoch 14: loss = 1.718 | acc = 0.719 | val_loss = 1.724 | val_acc = 0.715 |\n",
            "Epoch 15: loss = 1.716 | acc = 0.726 | val_loss = 1.721 | val_acc = 0.718 |\n",
            "Epoch 16: loss = 1.713 | acc = 0.729 | val_loss = 1.718 | val_acc = 0.718 |\n",
            "Epoch 17: loss = 1.711 | acc = 0.729 | val_loss = 1.715 | val_acc = 0.721 |\n",
            "Epoch 18: loss = 1.709 | acc = 0.729 | val_loss = 1.714 | val_acc = 0.72 |\n",
            "Epoch 19: loss = 1.707 | acc = 0.731 | val_loss = 1.713 | val_acc = 0.715 |\n",
            "Epoch 20: loss = 1.705 | acc = 0.731 | val_loss = 1.711 | val_acc = 0.725 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.081 | acc = 0.101 | val_loss = 1.988 | val_acc = 0.103 |\n",
            "Epoch 2: loss = 1.963 | acc = 0.109 | val_loss = 1.945 | val_acc = 0.114 |\n",
            "Epoch 3: loss = 1.88 | acc = 0.115 | val_loss = 1.853 | val_acc = 0.109 |\n",
            "Epoch 4: loss = 1.846 | acc = 0.113 | val_loss = 1.845 | val_acc = 0.113 |\n",
            "Epoch 5: loss = 1.838 | acc = 0.11 | val_loss = 1.843 | val_acc = 0.118 |\n",
            "Epoch 6: loss = 1.835 | acc = 0.129 | val_loss = 1.836 | val_acc = 0.16 |\n",
            "Epoch 7: loss = 1.829 | acc = 0.166 | val_loss = 1.829 | val_acc = 0.175 |\n",
            "Epoch 8: loss = 1.825 | acc = 0.2 | val_loss = 1.83 | val_acc = 0.191 |\n",
            "Epoch 9: loss = 1.823 | acc = 0.213 | val_loss = 1.829 | val_acc = 0.273 |\n",
            "Epoch 10: loss = 1.782 | acc = 0.503 | val_loss = 1.708 | val_acc = 0.68 |\n",
            "Epoch 11: loss = 1.688 | acc = 0.705 | val_loss = 1.695 | val_acc = 0.686 |\n",
            "Epoch 12: loss = 1.685 | acc = 0.71 | val_loss = 1.684 | val_acc = 0.708 |\n",
            "Epoch 13: loss = 1.674 | acc = 0.725 | val_loss = 1.675 | val_acc = 0.709 |\n",
            "Epoch 14: loss = 1.675 | acc = 0.728 | val_loss = 1.682 | val_acc = 0.722 |\n",
            "Epoch 15: loss = 1.669 | acc = 0.731 | val_loss = 1.676 | val_acc = 0.712 |\n",
            "Epoch 16: loss = 1.664 | acc = 0.725 | val_loss = 1.666 | val_acc = 0.71 |\n",
            "Epoch 17: loss = 1.664 | acc = 0.731 | val_loss = 1.676 | val_acc = 0.733 |\n",
            "Epoch 18: loss = 1.666 | acc = 0.736 | val_loss = 1.665 | val_acc = 0.728 |\n",
            "Epoch 19: loss = 1.66 | acc = 0.733 | val_loss = 1.681 | val_acc = 0.705 |\n",
            "Epoch 20: loss = 1.666 | acc = 0.719 | val_loss = 1.67 | val_acc = 0.72 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.93 | acc = 0.39 | val_loss = 1.913 | val_acc = 0.378 |\n",
            "Epoch 2: loss = 1.92 | acc = 0.327 | val_loss = 1.931 | val_acc = 0.28 |\n",
            "Epoch 3: loss = 2.005 | acc = 0.224 | val_loss = 2.041 | val_acc = 0.282 |\n",
            "Epoch 4: loss = 1.99 | acc = 0.24 | val_loss = 1.879 | val_acc = 0.236 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.308 | acc = 0.129 | val_loss = 2.3 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.302 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.104 | val_loss = 2.303 | val_acc = 0.109 |\n",
            "Epoch 4: loss = 2.302 | acc = 0.115 | val_loss = 2.303 | val_acc = 0.121 |\n",
            "Epoch 5: loss = 2.288 | acc = 0.171 | val_loss = 2.283 | val_acc = 0.199 |\n",
            "Epoch 6: loss = 2.302 | acc = 0.128 | val_loss = 2.303 | val_acc = 0.124 |\n",
            "Epoch 7: loss = 2.302 | acc = 0.18 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 8: loss = 2.302 | acc = 0.118 | val_loss = 2.303 | val_acc = 0.136 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.367 | acc = 0.133 | val_loss = 2.36 | val_acc = 0.132 |\n",
            "Epoch 2: loss = 2.357 | acc = 0.133 | val_loss = 2.349 | val_acc = 0.133 |\n",
            "Epoch 3: loss = 2.345 | acc = 0.135 | val_loss = 2.337 | val_acc = 0.135 |\n",
            "Epoch 4: loss = 2.331 | acc = 0.137 | val_loss = 2.321 | val_acc = 0.137 |\n",
            "Epoch 5: loss = 2.313 | acc = 0.141 | val_loss = 2.301 | val_acc = 0.147 |\n",
            "Epoch 6: loss = 2.291 | acc = 0.15 | val_loss = 2.278 | val_acc = 0.16 |\n",
            "Epoch 7: loss = 2.265 | acc = 0.166 | val_loss = 2.25 | val_acc = 0.184 |\n",
            "Epoch 8: loss = 2.236 | acc = 0.197 | val_loss = 2.219 | val_acc = 0.219 |\n",
            "Epoch 9: loss = 2.206 | acc = 0.232 | val_loss = 2.192 | val_acc = 0.252 |\n",
            "Epoch 10: loss = 2.183 | acc = 0.263 | val_loss = 2.173 | val_acc = 0.276 |\n",
            "Epoch 11: loss = 2.166 | acc = 0.286 | val_loss = 2.16 | val_acc = 0.295 |\n",
            "Epoch 12: loss = 2.154 | acc = 0.306 | val_loss = 2.148 | val_acc = 0.316 |\n",
            "Epoch 13: loss = 2.143 | acc = 0.323 | val_loss = 2.139 | val_acc = 0.329 |\n",
            "Epoch 14: loss = 2.134 | acc = 0.338 | val_loss = 2.131 | val_acc = 0.346 |\n",
            "Epoch 15: loss = 2.125 | acc = 0.351 | val_loss = 2.122 | val_acc = 0.358 |\n",
            "Epoch 16: loss = 2.117 | acc = 0.362 | val_loss = 2.114 | val_acc = 0.37 |\n",
            "Epoch 17: loss = 2.109 | acc = 0.373 | val_loss = 2.105 | val_acc = 0.383 |\n",
            "Epoch 18: loss = 2.1 | acc = 0.384 | val_loss = 2.096 | val_acc = 0.392 |\n",
            "Epoch 19: loss = 2.09 | acc = 0.399 | val_loss = 2.087 | val_acc = 0.404 |\n",
            "Epoch 20: loss = 2.08 | acc = 0.415 | val_loss = 2.078 | val_acc = 0.419 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.328 | acc = 0.1 | val_loss = 2.296 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.273 | acc = 0.1 | val_loss = 2.258 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.239 | acc = 0.1 | val_loss = 2.223 | val_acc = 0.101 |\n",
            "Epoch 4: loss = 2.209 | acc = 0.101 | val_loss = 2.193 | val_acc = 0.101 |\n",
            "Epoch 5: loss = 2.163 | acc = 0.102 | val_loss = 2.146 | val_acc = 0.102 |\n",
            "Epoch 6: loss = 2.133 | acc = 0.102 | val_loss = 2.122 | val_acc = 0.103 |\n",
            "Epoch 7: loss = 2.098 | acc = 0.103 | val_loss = 2.079 | val_acc = 0.103 |\n",
            "Epoch 8: loss = 2.06 | acc = 0.104 | val_loss = 2.047 | val_acc = 0.105 |\n",
            "Epoch 9: loss = 2.033 | acc = 0.105 | val_loss = 2.026 | val_acc = 0.106 |\n",
            "Epoch 10: loss = 2.017 | acc = 0.107 | val_loss = 2.012 | val_acc = 0.109 |\n",
            "Epoch 11: loss = 2.004 | acc = 0.111 | val_loss = 2.002 | val_acc = 0.115 |\n",
            "Epoch 12: loss = 1.993 | acc = 0.119 | val_loss = 1.989 | val_acc = 0.13 |\n",
            "Epoch 13: loss = 1.969 | acc = 0.142 | val_loss = 1.959 | val_acc = 0.164 |\n",
            "Epoch 14: loss = 1.944 | acc = 0.237 | val_loss = 1.928 | val_acc = 0.405 |\n",
            "Epoch 15: loss = 1.9 | acc = 0.545 | val_loss = 1.888 | val_acc = 0.561 |\n",
            "Epoch 16: loss = 1.877 | acc = 0.581 | val_loss = 1.873 | val_acc = 0.572 |\n",
            "Epoch 17: loss = 1.864 | acc = 0.588 | val_loss = 1.863 | val_acc = 0.578 |\n",
            "Epoch 18: loss = 1.854 | acc = 0.6 | val_loss = 1.855 | val_acc = 0.593 |\n",
            "Epoch 19: loss = 1.846 | acc = 0.609 | val_loss = 1.847 | val_acc = 0.599 |\n",
            "Epoch 20: loss = 1.839 | acc = 0.613 | val_loss = 1.841 | val_acc = 0.604 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.028 | acc = 0.375 | val_loss = 1.852 | val_acc = 0.559 |\n",
            "Epoch 2: loss = 1.816 | acc = 0.591 | val_loss = 1.797 | val_acc = 0.608 |\n",
            "Epoch 3: loss = 1.783 | acc = 0.624 | val_loss = 1.776 | val_acc = 0.625 |\n",
            "Epoch 4: loss = 1.768 | acc = 0.638 | val_loss = 1.764 | val_acc = 0.636 |\n",
            "Epoch 5: loss = 1.752 | acc = 0.653 | val_loss = 1.751 | val_acc = 0.66 |\n",
            "Epoch 6: loss = 1.743 | acc = 0.668 | val_loss = 1.748 | val_acc = 0.655 |\n",
            "Epoch 7: loss = 1.74 | acc = 0.67 | val_loss = 1.742 | val_acc = 0.663 |\n",
            "Epoch 8: loss = 1.734 | acc = 0.673 | val_loss = 1.738 | val_acc = 0.666 |\n",
            "Epoch 9: loss = 1.73 | acc = 0.677 | val_loss = 1.735 | val_acc = 0.672 |\n",
            "Epoch 10: loss = 1.727 | acc = 0.682 | val_loss = 1.732 | val_acc = 0.675 |\n",
            "Epoch 11: loss = 1.727 | acc = 0.682 | val_loss = 1.73 | val_acc = 0.681 |\n",
            "Epoch 12: loss = 1.723 | acc = 0.686 | val_loss = 1.732 | val_acc = 0.676 |\n",
            "Epoch 13: loss = 1.718 | acc = 0.692 | val_loss = 1.726 | val_acc = 0.68 |\n",
            "Epoch 14: loss = 1.714 | acc = 0.691 | val_loss = 1.717 | val_acc = 0.689 |\n",
            "Epoch 15: loss = 1.712 | acc = 0.691 | val_loss = 1.718 | val_acc = 0.684 |\n",
            "Epoch 16: loss = 1.712 | acc = 0.689 | val_loss = 1.718 | val_acc = 0.671 |\n",
            "Epoch 17: loss = 1.71 | acc = 0.688 | val_loss = 1.715 | val_acc = 0.681 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.842 | acc = 0.519 | val_loss = 1.819 | val_acc = 0.493 |\n",
            "Epoch 2: loss = 1.854 | acc = 0.448 | val_loss = 1.906 | val_acc = 0.388 |\n",
            "Epoch 3: loss = 1.817 | acc = 0.451 | val_loss = 1.789 | val_acc = 0.515 |\n",
            "Epoch 4: loss = 1.796 | acc = 0.486 | val_loss = 1.801 | val_acc = 0.48 |\n",
            "Epoch 5: loss = 1.839 | acc = 0.436 | val_loss = 1.853 | val_acc = 0.398 |\n",
            "Epoch 6: loss = 1.836 | acc = 0.414 | val_loss = 1.846 | val_acc = 0.381 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.153 | val_loss = 2.337 | val_acc = 0.178 |\n",
            "Epoch 2: loss = 2.29 | acc = 0.163 | val_loss = 2.367 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.31 | acc = 0.148 | val_loss = 2.303 | val_acc = 0.197 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.197 | val_loss = 2.303 | val_acc = 0.197 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.197 | val_loss = 2.303 | val_acc = 0.197 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.197 | val_loss = 2.303 | val_acc = 0.197 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.385 | acc = 0.099 | val_loss = 2.38 | val_acc = 0.099 |\n",
            "Epoch 2: loss = 2.375 | acc = 0.101 | val_loss = 2.37 | val_acc = 0.099 |\n",
            "Epoch 3: loss = 2.365 | acc = 0.101 | val_loss = 2.36 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.355 | acc = 0.102 | val_loss = 2.351 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.346 | acc = 0.102 | val_loss = 2.341 | val_acc = 0.102 |\n",
            "Epoch 6: loss = 2.337 | acc = 0.103 | val_loss = 2.332 | val_acc = 0.103 |\n",
            "Epoch 7: loss = 2.327 | acc = 0.105 | val_loss = 2.323 | val_acc = 0.103 |\n",
            "Epoch 8: loss = 2.319 | acc = 0.107 | val_loss = 2.314 | val_acc = 0.106 |\n",
            "Epoch 9: loss = 2.31 | acc = 0.109 | val_loss = 2.305 | val_acc = 0.108 |\n",
            "Epoch 10: loss = 2.301 | acc = 0.113 | val_loss = 2.297 | val_acc = 0.113 |\n",
            "Epoch 11: loss = 2.292 | acc = 0.119 | val_loss = 2.288 | val_acc = 0.12 |\n",
            "Epoch 12: loss = 2.284 | acc = 0.128 | val_loss = 2.279 | val_acc = 0.132 |\n",
            "Epoch 13: loss = 2.275 | acc = 0.141 | val_loss = 2.271 | val_acc = 0.146 |\n",
            "Epoch 14: loss = 2.267 | acc = 0.157 | val_loss = 2.263 | val_acc = 0.165 |\n",
            "Epoch 15: loss = 2.259 | acc = 0.175 | val_loss = 2.256 | val_acc = 0.182 |\n",
            "Epoch 16: loss = 2.252 | acc = 0.193 | val_loss = 2.248 | val_acc = 0.2 |\n",
            "Epoch 17: loss = 2.245 | acc = 0.211 | val_loss = 2.241 | val_acc = 0.215 |\n",
            "Epoch 18: loss = 2.238 | acc = 0.224 | val_loss = 2.234 | val_acc = 0.232 |\n",
            "Epoch 19: loss = 2.231 | acc = 0.237 | val_loss = 2.227 | val_acc = 0.243 |\n",
            "Epoch 20: loss = 2.225 | acc = 0.248 | val_loss = 2.221 | val_acc = 0.253 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.3 | acc = 0.095 | val_loss = 2.275 | val_acc = 0.09 |\n",
            "Epoch 2: loss = 2.261 | acc = 0.091 | val_loss = 2.243 | val_acc = 0.086 |\n",
            "Epoch 3: loss = 2.232 | acc = 0.087 | val_loss = 2.218 | val_acc = 0.085 |\n",
            "Epoch 4: loss = 2.208 | acc = 0.087 | val_loss = 2.196 | val_acc = 0.085 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.157 | acc = 0.187 | val_loss = 1.974 | val_acc = 0.4 |\n",
            "Epoch 2: loss = 1.897 | acc = 0.516 | val_loss = 1.855 | val_acc = 0.548 |\n",
            "Epoch 3: loss = 1.832 | acc = 0.568 | val_loss = 1.817 | val_acc = 0.588 |\n",
            "Epoch 4: loss = 1.802 | acc = 0.604 | val_loss = 1.799 | val_acc = 0.6 |\n",
            "Epoch 5: loss = 1.789 | acc = 0.612 | val_loss = 1.788 | val_acc = 0.608 |\n",
            "Epoch 6: loss = 1.776 | acc = 0.622 | val_loss = 1.773 | val_acc = 0.621 |\n",
            "Epoch 7: loss = 1.766 | acc = 0.63 | val_loss = 1.771 | val_acc = 0.616 |\n",
            "Epoch 8: loss = 1.759 | acc = 0.63 | val_loss = 1.762 | val_acc = 0.631 |\n",
            "Epoch 9: loss = 1.753 | acc = 0.634 | val_loss = 1.756 | val_acc = 0.624 |\n",
            "Epoch 10: loss = 1.748 | acc = 0.641 | val_loss = 1.752 | val_acc = 0.638 |\n",
            "Epoch 11: loss = 1.743 | acc = 0.646 | val_loss = 1.749 | val_acc = 0.636 |\n",
            "Epoch 12: loss = 1.74 | acc = 0.649 | val_loss = 1.745 | val_acc = 0.644 |\n",
            "Epoch 13: loss = 1.738 | acc = 0.647 | val_loss = 1.745 | val_acc = 0.64 |\n",
            "Epoch 14: loss = 1.736 | acc = 0.649 | val_loss = 1.739 | val_acc = 0.645 |\n",
            "Epoch 15: loss = 1.73 | acc = 0.652 | val_loss = 1.735 | val_acc = 0.642 |\n",
            "Epoch 16: loss = 1.729 | acc = 0.654 | val_loss = 1.737 | val_acc = 0.646 |\n",
            "Epoch 17: loss = 1.727 | acc = 0.657 | val_loss = 1.731 | val_acc = 0.648 |\n",
            "Epoch 18: loss = 1.719 | acc = 0.672 | val_loss = 1.697 | val_acc = 0.723 |\n",
            "Epoch 19: loss = 1.677 | acc = 0.734 | val_loss = 1.672 | val_acc = 0.728 |\n",
            "Epoch 20: loss = 1.661 | acc = 0.741 | val_loss = 1.666 | val_acc = 0.721 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.926 | acc = 0.521 | val_loss = 1.85 | val_acc = 0.592 |\n",
            "Epoch 2: loss = 1.85 | acc = 0.59 | val_loss = 1.866 | val_acc = 0.524 |\n",
            "Epoch 3: loss = 1.865 | acc = 0.572 | val_loss = 1.881 | val_acc = 0.519 |\n",
            "Epoch 4: loss = 1.873 | acc = 0.558 | val_loss = 1.841 | val_acc = 0.54 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.161 | acc = 0.167 | val_loss = 2.03 | val_acc = 0.226 |\n",
            "Epoch 2: loss = 2.104 | acc = 0.148 | val_loss = 2.045 | val_acc = 0.14 |\n",
            "Epoch 3: loss = 2.213 | acc = 0.132 | val_loss = 2.391 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.117 | acc = 0.149 | val_loss = 2.049 | val_acc = 0.105 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.283 | acc = 0.083 | val_loss = 2.251 | val_acc = 0.092 |\n",
            "Epoch 2: loss = 2.252 | acc = 0.097 | val_loss = 2.259 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.27 | acc = 0.101 | val_loss = 2.281 | val_acc = 0.102 |\n",
            "Epoch 4: loss = 2.289 | acc = 0.129 | val_loss = 2.294 | val_acc = 0.19 |\n",
            "Epoch 5: loss = 2.298 | acc = 0.193 | val_loss = 2.3 | val_acc = 0.319 |\n",
            "Epoch 6: loss = 2.301 | acc = 0.193 | val_loss = 2.302 | val_acc = 0.122 |\n",
            "Epoch 7: loss = 2.302 | acc = 0.129 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 8: loss = 2.302 | acc = 0.108 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.192 | acc = 0.147 | val_loss = 2.064 | val_acc = 0.453 |\n",
            "Epoch 2: loss = 2.024 | acc = 0.618 | val_loss = 2.006 | val_acc = 0.65 |\n",
            "Epoch 3: loss = 2.001 | acc = 0.632 | val_loss = 2.0 | val_acc = 0.608 |\n",
            "Epoch 4: loss = 1.992 | acc = 0.529 | val_loss = 1.985 | val_acc = 0.498 |\n",
            "Epoch 5: loss = 1.976 | acc = 0.449 | val_loss = 1.971 | val_acc = 0.426 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.889 | acc = 0.45 | val_loss = 1.776 | val_acc = 0.642 |\n",
            "Epoch 2: loss = 1.765 | acc = 0.654 | val_loss = 1.769 | val_acc = 0.591 |\n",
            "Epoch 3: loss = 1.786 | acc = 0.601 | val_loss = 1.817 | val_acc = 0.571 |\n",
            "Epoch 4: loss = 1.82 | acc = 0.505 | val_loss = 1.831 | val_acc = 0.533 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.99 | acc = 0.204 | val_loss = 2.044 | val_acc = 0.19 |\n",
            "Epoch 2: loss = 2.137 | acc = 0.154 | val_loss = 2.112 | val_acc = 0.132 |\n",
            "Epoch 3: loss = 2.079 | acc = 0.184 | val_loss = 2.0 | val_acc = 0.199 |\n",
            "Epoch 4: loss = 2.062 | acc = 0.167 | val_loss = 2.03 | val_acc = 0.17 |\n",
            "Epoch 5: loss = 2.192 | acc = 0.13 | val_loss = 2.365 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.311 | acc = 0.103 | val_loss = 2.301 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.333 | acc = 0.14 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.309 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.326 | acc = 0.145 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.351 | acc = 0.101 | val_loss = 2.392 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.363 | acc = 0.074 | val_loss = 2.334 | val_acc = 0.078 |\n",
            "Epoch 2: loss = 2.314 | acc = 0.082 | val_loss = 2.292 | val_acc = 0.09 |\n",
            "Epoch 3: loss = 2.28 | acc = 0.106 | val_loss = 2.269 | val_acc = 0.119 |\n",
            "Epoch 4: loss = 2.268 | acc = 0.14 | val_loss = 2.267 | val_acc = 0.163 |\n",
            "Epoch 5: loss = 2.27 | acc = 0.201 | val_loss = 2.273 | val_acc = 0.226 |\n",
            "Epoch 6: loss = 2.278 | acc = 0.254 | val_loss = 2.282 | val_acc = 0.28 |\n",
            "Epoch 7: loss = 2.286 | acc = 0.286 | val_loss = 2.289 | val_acc = 0.327 |\n",
            "Epoch 8: loss = 2.292 | acc = 0.351 | val_loss = 2.294 | val_acc = 0.333 |\n",
            "Epoch 9: loss = 2.296 | acc = 0.352 | val_loss = 2.298 | val_acc = 0.314 |\n",
            "Epoch 10: loss = 2.299 | acc = 0.301 | val_loss = 2.3 | val_acc = 0.358 |\n",
            "Epoch 11: loss = 2.3 | acc = 0.248 | val_loss = 2.301 | val_acc = 0.328 |\n",
            "Epoch 12: loss = 2.301 | acc = 0.217 | val_loss = 2.302 | val_acc = 0.21 |\n",
            "Epoch 13: loss = 2.302 | acc = 0.193 | val_loss = 2.302 | val_acc = 0.174 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.277 | acc = 0.102 | val_loss = 2.197 | val_acc = 0.118 |\n",
            "Epoch 2: loss = 2.114 | acc = 0.287 | val_loss = 2.039 | val_acc = 0.49 |\n",
            "Epoch 3: loss = 2.017 | acc = 0.547 | val_loss = 2.006 | val_acc = 0.571 |\n",
            "Epoch 4: loss = 1.996 | acc = 0.605 | val_loss = 1.996 | val_acc = 0.6 |\n",
            "Epoch 5: loss = 1.996 | acc = 0.625 | val_loss = 1.999 | val_acc = 0.607 |\n",
            "Epoch 6: loss = 1.998 | acc = 0.596 | val_loss = 2.0 | val_acc = 0.554 |\n",
            "Epoch 7: loss = 1.994 | acc = 0.548 | val_loss = 1.992 | val_acc = 0.512 |\n",
            "Epoch 8: loss = 1.985 | acc = 0.503 | val_loss = 1.983 | val_acc = 0.484 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.949 | acc = 0.313 | val_loss = 1.792 | val_acc = 0.567 |\n",
            "Epoch 2: loss = 1.756 | acc = 0.636 | val_loss = 1.746 | val_acc = 0.654 |\n",
            "Epoch 3: loss = 1.733 | acc = 0.677 | val_loss = 1.732 | val_acc = 0.672 |\n",
            "Epoch 4: loss = 1.727 | acc = 0.678 | val_loss = 1.725 | val_acc = 0.645 |\n",
            "Epoch 5: loss = 1.722 | acc = 0.663 | val_loss = 1.723 | val_acc = 0.67 |\n",
            "Epoch 6: loss = 1.72 | acc = 0.657 | val_loss = 1.735 | val_acc = 0.629 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.936 | acc = 0.364 | val_loss = 1.953 | val_acc = 0.351 |\n",
            "Epoch 2: loss = 1.936 | acc = 0.324 | val_loss = 2.096 | val_acc = 0.241 |\n",
            "Epoch 3: loss = 2.034 | acc = 0.23 | val_loss = 1.976 | val_acc = 0.219 |\n",
            "Epoch 4: loss = 2.072 | acc = 0.189 | val_loss = 2.001 | val_acc = 0.194 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.299 | acc = 0.169 | val_loss = 2.303 | val_acc = 0.105 |\n",
            "Epoch 2: loss = 2.295 | acc = 0.146 | val_loss = 2.303 | val_acc = 0.199 |\n",
            "Epoch 3: loss = 2.346 | acc = 0.165 | val_loss = 2.367 | val_acc = 0.167 |\n",
            "Epoch 4: loss = 2.312 | acc = 0.17 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.321 | acc = 0.1 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.319 | acc = 0.119 | val_loss = 2.305 | val_acc = 0.123 |\n",
            "Epoch 2: loss = 2.294 | acc = 0.124 | val_loss = 2.282 | val_acc = 0.128 |\n",
            "Epoch 3: loss = 2.272 | acc = 0.133 | val_loss = 2.263 | val_acc = 0.14 |\n",
            "Epoch 4: loss = 2.256 | acc = 0.149 | val_loss = 2.25 | val_acc = 0.161 |\n",
            "Epoch 5: loss = 2.245 | acc = 0.181 | val_loss = 2.243 | val_acc = 0.206 |\n",
            "Epoch 6: loss = 2.241 | acc = 0.229 | val_loss = 2.242 | val_acc = 0.257 |\n",
            "Epoch 7: loss = 2.242 | acc = 0.279 | val_loss = 2.245 | val_acc = 0.301 |\n",
            "Epoch 8: loss = 2.247 | acc = 0.323 | val_loss = 2.251 | val_acc = 0.336 |\n",
            "Epoch 9: loss = 2.254 | acc = 0.357 | val_loss = 2.258 | val_acc = 0.361 |\n",
            "Epoch 10: loss = 2.262 | acc = 0.377 | val_loss = 2.266 | val_acc = 0.379 |\n",
            "Epoch 11: loss = 2.269 | acc = 0.394 | val_loss = 2.273 | val_acc = 0.387 |\n",
            "Epoch 12: loss = 2.276 | acc = 0.396 | val_loss = 2.279 | val_acc = 0.387 |\n",
            "Epoch 13: loss = 2.281 | acc = 0.397 | val_loss = 2.284 | val_acc = 0.381 |\n",
            "Epoch 14: loss = 2.286 | acc = 0.378 | val_loss = 2.288 | val_acc = 0.376 |\n",
            "Epoch 15: loss = 2.29 | acc = 0.391 | val_loss = 2.291 | val_acc = 0.36 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.104 | val_loss = 2.239 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 2.188 | acc = 0.114 | val_loss = 2.153 | val_acc = 0.138 |\n",
            "Epoch 3: loss = 2.11 | acc = 0.231 | val_loss = 2.071 | val_acc = 0.376 |\n",
            "Epoch 4: loss = 2.041 | acc = 0.498 | val_loss = 2.022 | val_acc = 0.532 |\n",
            "Epoch 5: loss = 2.003 | acc = 0.579 | val_loss = 1.994 | val_acc = 0.604 |\n",
            "Epoch 6: loss = 1.989 | acc = 0.627 | val_loss = 1.989 | val_acc = 0.631 |\n",
            "Epoch 7: loss = 1.987 | acc = 0.655 | val_loss = 1.989 | val_acc = 0.662 |\n",
            "Epoch 8: loss = 1.988 | acc = 0.672 | val_loss = 1.992 | val_acc = 0.67 |\n",
            "Epoch 9: loss = 1.992 | acc = 0.677 | val_loss = 1.996 | val_acc = 0.668 |\n",
            "Epoch 10: loss = 1.995 | acc = 0.668 | val_loss = 1.998 | val_acc = 0.66 |\n",
            "Epoch 11: loss = 1.997 | acc = 0.656 | val_loss = 2.0 | val_acc = 0.639 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.976 | acc = 0.464 | val_loss = 1.862 | val_acc = 0.597 |\n",
            "Epoch 2: loss = 1.824 | acc = 0.659 | val_loss = 1.808 | val_acc = 0.679 |\n",
            "Epoch 3: loss = 1.765 | acc = 0.701 | val_loss = 1.739 | val_acc = 0.71 |\n",
            "Epoch 4: loss = 1.727 | acc = 0.724 | val_loss = 1.727 | val_acc = 0.722 |\n",
            "Epoch 5: loss = 1.715 | acc = 0.733 | val_loss = 1.714 | val_acc = 0.72 |\n",
            "Epoch 6: loss = 1.709 | acc = 0.736 | val_loss = 1.709 | val_acc = 0.728 |\n",
            "Epoch 7: loss = 1.702 | acc = 0.739 | val_loss = 1.702 | val_acc = 0.731 |\n",
            "Epoch 8: loss = 1.698 | acc = 0.74 | val_loss = 1.703 | val_acc = 0.721 |\n",
            "Epoch 9: loss = 1.695 | acc = 0.742 | val_loss = 1.697 | val_acc = 0.726 |\n",
            "Epoch 10: loss = 1.692 | acc = 0.739 | val_loss = 1.695 | val_acc = 0.725 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.865 | acc = 0.503 | val_loss = 1.86 | val_acc = 0.472 |\n",
            "Epoch 2: loss = 1.859 | acc = 0.411 | val_loss = 1.882 | val_acc = 0.319 |\n",
            "Epoch 3: loss = 1.855 | acc = 0.393 | val_loss = 1.834 | val_acc = 0.404 |\n",
            "Epoch 4: loss = 1.906 | acc = 0.322 | val_loss = 1.88 | val_acc = 0.405 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.211 | acc = 0.133 | val_loss = 2.258 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.325 | acc = 0.116 | val_loss = 2.337 | val_acc = 0.113 |\n",
            "Epoch 3: loss = 2.319 | acc = 0.106 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.337 | acc = 0.1 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.328 | acc = 0.1 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.372 | acc = 0.167 | val_loss = 2.363 | val_acc = 0.166 |\n",
            "Epoch 2: loss = 2.355 | acc = 0.17 | val_loss = 2.347 | val_acc = 0.169 |\n",
            "Epoch 3: loss = 2.338 | acc = 0.174 | val_loss = 2.33 | val_acc = 0.172 |\n",
            "Epoch 4: loss = 2.322 | acc = 0.178 | val_loss = 2.314 | val_acc = 0.175 |\n",
            "Epoch 5: loss = 2.306 | acc = 0.182 | val_loss = 2.299 | val_acc = 0.179 |\n",
            "Epoch 6: loss = 2.292 | acc = 0.186 | val_loss = 2.286 | val_acc = 0.185 |\n",
            "Epoch 7: loss = 2.28 | acc = 0.189 | val_loss = 2.275 | val_acc = 0.187 |\n",
            "Epoch 8: loss = 2.27 | acc = 0.194 | val_loss = 2.267 | val_acc = 0.191 |\n",
            "Epoch 9: loss = 2.263 | acc = 0.199 | val_loss = 2.26 | val_acc = 0.196 |\n",
            "Epoch 10: loss = 2.258 | acc = 0.206 | val_loss = 2.256 | val_acc = 0.202 |\n",
            "Epoch 11: loss = 2.254 | acc = 0.212 | val_loss = 2.253 | val_acc = 0.209 |\n",
            "Epoch 12: loss = 2.253 | acc = 0.219 | val_loss = 2.252 | val_acc = 0.216 |\n",
            "Epoch 13: loss = 2.252 | acc = 0.227 | val_loss = 2.252 | val_acc = 0.224 |\n",
            "Epoch 14: loss = 2.253 | acc = 0.236 | val_loss = 2.253 | val_acc = 0.234 |\n",
            "Epoch 15: loss = 2.254 | acc = 0.244 | val_loss = 2.255 | val_acc = 0.242 |\n",
            "Epoch 16: loss = 2.256 | acc = 0.253 | val_loss = 2.257 | val_acc = 0.253 |\n",
            "Epoch 17: loss = 2.258 | acc = 0.261 | val_loss = 2.259 | val_acc = 0.263 |\n",
            "Epoch 18: loss = 2.261 | acc = 0.271 | val_loss = 2.262 | val_acc = 0.271 |\n",
            "Epoch 19: loss = 2.264 | acc = 0.279 | val_loss = 2.265 | val_acc = 0.28 |\n",
            "Epoch 20: loss = 2.267 | acc = 0.286 | val_loss = 2.268 | val_acc = 0.289 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.297 | acc = 0.104 | val_loss = 2.25 | val_acc = 0.126 |\n",
            "Epoch 2: loss = 2.194 | acc = 0.187 | val_loss = 2.151 | val_acc = 0.263 |\n",
            "Epoch 3: loss = 2.115 | acc = 0.326 | val_loss = 2.09 | val_acc = 0.375 |\n",
            "Epoch 4: loss = 2.067 | acc = 0.421 | val_loss = 2.05 | val_acc = 0.456 |\n",
            "Epoch 5: loss = 2.034 | acc = 0.492 | val_loss = 2.022 | val_acc = 0.516 |\n",
            "Epoch 6: loss = 2.011 | acc = 0.538 | val_loss = 2.003 | val_acc = 0.554 |\n",
            "Epoch 7: loss = 1.995 | acc = 0.576 | val_loss = 1.99 | val_acc = 0.585 |\n",
            "Epoch 8: loss = 1.985 | acc = 0.604 | val_loss = 1.983 | val_acc = 0.614 |\n",
            "Epoch 9: loss = 1.978 | acc = 0.625 | val_loss = 1.978 | val_acc = 0.628 |\n",
            "Epoch 10: loss = 1.975 | acc = 0.639 | val_loss = 1.976 | val_acc = 0.641 |\n",
            "Epoch 11: loss = 1.974 | acc = 0.649 | val_loss = 1.976 | val_acc = 0.651 |\n",
            "Epoch 12: loss = 1.975 | acc = 0.658 | val_loss = 1.977 | val_acc = 0.661 |\n",
            "Epoch 13: loss = 1.977 | acc = 0.665 | val_loss = 1.979 | val_acc = 0.664 |\n",
            "Epoch 14: loss = 1.979 | acc = 0.668 | val_loss = 1.982 | val_acc = 0.665 |\n",
            "Epoch 15: loss = 1.981 | acc = 0.673 | val_loss = 1.984 | val_acc = 0.67 |\n",
            "Epoch 16: loss = 1.984 | acc = 0.678 | val_loss = 1.987 | val_acc = 0.682 |\n",
            "Epoch 17: loss = 1.986 | acc = 0.682 | val_loss = 1.989 | val_acc = 0.682 |\n",
            "Epoch 18: loss = 1.988 | acc = 0.684 | val_loss = 1.991 | val_acc = 0.681 |\n",
            "Epoch 19: loss = 1.99 | acc = 0.683 | val_loss = 1.993 | val_acc = 0.681 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.104 | acc = 0.101 | val_loss = 1.945 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 1.891 | acc = 0.101 | val_loss = 1.861 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 1.841 | acc = 0.102 | val_loss = 1.83 | val_acc = 0.102 |\n",
            "Epoch 4: loss = 1.817 | acc = 0.112 | val_loss = 1.815 | val_acc = 0.128 |\n",
            "Epoch 5: loss = 1.79 | acc = 0.45 | val_loss = 1.761 | val_acc = 0.694 |\n",
            "Epoch 6: loss = 1.744 | acc = 0.704 | val_loss = 1.739 | val_acc = 0.706 |\n",
            "Epoch 7: loss = 1.729 | acc = 0.715 | val_loss = 1.73 | val_acc = 0.714 |\n",
            "Epoch 8: loss = 1.722 | acc = 0.721 | val_loss = 1.726 | val_acc = 0.707 |\n",
            "Epoch 9: loss = 1.717 | acc = 0.722 | val_loss = 1.717 | val_acc = 0.72 |\n",
            "Epoch 10: loss = 1.712 | acc = 0.728 | val_loss = 1.714 | val_acc = 0.721 |\n",
            "Epoch 11: loss = 1.708 | acc = 0.731 | val_loss = 1.713 | val_acc = 0.715 |\n",
            "Epoch 12: loss = 1.705 | acc = 0.731 | val_loss = 1.71 | val_acc = 0.729 |\n",
            "Epoch 13: loss = 1.703 | acc = 0.735 | val_loss = 1.707 | val_acc = 0.728 |\n",
            "Epoch 14: loss = 1.699 | acc = 0.739 | val_loss = 1.7 | val_acc = 0.736 |\n",
            "Epoch 15: loss = 1.697 | acc = 0.738 | val_loss = 1.699 | val_acc = 0.727 |\n",
            "Epoch 16: loss = 1.693 | acc = 0.743 | val_loss = 1.696 | val_acc = 0.729 |\n",
            "Epoch 17: loss = 1.69 | acc = 0.744 | val_loss = 1.694 | val_acc = 0.737 |\n",
            "Epoch 18: loss = 1.687 | acc = 0.749 | val_loss = 1.69 | val_acc = 0.741 |\n",
            "Epoch 19: loss = 1.685 | acc = 0.749 | val_loss = 1.688 | val_acc = 0.74 |\n",
            "Epoch 20: loss = 1.682 | acc = 0.747 | val_loss = 1.684 | val_acc = 0.742 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.984 | acc = 0.102 | val_loss = 1.912 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 1.91 | acc = 0.116 | val_loss = 1.897 | val_acc = 0.133 |\n",
            "Epoch 3: loss = 1.849 | acc = 0.417 | val_loss = 1.821 | val_acc = 0.541 |\n",
            "Epoch 4: loss = 1.768 | acc = 0.611 | val_loss = 1.757 | val_acc = 0.644 |\n",
            "Epoch 5: loss = 1.775 | acc = 0.586 | val_loss = 1.77 | val_acc = 0.534 |\n",
            "Epoch 6: loss = 1.779 | acc = 0.598 | val_loss = 1.737 | val_acc = 0.654 |\n",
            "Epoch 7: loss = 1.76 | acc = 0.617 | val_loss = 1.774 | val_acc = 0.576 |\n",
            "Epoch 8: loss = 1.76 | acc = 0.615 | val_loss = 1.769 | val_acc = 0.6 |\n",
            "Epoch 9: loss = 1.773 | acc = 0.588 | val_loss = 1.791 | val_acc = 0.575 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.21 | acc = 0.142 | val_loss = 2.079 | val_acc = 0.168 |\n",
            "Epoch 2: loss = 2.131 | acc = 0.158 | val_loss = 2.392 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.134 | acc = 0.147 | val_loss = 2.421 | val_acc = 0.104 |\n",
            "Epoch 4: loss = 2.072 | acc = 0.173 | val_loss = 1.998 | val_acc = 0.164 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.309 | acc = 0.113 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.299 | acc = 0.107 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.027 | acc = 0.362 | val_loss = 2.0 | val_acc = 0.274 |\n",
            "Epoch 2: loss = 1.995 | acc = 0.309 | val_loss = 1.995 | val_acc = 0.333 |\n",
            "Epoch 3: loss = 1.999 | acc = 0.302 | val_loss = 2.013 | val_acc = 0.31 |\n",
            "Epoch 4: loss = 2.007 | acc = 0.29 | val_loss = 2.002 | val_acc = 0.29 |\n",
            "Epoch 5: loss = 2.002 | acc = 0.288 | val_loss = 1.995 | val_acc = 0.323 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.185 | acc = 0.174 | val_loss = 2.318 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.265 | acc = 0.134 | val_loss = 2.281 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.279 | acc = 0.123 | val_loss = 2.218 | val_acc = 0.101 |\n",
            "Epoch 4: loss = 2.262 | acc = 0.13 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.251 | acc = 0.133 | val_loss = 2.273 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.26 | acc = 0.131 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.337 | acc = 0.104 | val_loss = 2.418 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.366 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.315 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.322 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.31 | acc = 0.106 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.281 | acc = 0.111 | val_loss = 2.3 | val_acc = 0.181 |\n",
            "Epoch 2: loss = 2.302 | acc = 0.107 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.036 | acc = 0.481 | val_loss = 1.982 | val_acc = 0.449 |\n",
            "Epoch 2: loss = 1.968 | acc = 0.423 | val_loss = 1.956 | val_acc = 0.43 |\n",
            "Epoch 3: loss = 1.954 | acc = 0.4 | val_loss = 1.95 | val_acc = 0.376 |\n",
            "Epoch 4: loss = 1.95 | acc = 0.385 | val_loss = 1.954 | val_acc = 0.413 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.026 | acc = 0.264 | val_loss = 2.239 | val_acc = 0.154 |\n",
            "Epoch 2: loss = 2.189 | acc = 0.154 | val_loss = 2.171 | val_acc = 0.164 |\n",
            "Epoch 3: loss = 2.179 | acc = 0.158 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.206 | acc = 0.154 | val_loss = 2.293 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.16 | acc = 0.162 | val_loss = 2.111 | val_acc = 0.131 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.334 | acc = 0.104 | val_loss = 2.336 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.31 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.32 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.304 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.363 | acc = 0.101 | val_loss = 2.312 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.305 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.275 | acc = 0.139 | val_loss = 2.271 | val_acc = 0.167 |\n",
            "Epoch 2: loss = 2.29 | acc = 0.204 | val_loss = 2.3 | val_acc = 0.132 |\n",
            "Epoch 3: loss = 2.302 | acc = 0.122 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.05 | acc = 0.504 | val_loss = 1.997 | val_acc = 0.586 |\n",
            "Epoch 2: loss = 1.989 | acc = 0.516 | val_loss = 1.974 | val_acc = 0.424 |\n",
            "Epoch 3: loss = 1.96 | acc = 0.438 | val_loss = 1.953 | val_acc = 0.404 |\n",
            "Epoch 4: loss = 1.947 | acc = 0.422 | val_loss = 1.948 | val_acc = 0.459 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.945 | acc = 0.391 | val_loss = 1.88 | val_acc = 0.447 |\n",
            "Epoch 2: loss = 1.964 | acc = 0.329 | val_loss = 2.024 | val_acc = 0.252 |\n",
            "Epoch 3: loss = 2.03 | acc = 0.225 | val_loss = 1.998 | val_acc = 0.184 |\n",
            "Epoch 4: loss = 2.038 | acc = 0.176 | val_loss = 2.02 | val_acc = 0.141 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.322 | acc = 0.115 | val_loss = 2.361 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.38 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.104 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.329 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.389 | acc = 0.141 | val_loss = 2.354 | val_acc = 0.146 |\n",
            "Epoch 2: loss = 2.33 | acc = 0.128 | val_loss = 2.313 | val_acc = 0.109 |\n",
            "Epoch 3: loss = 2.307 | acc = 0.102 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.333 | acc = 0.102 | val_loss = 2.264 | val_acc = 0.105 |\n",
            "Epoch 2: loss = 2.257 | acc = 0.184 | val_loss = 2.266 | val_acc = 0.316 |\n",
            "Epoch 3: loss = 2.281 | acc = 0.367 | val_loss = 2.291 | val_acc = 0.371 |\n",
            "Epoch 4: loss = 2.296 | acc = 0.373 | val_loss = 2.3 | val_acc = 0.185 |\n",
            "Epoch 5: loss = 2.301 | acc = 0.16 | val_loss = 2.302 | val_acc = 0.141 |\n",
            "Epoch 6: loss = 2.302 | acc = 0.121 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.117 | acc = 0.228 | val_loss = 1.982 | val_acc = 0.629 |\n",
            "Epoch 2: loss = 1.985 | acc = 0.676 | val_loss = 1.999 | val_acc = 0.632 |\n",
            "Epoch 3: loss = 1.995 | acc = 0.585 | val_loss = 1.989 | val_acc = 0.483 |\n",
            "Epoch 4: loss = 1.977 | acc = 0.479 | val_loss = 1.969 | val_acc = 0.473 |\n",
            "Epoch 5: loss = 1.96 | acc = 0.455 | val_loss = 1.955 | val_acc = 0.377 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.918 | acc = 0.462 | val_loss = 1.802 | val_acc = 0.591 |\n",
            "Epoch 2: loss = 1.809 | acc = 0.551 | val_loss = 1.82 | val_acc = 0.476 |\n",
            "Epoch 3: loss = 1.822 | acc = 0.469 | val_loss = 1.824 | val_acc = 0.503 |\n",
            "Epoch 4: loss = 1.877 | acc = 0.421 | val_loss = 1.889 | val_acc = 0.35 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.283 | acc = 0.144 | val_loss = 2.302 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 2.298 | acc = 0.126 | val_loss = 2.36 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.304 | acc = 0.121 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.112 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.104 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.304 | acc = 0.099 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.304 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.304 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.111 | val_loss = 2.311 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.314 | acc = 0.101 | val_loss = 2.316 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.313 | acc = 0.099 | val_loss = 2.321 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.313 | acc = 0.1 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.363 | acc = 0.099 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.359 | acc = 0.1 | val_loss = 2.367 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.366 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.355 | acc = 0.099 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.104 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.304 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.304 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.304 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.21 | acc = 0.184 | val_loss = 2.228 | val_acc = 0.161 |\n",
            "Epoch 2: loss = 2.223 | acc = 0.179 | val_loss = 2.23 | val_acc = 0.29 |\n",
            "Epoch 3: loss = 2.217 | acc = 0.182 | val_loss = 2.229 | val_acc = 0.193 |\n",
            "Epoch 4: loss = 2.217 | acc = 0.183 | val_loss = 2.238 | val_acc = 0.191 |\n",
            "Epoch 5: loss = 2.224 | acc = 0.174 | val_loss = 2.222 | val_acc = 0.196 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.328 | acc = 0.104 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.324 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.313 | acc = 0.102 | val_loss = 2.344 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.326 | acc = 0.099 | val_loss = 2.391 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.107 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.094 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.3 | acc = 0.106 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.1 | acc = 0.235 | val_loss = 2.125 | val_acc = 0.204 |\n",
            "Epoch 2: loss = 2.108 | acc = 0.206 | val_loss = 2.152 | val_acc = 0.216 |\n",
            "Epoch 3: loss = 2.122 | acc = 0.206 | val_loss = 2.103 | val_acc = 0.197 |\n",
            "Epoch 4: loss = 2.116 | acc = 0.207 | val_loss = 2.112 | val_acc = 0.181 |\n",
            "Epoch 5: loss = 2.109 | acc = 0.205 | val_loss = 2.106 | val_acc = 0.2 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.1 | val_loss = 2.306 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.316 | acc = 0.099 | val_loss = 2.42 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.314 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.306 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.307 | acc = 0.113 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.311 | acc = 0.105 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.299 | acc = 0.126 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.054 | acc = 0.304 | val_loss = 2.047 | val_acc = 0.191 |\n",
            "Epoch 2: loss = 2.045 | acc = 0.198 | val_loss = 2.055 | val_acc = 0.188 |\n",
            "Epoch 3: loss = 2.042 | acc = 0.195 | val_loss = 2.045 | val_acc = 0.191 |\n",
            "Epoch 4: loss = 2.041 | acc = 0.194 | val_loss = 2.031 | val_acc = 0.278 |\n",
            "Epoch 5: loss = 2.038 | acc = 0.195 | val_loss = 2.047 | val_acc = 0.194 |\n",
            "Epoch 6: loss = 2.038 | acc = 0.196 | val_loss = 2.035 | val_acc = 0.241 |\n",
            "Epoch 7: loss = 2.042 | acc = 0.194 | val_loss = 2.032 | val_acc = 0.191 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.302 | acc = 0.122 | val_loss = 2.3 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.305 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.304 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.304 | acc = 0.101 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.321 | acc = 0.1 | val_loss = 2.32 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.32 | acc = 0.101 | val_loss = 2.326 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.319 | acc = 0.1 | val_loss = 2.312 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.32 | acc = 0.099 | val_loss = 2.326 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.386 | acc = 0.102 | val_loss = 2.376 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.383 | acc = 0.1 | val_loss = 2.403 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.384 | acc = 0.1 | val_loss = 2.354 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.386 | acc = 0.101 | val_loss = 2.418 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.1 | val_loss = 2.311 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.312 | acc = 0.099 | val_loss = 2.311 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.312 | acc = 0.099 | val_loss = 2.311 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.311 | acc = 0.099 | val_loss = 2.311 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.371 | acc = 0.1 | val_loss = 2.359 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.371 | acc = 0.1 | val_loss = 2.389 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.37 | acc = 0.099 | val_loss = 2.385 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.367 | acc = 0.1 | val_loss = 2.402 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.307 | acc = 0.1 | val_loss = 2.306 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.307 | acc = 0.1 | val_loss = 2.307 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.307 | acc = 0.1 | val_loss = 2.308 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.307 | acc = 0.099 | val_loss = 2.309 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.34 | acc = 0.1 | val_loss = 2.368 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.344 | acc = 0.1 | val_loss = 2.349 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.344 | acc = 0.101 | val_loss = 2.349 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.344 | acc = 0.1 | val_loss = 2.376 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.101 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.305 | acc = 0.098 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.305 | acc = 0.101 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.305 | acc = 0.1 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.322 | acc = 0.099 | val_loss = 2.315 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.321 | acc = 0.1 | val_loss = 2.323 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.325 | acc = 0.1 | val_loss = 2.316 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.321 | acc = 0.099 | val_loss = 2.309 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.404 | acc = 0.088 | val_loss = 2.381 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.415 | acc = 0.089 | val_loss = 2.392 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.424 | acc = 0.09 | val_loss = 2.401 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.432 | acc = 0.089 | val_loss = 2.408 | val_acc = 0.101 |\n",
            "Epoch 5: loss = 2.437 | acc = 0.089 | val_loss = 2.413 | val_acc = 0.101 |\n",
            "Epoch 6: loss = 2.439 | acc = 0.09 | val_loss = 2.418 | val_acc = 0.102 |\n",
            "Epoch 7: loss = 2.444 | acc = 0.092 | val_loss = 2.421 | val_acc = 0.102 |\n",
            "Epoch 8: loss = 2.446 | acc = 0.094 | val_loss = 2.424 | val_acc = 0.104 |\n",
            "Epoch 9: loss = 2.449 | acc = 0.094 | val_loss = 2.426 | val_acc = 0.107 |\n",
            "Epoch 10: loss = 2.449 | acc = 0.096 | val_loss = 2.428 | val_acc = 0.108 |\n",
            "Epoch 11: loss = 2.453 | acc = 0.099 | val_loss = 2.429 | val_acc = 0.112 |\n",
            "Epoch 12: loss = 2.455 | acc = 0.104 | val_loss = 2.431 | val_acc = 0.114 |\n",
            "Epoch 13: loss = 2.454 | acc = 0.107 | val_loss = 2.431 | val_acc = 0.115 |\n",
            "Epoch 14: loss = 2.455 | acc = 0.11 | val_loss = 2.432 | val_acc = 0.117 |\n",
            "Epoch 15: loss = 2.454 | acc = 0.113 | val_loss = 2.433 | val_acc = 0.119 |\n",
            "Epoch 16: loss = 2.455 | acc = 0.116 | val_loss = 2.434 | val_acc = 0.122 |\n",
            "Epoch 17: loss = 2.455 | acc = 0.12 | val_loss = 2.434 | val_acc = 0.125 |\n",
            "Epoch 18: loss = 2.454 | acc = 0.123 | val_loss = 2.435 | val_acc = 0.127 |\n",
            "Epoch 19: loss = 2.46 | acc = 0.123 | val_loss = 2.436 | val_acc = 0.13 |\n",
            "Epoch 20: loss = 2.457 | acc = 0.127 | val_loss = 2.438 | val_acc = 0.133 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.395 | acc = 0.098 | val_loss = 2.417 | val_acc = 0.098 |\n",
            "Epoch 2: loss = 2.432 | acc = 0.089 | val_loss = 2.452 | val_acc = 0.095 |\n",
            "Epoch 3: loss = 2.441 | acc = 0.078 | val_loss = 2.449 | val_acc = 0.079 |\n",
            "Epoch 4: loss = 2.435 | acc = 0.082 | val_loss = 2.445 | val_acc = 0.088 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.422 | acc = 0.102 | val_loss = 2.43 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 2.425 | acc = 0.111 | val_loss = 2.422 | val_acc = 0.106 |\n",
            "Epoch 3: loss = 2.424 | acc = 0.136 | val_loss = 2.422 | val_acc = 0.111 |\n",
            "Epoch 4: loss = 2.423 | acc = 0.151 | val_loss = 2.422 | val_acc = 0.117 |\n",
            "Epoch 5: loss = 2.421 | acc = 0.156 | val_loss = 2.422 | val_acc = 0.118 |\n",
            "Epoch 6: loss = 2.422 | acc = 0.158 | val_loss = 2.422 | val_acc = 0.116 |\n",
            "Epoch 7: loss = 2.42 | acc = 0.161 | val_loss = 2.422 | val_acc = 0.117 |\n",
            "Epoch 8: loss = 2.422 | acc = 0.16 | val_loss = 2.423 | val_acc = 0.118 |\n",
            "Epoch 9: loss = 2.419 | acc = 0.16 | val_loss = 2.423 | val_acc = 0.118 |\n",
            "Epoch 10: loss = 2.421 | acc = 0.161 | val_loss = 2.423 | val_acc = 0.12 |\n",
            "Epoch 11: loss = 2.421 | acc = 0.161 | val_loss = 2.423 | val_acc = 0.121 |\n",
            "Epoch 12: loss = 2.421 | acc = 0.162 | val_loss = 2.423 | val_acc = 0.121 |\n",
            "Epoch 13: loss = 2.422 | acc = 0.163 | val_loss = 2.423 | val_acc = 0.122 |\n",
            "Epoch 14: loss = 2.423 | acc = 0.162 | val_loss = 2.423 | val_acc = 0.122 |\n",
            "Epoch 15: loss = 2.422 | acc = 0.163 | val_loss = 2.423 | val_acc = 0.122 |\n",
            "Epoch 16: loss = 2.423 | acc = 0.161 | val_loss = 2.423 | val_acc = 0.122 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.423 | acc = 0.11 | val_loss = 2.423 | val_acc = 0.054 |\n",
            "Epoch 2: loss = 2.425 | acc = 0.119 | val_loss = 2.423 | val_acc = 0.088 |\n",
            "Epoch 3: loss = 2.427 | acc = 0.146 | val_loss = 2.423 | val_acc = 0.097 |\n",
            "Epoch 4: loss = 2.426 | acc = 0.147 | val_loss = 2.423 | val_acc = 0.097 |\n",
            "Epoch 5: loss = 2.426 | acc = 0.144 | val_loss = 2.423 | val_acc = 0.087 |\n",
            "Epoch 6: loss = 2.422 | acc = 0.133 | val_loss = 2.423 | val_acc = 0.089 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.415 | acc = 0.109 | val_loss = 2.423 | val_acc = 0.118 |\n",
            "Epoch 2: loss = 2.418 | acc = 0.116 | val_loss = 2.423 | val_acc = 0.14 |\n",
            "Epoch 3: loss = 2.417 | acc = 0.121 | val_loss = 2.423 | val_acc = 0.12 |\n",
            "Epoch 4: loss = 2.415 | acc = 0.114 | val_loss = 2.423 | val_acc = 0.111 |\n",
            "Epoch 5: loss = 2.414 | acc = 0.107 | val_loss = 2.423 | val_acc = 0.106 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.366 | acc = 0.084 | val_loss = 2.381 | val_acc = 0.071 |\n",
            "Epoch 2: loss = 2.373 | acc = 0.084 | val_loss = 2.366 | val_acc = 0.098 |\n",
            "Epoch 3: loss = 2.379 | acc = 0.086 | val_loss = 2.391 | val_acc = 0.073 |\n",
            "Epoch 4: loss = 2.386 | acc = 0.087 | val_loss = 2.384 | val_acc = 0.096 |\n",
            "Epoch 5: loss = 2.392 | acc = 0.087 | val_loss = 2.398 | val_acc = 0.075 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.376 | acc = 0.084 | val_loss = 2.365 | val_acc = 0.079 |\n",
            "Epoch 2: loss = 2.379 | acc = 0.088 | val_loss = 2.392 | val_acc = 0.103 |\n",
            "Epoch 3: loss = 2.386 | acc = 0.09 | val_loss = 2.374 | val_acc = 0.088 |\n",
            "Epoch 4: loss = 2.397 | acc = 0.088 | val_loss = 2.41 | val_acc = 0.08 |\n",
            "Epoch 5: loss = 2.404 | acc = 0.088 | val_loss = 2.4 | val_acc = 0.103 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.439 | acc = 0.127 | val_loss = 2.409 | val_acc = 0.144 |\n",
            "Epoch 2: loss = 2.446 | acc = 0.134 | val_loss = 2.479 | val_acc = 0.093 |\n",
            "Epoch 3: loss = 2.446 | acc = 0.123 | val_loss = 2.416 | val_acc = 0.159 |\n",
            "Epoch 4: loss = 2.435 | acc = 0.145 | val_loss = 2.454 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.447 | acc = 0.17 | val_loss = 2.422 | val_acc = 0.218 |\n",
            "Epoch 6: loss = 2.434 | acc = 0.135 | val_loss = 2.445 | val_acc = 0.1 |\n",
            "Epoch 7: loss = 2.432 | acc = 0.113 | val_loss = 2.423 | val_acc = 0.132 |\n",
            "Epoch 8: loss = 2.428 | acc = 0.127 | val_loss = 2.43 | val_acc = 0.103 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.42 | acc = 0.124 | val_loss = 2.426 | val_acc = 0.14 |\n",
            "Epoch 2: loss = 2.424 | acc = 0.136 | val_loss = 2.423 | val_acc = 0.159 |\n",
            "Epoch 3: loss = 2.424 | acc = 0.153 | val_loss = 2.426 | val_acc = 0.161 |\n",
            "Epoch 4: loss = 2.423 | acc = 0.163 | val_loss = 2.423 | val_acc = 0.186 |\n",
            "Epoch 5: loss = 2.424 | acc = 0.173 | val_loss = 2.426 | val_acc = 0.153 |\n",
            "Epoch 6: loss = 2.422 | acc = 0.173 | val_loss = 2.423 | val_acc = 0.194 |\n",
            "Epoch 7: loss = 2.428 | acc = 0.168 | val_loss = 2.426 | val_acc = 0.148 |\n",
            "Epoch 8: loss = 2.425 | acc = 0.168 | val_loss = 2.423 | val_acc = 0.184 |\n",
            "Epoch 9: loss = 2.423 | acc = 0.163 | val_loss = 2.426 | val_acc = 0.145 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.419 | acc = 0.134 | val_loss = 2.423 | val_acc = 0.085 |\n",
            "Epoch 2: loss = 2.42 | acc = 0.136 | val_loss = 2.423 | val_acc = 0.193 |\n",
            "Epoch 3: loss = 2.422 | acc = 0.131 | val_loss = 2.423 | val_acc = 0.068 |\n",
            "Epoch 4: loss = 2.421 | acc = 0.117 | val_loss = 2.423 | val_acc = 0.175 |\n",
            "Epoch 5: loss = 2.422 | acc = 0.111 | val_loss = 2.423 | val_acc = 0.051 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.384 | acc = 0.095 | val_loss = 2.38 | val_acc = 0.09 |\n",
            "Epoch 2: loss = 2.385 | acc = 0.097 | val_loss = 2.389 | val_acc = 0.111 |\n",
            "Epoch 3: loss = 2.385 | acc = 0.1 | val_loss = 2.383 | val_acc = 0.09 |\n",
            "Epoch 4: loss = 2.389 | acc = 0.098 | val_loss = 2.392 | val_acc = 0.112 |\n",
            "Epoch 5: loss = 2.39 | acc = 0.099 | val_loss = 2.387 | val_acc = 0.093 |\n",
            "Epoch 6: loss = 2.394 | acc = 0.098 | val_loss = 2.395 | val_acc = 0.114 |\n",
            "Epoch 7: loss = 2.394 | acc = 0.101 | val_loss = 2.39 | val_acc = 0.091 |\n",
            "Epoch 8: loss = 2.396 | acc = 0.101 | val_loss = 2.398 | val_acc = 0.116 |\n",
            "Epoch 9: loss = 2.398 | acc = 0.101 | val_loss = 2.394 | val_acc = 0.092 |\n",
            "Epoch 10: loss = 2.4 | acc = 0.101 | val_loss = 2.401 | val_acc = 0.117 |\n",
            "Epoch 11: loss = 2.4 | acc = 0.103 | val_loss = 2.396 | val_acc = 0.093 |\n",
            "Epoch 12: loss = 2.402 | acc = 0.103 | val_loss = 2.404 | val_acc = 0.119 |\n",
            "Epoch 13: loss = 2.401 | acc = 0.103 | val_loss = 2.398 | val_acc = 0.094 |\n",
            "Epoch 14: loss = 2.403 | acc = 0.103 | val_loss = 2.407 | val_acc = 0.12 |\n",
            "Epoch 15: loss = 2.405 | acc = 0.101 | val_loss = 2.399 | val_acc = 0.094 |\n",
            "Epoch 16: loss = 2.405 | acc = 0.103 | val_loss = 2.409 | val_acc = 0.121 |\n",
            "Epoch 17: loss = 2.406 | acc = 0.103 | val_loss = 2.4 | val_acc = 0.095 |\n",
            "Epoch 18: loss = 2.406 | acc = 0.102 | val_loss = 2.412 | val_acc = 0.122 |\n",
            "Epoch 19: loss = 2.406 | acc = 0.104 | val_loss = 2.4 | val_acc = 0.091 |\n",
            "Epoch 20: loss = 2.41 | acc = 0.102 | val_loss = 2.414 | val_acc = 0.122 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.395 | acc = 0.097 | val_loss = 2.392 | val_acc = 0.079 |\n",
            "Epoch 2: loss = 2.408 | acc = 0.099 | val_loss = 2.427 | val_acc = 0.12 |\n",
            "Epoch 3: loss = 2.42 | acc = 0.102 | val_loss = 2.413 | val_acc = 0.075 |\n",
            "Epoch 4: loss = 2.427 | acc = 0.105 | val_loss = 2.442 | val_acc = 0.133 |\n",
            "Epoch 5: loss = 2.439 | acc = 0.106 | val_loss = 2.429 | val_acc = 0.079 |\n",
            "Epoch 6: loss = 2.444 | acc = 0.113 | val_loss = 2.46 | val_acc = 0.135 |\n",
            "Epoch 7: loss = 2.449 | acc = 0.115 | val_loss = 2.436 | val_acc = 0.095 |\n",
            "Epoch 8: loss = 2.453 | acc = 0.116 | val_loss = 2.477 | val_acc = 0.133 |\n",
            "Epoch 9: loss = 2.463 | acc = 0.119 | val_loss = 2.443 | val_acc = 0.108 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.423 | acc = 0.101 | val_loss = 2.413 | val_acc = 0.089 |\n",
            "Epoch 2: loss = 2.431 | acc = 0.094 | val_loss = 2.416 | val_acc = 0.122 |\n",
            "Epoch 3: loss = 2.419 | acc = 0.109 | val_loss = 2.428 | val_acc = 0.086 |\n",
            "Epoch 4: loss = 2.42 | acc = 0.103 | val_loss = 2.417 | val_acc = 0.116 |\n",
            "Epoch 5: loss = 2.425 | acc = 0.095 | val_loss = 2.437 | val_acc = 0.078 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.412 | acc = 0.111 | val_loss = 2.425 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.421 | acc = 0.117 | val_loss = 2.411 | val_acc = 0.126 |\n",
            "Epoch 3: loss = 2.421 | acc = 0.12 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.418 | acc = 0.147 | val_loss = 2.411 | val_acc = 0.194 |\n",
            "Epoch 5: loss = 2.42 | acc = 0.15 | val_loss = 2.426 | val_acc = 0.099 |\n",
            "Epoch 6: loss = 2.415 | acc = 0.146 | val_loss = 2.411 | val_acc = 0.17 |\n",
            "Epoch 7: loss = 2.417 | acc = 0.147 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.415 | acc = 0.098 | val_loss = 2.411 | val_acc = 0.103 |\n",
            "Epoch 2: loss = 2.417 | acc = 0.104 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.421 | acc = 0.114 | val_loss = 2.411 | val_acc = 0.134 |\n",
            "Epoch 4: loss = 2.422 | acc = 0.119 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.416 | acc = 0.124 | val_loss = 2.411 | val_acc = 0.148 |\n",
            "Epoch 6: loss = 2.418 | acc = 0.126 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Epoch 7: loss = 2.42 | acc = 0.128 | val_loss = 2.411 | val_acc = 0.152 |\n",
            "Epoch 8: loss = 2.419 | acc = 0.126 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Epoch 9: loss = 2.419 | acc = 0.125 | val_loss = 2.411 | val_acc = 0.15 |\n",
            "Epoch 10: loss = 2.419 | acc = 0.126 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.376 | acc = 0.116 | val_loss = 2.382 | val_acc = 0.106 |\n",
            "Epoch 2: loss = 2.376 | acc = 0.116 | val_loss = 2.383 | val_acc = 0.106 |\n",
            "Epoch 3: loss = 2.378 | acc = 0.118 | val_loss = 2.384 | val_acc = 0.106 |\n",
            "Epoch 4: loss = 2.377 | acc = 0.116 | val_loss = 2.384 | val_acc = 0.105 |\n",
            "Epoch 5: loss = 2.378 | acc = 0.117 | val_loss = 2.385 | val_acc = 0.105 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.374 | acc = 0.114 | val_loss = 2.395 | val_acc = 0.095 |\n",
            "Epoch 2: loss = 2.382 | acc = 0.117 | val_loss = 2.4 | val_acc = 0.095 |\n",
            "Epoch 3: loss = 2.39 | acc = 0.117 | val_loss = 2.406 | val_acc = 0.096 |\n",
            "Epoch 4: loss = 2.396 | acc = 0.115 | val_loss = 2.41 | val_acc = 0.093 |\n",
            "Epoch 5: loss = 2.399 | acc = 0.114 | val_loss = 2.411 | val_acc = 0.093 |\n",
            "Epoch 6: loss = 2.4 | acc = 0.116 | val_loss = 2.415 | val_acc = 0.094 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.39 | acc = 0.101 | val_loss = 2.413 | val_acc = 0.107 |\n",
            "Epoch 2: loss = 2.415 | acc = 0.109 | val_loss = 2.419 | val_acc = 0.119 |\n",
            "Epoch 3: loss = 2.426 | acc = 0.111 | val_loss = 2.426 | val_acc = 0.113 |\n",
            "Epoch 4: loss = 2.441 | acc = 0.114 | val_loss = 2.445 | val_acc = 0.13 |\n",
            "Epoch 5: loss = 2.447 | acc = 0.128 | val_loss = 2.452 | val_acc = 0.152 |\n",
            "Epoch 6: loss = 2.453 | acc = 0.138 | val_loss = 2.458 | val_acc = 0.168 |\n",
            "Epoch 7: loss = 2.454 | acc = 0.146 | val_loss = 2.458 | val_acc = 0.168 |\n",
            "Epoch 8: loss = 2.451 | acc = 0.153 | val_loss = 2.45 | val_acc = 0.164 |\n",
            "Epoch 9: loss = 2.45 | acc = 0.156 | val_loss = 2.452 | val_acc = 0.143 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.425 | acc = 0.125 | val_loss = 2.456 | val_acc = 0.149 |\n",
            "Epoch 2: loss = 2.433 | acc = 0.124 | val_loss = 2.422 | val_acc = 0.131 |\n",
            "Epoch 3: loss = 2.429 | acc = 0.113 | val_loss = 2.425 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.425 | acc = 0.105 | val_loss = 2.426 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.418 | acc = 0.105 | val_loss = 2.426 | val_acc = 0.105 |\n",
            "Epoch 2: loss = 2.42 | acc = 0.115 | val_loss = 2.426 | val_acc = 0.136 |\n",
            "Epoch 3: loss = 2.42 | acc = 0.116 | val_loss = 2.426 | val_acc = 0.12 |\n",
            "Epoch 4: loss = 2.415 | acc = 0.114 | val_loss = 2.426 | val_acc = 0.119 |\n",
            "Epoch 5: loss = 2.417 | acc = 0.112 | val_loss = 2.426 | val_acc = 0.118 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.102 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.101 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.099 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.102 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.098 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.099 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.096 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.095 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.098 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.104 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.102 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.09 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.092 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.096 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 10~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.093 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-e3b905e1d7b4>:393: RuntimeWarning: overflow encountered in multiply\n",
            "  regularized_term_l2 = 2 * self.l2_lambda * self.weights[-i]  # L2 regularization term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.101 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.101 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.096 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.101 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.099 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.102 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.096 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.099 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.102 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 100~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = nan | acc = 0.103 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 2: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 3: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Epoch 4: loss = nan | acc = 0.1 | val_loss = nan | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.338 | acc = 0.089 | val_loss = 2.318 | val_acc = 0.093 |\n",
            "Epoch 2: loss = 2.297 | acc = 0.097 | val_loss = 2.284 | val_acc = 0.098 |\n",
            "Epoch 3: loss = 2.266 | acc = 0.103 | val_loss = 2.257 | val_acc = 0.105 |\n",
            "Epoch 4: loss = 2.241 | acc = 0.109 | val_loss = 2.233 | val_acc = 0.113 |\n",
            "Epoch 5: loss = 2.215 | acc = 0.127 | val_loss = 2.208 | val_acc = 0.146 |\n",
            "Epoch 6: loss = 2.183 | acc = 0.199 | val_loss = 2.17 | val_acc = 0.255 |\n",
            "Epoch 7: loss = 2.14 | acc = 0.316 | val_loss = 2.129 | val_acc = 0.335 |\n",
            "Epoch 8: loss = 2.108 | acc = 0.366 | val_loss = 2.102 | val_acc = 0.37 |\n",
            "Epoch 9: loss = 2.084 | acc = 0.394 | val_loss = 2.08 | val_acc = 0.397 |\n",
            "Epoch 10: loss = 2.065 | acc = 0.415 | val_loss = 2.064 | val_acc = 0.41 |\n",
            "Epoch 11: loss = 2.05 | acc = 0.429 | val_loss = 2.05 | val_acc = 0.426 |\n",
            "Epoch 12: loss = 2.036 | acc = 0.443 | val_loss = 2.037 | val_acc = 0.442 |\n",
            "Epoch 13: loss = 2.024 | acc = 0.455 | val_loss = 2.026 | val_acc = 0.449 |\n",
            "Epoch 14: loss = 2.014 | acc = 0.464 | val_loss = 2.017 | val_acc = 0.458 |\n",
            "Epoch 15: loss = 2.004 | acc = 0.471 | val_loss = 2.006 | val_acc = 0.467 |\n",
            "Epoch 16: loss = 1.989 | acc = 0.488 | val_loss = 1.986 | val_acc = 0.497 |\n",
            "Epoch 17: loss = 1.966 | acc = 0.522 | val_loss = 1.965 | val_acc = 0.525 |\n",
            "Epoch 18: loss = 1.95 | acc = 0.541 | val_loss = 1.951 | val_acc = 0.536 |\n",
            "Epoch 19: loss = 1.938 | acc = 0.552 | val_loss = 1.94 | val_acc = 0.548 |\n",
            "Epoch 20: loss = 1.929 | acc = 0.56 | val_loss = 1.931 | val_acc = 0.554 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.115 | acc = 0.273 | val_loss = 1.953 | val_acc = 0.501 |\n",
            "Epoch 2: loss = 1.895 | acc = 0.565 | val_loss = 1.861 | val_acc = 0.599 |\n",
            "Epoch 3: loss = 1.838 | acc = 0.623 | val_loss = 1.824 | val_acc = 0.624 |\n",
            "Epoch 4: loss = 1.807 | acc = 0.65 | val_loss = 1.803 | val_acc = 0.65 |\n",
            "Epoch 5: loss = 1.789 | acc = 0.662 | val_loss = 1.785 | val_acc = 0.662 |\n",
            "Epoch 6: loss = 1.775 | acc = 0.673 | val_loss = 1.775 | val_acc = 0.665 |\n",
            "Epoch 7: loss = 1.766 | acc = 0.683 | val_loss = 1.767 | val_acc = 0.679 |\n",
            "Epoch 8: loss = 1.756 | acc = 0.692 | val_loss = 1.76 | val_acc = 0.69 |\n",
            "Epoch 9: loss = 1.75 | acc = 0.699 | val_loss = 1.755 | val_acc = 0.69 |\n",
            "Epoch 10: loss = 1.744 | acc = 0.705 | val_loss = 1.751 | val_acc = 0.697 |\n",
            "Epoch 11: loss = 1.741 | acc = 0.707 | val_loss = 1.748 | val_acc = 0.697 |\n",
            "Epoch 12: loss = 1.735 | acc = 0.712 | val_loss = 1.739 | val_acc = 0.707 |\n",
            "Epoch 13: loss = 1.731 | acc = 0.714 | val_loss = 1.737 | val_acc = 0.708 |\n",
            "Epoch 14: loss = 1.727 | acc = 0.721 | val_loss = 1.735 | val_acc = 0.708 |\n",
            "Epoch 15: loss = 1.725 | acc = 0.722 | val_loss = 1.731 | val_acc = 0.711 |\n",
            "Epoch 16: loss = 1.722 | acc = 0.723 | val_loss = 1.729 | val_acc = 0.714 |\n",
            "Epoch 17: loss = 1.719 | acc = 0.724 | val_loss = 1.725 | val_acc = 0.714 |\n",
            "Epoch 18: loss = 1.717 | acc = 0.724 | val_loss = 1.724 | val_acc = 0.716 |\n",
            "Epoch 19: loss = 1.716 | acc = 0.725 | val_loss = 1.723 | val_acc = 0.714 |\n",
            "Epoch 20: loss = 1.717 | acc = 0.725 | val_loss = 1.722 | val_acc = 0.716 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.848 | acc = 0.525 | val_loss = 1.774 | val_acc = 0.603 |\n",
            "Epoch 2: loss = 1.749 | acc = 0.592 | val_loss = 1.726 | val_acc = 0.597 |\n",
            "Epoch 3: loss = 1.72 | acc = 0.603 | val_loss = 1.724 | val_acc = 0.587 |\n",
            "Epoch 4: loss = 1.718 | acc = 0.585 | val_loss = 1.733 | val_acc = 0.624 |\n",
            "Epoch 5: loss = 1.716 | acc = 0.616 | val_loss = 1.722 | val_acc = 0.616 |\n",
            "Epoch 6: loss = 1.724 | acc = 0.618 | val_loss = 1.737 | val_acc = 0.598 |\n",
            "Epoch 7: loss = 1.734 | acc = 0.579 | val_loss = 1.745 | val_acc = 0.554 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.106 | acc = 0.251 | val_loss = 2.029 | val_acc = 0.297 |\n",
            "Epoch 2: loss = 2.088 | acc = 0.222 | val_loss = 2.323 | val_acc = 0.118 |\n",
            "Epoch 3: loss = 2.18 | acc = 0.2 | val_loss = 2.252 | val_acc = 0.101 |\n",
            "Epoch 4: loss = 2.243 | acc = 0.156 | val_loss = 2.189 | val_acc = 0.158 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.371 | acc = 0.103 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.312 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.367 | acc = 0.131 | val_loss = 2.355 | val_acc = 0.128 |\n",
            "Epoch 2: loss = 2.333 | acc = 0.126 | val_loss = 2.314 | val_acc = 0.122 |\n",
            "Epoch 3: loss = 2.283 | acc = 0.122 | val_loss = 2.26 | val_acc = 0.12 |\n",
            "Epoch 4: loss = 2.235 | acc = 0.119 | val_loss = 2.222 | val_acc = 0.12 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.187 | acc = 0.278 | val_loss = 2.077 | val_acc = 0.394 |\n",
            "Epoch 2: loss = 2.006 | acc = 0.455 | val_loss = 1.96 | val_acc = 0.491 |\n",
            "Epoch 3: loss = 1.927 | acc = 0.525 | val_loss = 1.906 | val_acc = 0.536 |\n",
            "Epoch 4: loss = 1.884 | acc = 0.563 | val_loss = 1.873 | val_acc = 0.572 |\n",
            "Epoch 5: loss = 1.857 | acc = 0.581 | val_loss = 1.849 | val_acc = 0.592 |\n",
            "Epoch 6: loss = 1.837 | acc = 0.595 | val_loss = 1.833 | val_acc = 0.597 |\n",
            "Epoch 7: loss = 1.821 | acc = 0.607 | val_loss = 1.818 | val_acc = 0.608 |\n",
            "Epoch 8: loss = 1.808 | acc = 0.618 | val_loss = 1.806 | val_acc = 0.618 |\n",
            "Epoch 9: loss = 1.798 | acc = 0.624 | val_loss = 1.798 | val_acc = 0.622 |\n",
            "Epoch 10: loss = 1.789 | acc = 0.633 | val_loss = 1.79 | val_acc = 0.63 |\n",
            "Epoch 11: loss = 1.781 | acc = 0.638 | val_loss = 1.783 | val_acc = 0.633 |\n",
            "Epoch 12: loss = 1.775 | acc = 0.643 | val_loss = 1.778 | val_acc = 0.642 |\n",
            "Epoch 13: loss = 1.769 | acc = 0.649 | val_loss = 1.773 | val_acc = 0.639 |\n",
            "Epoch 14: loss = 1.765 | acc = 0.652 | val_loss = 1.769 | val_acc = 0.649 |\n",
            "Epoch 15: loss = 1.761 | acc = 0.657 | val_loss = 1.765 | val_acc = 0.648 |\n",
            "Epoch 16: loss = 1.757 | acc = 0.66 | val_loss = 1.761 | val_acc = 0.65 |\n",
            "Epoch 17: loss = 1.754 | acc = 0.662 | val_loss = 1.759 | val_acc = 0.654 |\n",
            "Epoch 18: loss = 1.751 | acc = 0.664 | val_loss = 1.756 | val_acc = 0.656 |\n",
            "Epoch 19: loss = 1.749 | acc = 0.666 | val_loss = 1.754 | val_acc = 0.656 |\n",
            "Epoch 20: loss = 1.746 | acc = 0.668 | val_loss = 1.751 | val_acc = 0.664 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.921 | acc = 0.496 | val_loss = 1.792 | val_acc = 0.622 |\n",
            "Epoch 2: loss = 1.762 | acc = 0.633 | val_loss = 1.751 | val_acc = 0.64 |\n",
            "Epoch 3: loss = 1.734 | acc = 0.656 | val_loss = 1.73 | val_acc = 0.657 |\n",
            "Epoch 4: loss = 1.723 | acc = 0.674 | val_loss = 1.722 | val_acc = 0.674 |\n",
            "Epoch 5: loss = 1.719 | acc = 0.688 | val_loss = 1.718 | val_acc = 0.684 |\n",
            "Epoch 6: loss = 1.712 | acc = 0.698 | val_loss = 1.714 | val_acc = 0.692 |\n",
            "Epoch 7: loss = 1.711 | acc = 0.685 | val_loss = 1.707 | val_acc = 0.679 |\n",
            "Epoch 8: loss = 1.704 | acc = 0.698 | val_loss = 1.712 | val_acc = 0.698 |\n",
            "Epoch 9: loss = 1.707 | acc = 0.704 | val_loss = 1.711 | val_acc = 0.689 |\n",
            "Epoch 10: loss = 1.688 | acc = 0.708 | val_loss = 1.672 | val_acc = 0.707 |\n",
            "Epoch 11: loss = 1.668 | acc = 0.701 | val_loss = 1.677 | val_acc = 0.7 |\n",
            "Epoch 12: loss = 1.664 | acc = 0.703 | val_loss = 1.665 | val_acc = 0.708 |\n",
            "Epoch 13: loss = 1.667 | acc = 0.71 | val_loss = 1.665 | val_acc = 0.71 |\n",
            "Epoch 14: loss = 1.66 | acc = 0.714 | val_loss = 1.66 | val_acc = 0.707 |\n",
            "Epoch 15: loss = 1.661 | acc = 0.708 | val_loss = 1.666 | val_acc = 0.699 |\n",
            "Epoch 16: loss = 1.661 | acc = 0.706 | val_loss = 1.666 | val_acc = 0.695 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.067 | acc = 0.312 | val_loss = 2.156 | val_acc = 0.248 |\n",
            "Epoch 2: loss = 2.031 | acc = 0.271 | val_loss = 1.982 | val_acc = 0.291 |\n",
            "Epoch 3: loss = 2.063 | acc = 0.247 | val_loss = 2.066 | val_acc = 0.291 |\n",
            "Epoch 4: loss = 2.028 | acc = 0.257 | val_loss = 2.029 | val_acc = 0.274 |\n",
            "Epoch 5: loss = 2.002 | acc = 0.269 | val_loss = 1.974 | val_acc = 0.25 |\n",
            "Epoch 6: loss = 1.972 | acc = 0.288 | val_loss = 1.98 | val_acc = 0.266 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.403 | acc = 0.137 | val_loss = 2.411 | val_acc = 0.103 |\n",
            "Epoch 2: loss = 2.412 | acc = 0.104 | val_loss = 2.411 | val_acc = 0.105 |\n",
            "Epoch 3: loss = 2.411 | acc = 0.105 | val_loss = 2.411 | val_acc = 0.105 |\n",
            "Epoch 4: loss = 2.411 | acc = 0.105 | val_loss = 2.411 | val_acc = 0.105 |\n",
            "Epoch 5: loss = 2.411 | acc = 0.105 | val_loss = 2.411 | val_acc = 0.105 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.353 | acc = 0.101 | val_loss = 2.339 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 2.325 | acc = 0.101 | val_loss = 2.315 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.102 | val_loss = 2.296 | val_acc = 0.102 |\n",
            "Epoch 4: loss = 2.287 | acc = 0.102 | val_loss = 2.28 | val_acc = 0.102 |\n",
            "Epoch 5: loss = 2.272 | acc = 0.103 | val_loss = 2.267 | val_acc = 0.102 |\n",
            "Epoch 6: loss = 2.258 | acc = 0.103 | val_loss = 2.253 | val_acc = 0.102 |\n",
            "Epoch 7: loss = 2.245 | acc = 0.104 | val_loss = 2.239 | val_acc = 0.103 |\n",
            "Epoch 8: loss = 2.231 | acc = 0.104 | val_loss = 2.226 | val_acc = 0.104 |\n",
            "Epoch 9: loss = 2.217 | acc = 0.105 | val_loss = 2.211 | val_acc = 0.105 |\n",
            "Epoch 10: loss = 2.204 | acc = 0.106 | val_loss = 2.197 | val_acc = 0.105 |\n",
            "Epoch 11: loss = 2.189 | acc = 0.106 | val_loss = 2.183 | val_acc = 0.105 |\n",
            "Epoch 12: loss = 2.176 | acc = 0.107 | val_loss = 2.17 | val_acc = 0.106 |\n",
            "Epoch 13: loss = 2.164 | acc = 0.107 | val_loss = 2.159 | val_acc = 0.107 |\n",
            "Epoch 14: loss = 2.152 | acc = 0.108 | val_loss = 2.148 | val_acc = 0.108 |\n",
            "Epoch 15: loss = 2.142 | acc = 0.109 | val_loss = 2.138 | val_acc = 0.108 |\n",
            "Epoch 16: loss = 2.133 | acc = 0.11 | val_loss = 2.129 | val_acc = 0.109 |\n",
            "Epoch 17: loss = 2.124 | acc = 0.11 | val_loss = 2.121 | val_acc = 0.11 |\n",
            "Epoch 18: loss = 2.116 | acc = 0.112 | val_loss = 2.113 | val_acc = 0.11 |\n",
            "Epoch 19: loss = 2.108 | acc = 0.113 | val_loss = 2.105 | val_acc = 0.111 |\n",
            "Epoch 20: loss = 2.1 | acc = 0.114 | val_loss = 2.098 | val_acc = 0.112 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.25 | acc = 0.102 | val_loss = 2.177 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.13 | acc = 0.105 | val_loss = 2.09 | val_acc = 0.107 |\n",
            "Epoch 3: loss = 2.066 | acc = 0.112 | val_loss = 2.044 | val_acc = 0.115 |\n",
            "Epoch 4: loss = 2.027 | acc = 0.124 | val_loss = 2.01 | val_acc = 0.13 |\n",
            "Epoch 5: loss = 1.98 | acc = 0.144 | val_loss = 1.947 | val_acc = 0.16 |\n",
            "Epoch 6: loss = 1.928 | acc = 0.182 | val_loss = 1.915 | val_acc = 0.208 |\n",
            "Epoch 7: loss = 1.902 | acc = 0.237 | val_loss = 1.892 | val_acc = 0.281 |\n",
            "Epoch 8: loss = 1.879 | acc = 0.364 | val_loss = 1.867 | val_acc = 0.474 |\n",
            "Epoch 9: loss = 1.845 | acc = 0.54 | val_loss = 1.835 | val_acc = 0.562 |\n",
            "Epoch 10: loss = 1.819 | acc = 0.575 | val_loss = 1.815 | val_acc = 0.577 |\n",
            "Epoch 11: loss = 1.803 | acc = 0.587 | val_loss = 1.801 | val_acc = 0.586 |\n",
            "Epoch 12: loss = 1.79 | acc = 0.594 | val_loss = 1.79 | val_acc = 0.595 |\n",
            "Epoch 13: loss = 1.78 | acc = 0.604 | val_loss = 1.781 | val_acc = 0.603 |\n",
            "Epoch 14: loss = 1.771 | acc = 0.615 | val_loss = 1.773 | val_acc = 0.608 |\n",
            "Epoch 15: loss = 1.763 | acc = 0.622 | val_loss = 1.766 | val_acc = 0.616 |\n",
            "Epoch 16: loss = 1.756 | acc = 0.628 | val_loss = 1.76 | val_acc = 0.625 |\n",
            "Epoch 17: loss = 1.75 | acc = 0.637 | val_loss = 1.754 | val_acc = 0.633 |\n",
            "Epoch 18: loss = 1.745 | acc = 0.64 | val_loss = 1.75 | val_acc = 0.634 |\n",
            "Epoch 19: loss = 1.74 | acc = 0.645 | val_loss = 1.745 | val_acc = 0.635 |\n",
            "Epoch 20: loss = 1.736 | acc = 0.65 | val_loss = 1.74 | val_acc = 0.644 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.083 | acc = 0.186 | val_loss = 1.889 | val_acc = 0.516 |\n",
            "Epoch 2: loss = 1.83 | acc = 0.565 | val_loss = 1.804 | val_acc = 0.579 |\n",
            "Epoch 3: loss = 1.786 | acc = 0.599 | val_loss = 1.785 | val_acc = 0.605 |\n",
            "Epoch 4: loss = 1.772 | acc = 0.616 | val_loss = 1.769 | val_acc = 0.618 |\n",
            "Epoch 5: loss = 1.76 | acc = 0.628 | val_loss = 1.761 | val_acc = 0.624 |\n",
            "Epoch 6: loss = 1.752 | acc = 0.635 | val_loss = 1.756 | val_acc = 0.631 |\n",
            "Epoch 7: loss = 1.749 | acc = 0.638 | val_loss = 1.753 | val_acc = 0.624 |\n",
            "Epoch 8: loss = 1.742 | acc = 0.639 | val_loss = 1.745 | val_acc = 0.631 |\n",
            "Epoch 9: loss = 1.738 | acc = 0.643 | val_loss = 1.743 | val_acc = 0.629 |\n",
            "Epoch 10: loss = 1.732 | acc = 0.638 | val_loss = 1.739 | val_acc = 0.633 |\n",
            "Epoch 11: loss = 1.73 | acc = 0.645 | val_loss = 1.736 | val_acc = 0.641 |\n",
            "Epoch 12: loss = 1.725 | acc = 0.647 | val_loss = 1.732 | val_acc = 0.639 |\n",
            "Epoch 13: loss = 1.714 | acc = 0.682 | val_loss = 1.694 | val_acc = 0.718 |\n",
            "Epoch 14: loss = 1.672 | acc = 0.735 | val_loss = 1.674 | val_acc = 0.712 |\n",
            "Epoch 15: loss = 1.658 | acc = 0.733 | val_loss = 1.661 | val_acc = 0.725 |\n",
            "Epoch 16: loss = 1.655 | acc = 0.734 | val_loss = 1.661 | val_acc = 0.728 |\n",
            "Epoch 17: loss = 1.65 | acc = 0.742 | val_loss = 1.652 | val_acc = 0.736 |\n",
            "Epoch 18: loss = 1.651 | acc = 0.744 | val_loss = 1.656 | val_acc = 0.74 |\n",
            "Epoch 19: loss = 1.649 | acc = 0.745 | val_loss = 1.656 | val_acc = 0.736 |\n",
            "Epoch 20: loss = 1.645 | acc = 0.743 | val_loss = 1.647 | val_acc = 0.727 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.853 | acc = 0.505 | val_loss = 1.805 | val_acc = 0.534 |\n",
            "Epoch 2: loss = 1.765 | acc = 0.515 | val_loss = 1.836 | val_acc = 0.459 |\n",
            "Epoch 3: loss = 1.85 | acc = 0.429 | val_loss = 1.857 | val_acc = 0.38 |\n",
            "Epoch 4: loss = 1.872 | acc = 0.397 | val_loss = 1.81 | val_acc = 0.478 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.354 | acc = 0.116 | val_loss = 2.361 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.361 | acc = 0.1 | val_loss = 2.361 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.361 | acc = 0.1 | val_loss = 2.361 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.361 | acc = 0.1 | val_loss = 2.361 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.37 | acc = 0.093 | val_loss = 2.365 | val_acc = 0.093 |\n",
            "Epoch 2: loss = 2.36 | acc = 0.092 | val_loss = 2.356 | val_acc = 0.092 |\n",
            "Epoch 3: loss = 2.35 | acc = 0.091 | val_loss = 2.347 | val_acc = 0.09 |\n",
            "Epoch 4: loss = 2.341 | acc = 0.09 | val_loss = 2.339 | val_acc = 0.09 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.343 | acc = 0.094 | val_loss = 2.321 | val_acc = 0.092 |\n",
            "Epoch 2: loss = 2.307 | acc = 0.09 | val_loss = 2.298 | val_acc = 0.089 |\n",
            "Epoch 3: loss = 2.286 | acc = 0.089 | val_loss = 2.278 | val_acc = 0.087 |\n",
            "Epoch 4: loss = 2.265 | acc = 0.087 | val_loss = 2.259 | val_acc = 0.087 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.123 | acc = 0.127 | val_loss = 1.968 | val_acc = 0.193 |\n",
            "Epoch 2: loss = 1.905 | acc = 0.294 | val_loss = 1.869 | val_acc = 0.381 |\n",
            "Epoch 3: loss = 1.83 | acc = 0.506 | val_loss = 1.796 | val_acc = 0.598 |\n",
            "Epoch 4: loss = 1.766 | acc = 0.635 | val_loss = 1.753 | val_acc = 0.652 |\n",
            "Epoch 5: loss = 1.736 | acc = 0.67 | val_loss = 1.729 | val_acc = 0.669 |\n",
            "Epoch 6: loss = 1.717 | acc = 0.681 | val_loss = 1.715 | val_acc = 0.681 |\n",
            "Epoch 7: loss = 1.704 | acc = 0.695 | val_loss = 1.706 | val_acc = 0.686 |\n",
            "Epoch 8: loss = 1.694 | acc = 0.7 | val_loss = 1.695 | val_acc = 0.697 |\n",
            "Epoch 9: loss = 1.686 | acc = 0.711 | val_loss = 1.691 | val_acc = 0.703 |\n",
            "Epoch 10: loss = 1.68 | acc = 0.713 | val_loss = 1.684 | val_acc = 0.702 |\n",
            "Epoch 11: loss = 1.676 | acc = 0.711 | val_loss = 1.68 | val_acc = 0.709 |\n",
            "Epoch 12: loss = 1.671 | acc = 0.72 | val_loss = 1.676 | val_acc = 0.708 |\n",
            "Epoch 13: loss = 1.667 | acc = 0.723 | val_loss = 1.672 | val_acc = 0.714 |\n",
            "Epoch 14: loss = 1.664 | acc = 0.726 | val_loss = 1.672 | val_acc = 0.716 |\n",
            "Epoch 15: loss = 1.66 | acc = 0.733 | val_loss = 1.666 | val_acc = 0.722 |\n",
            "Epoch 16: loss = 1.656 | acc = 0.733 | val_loss = 1.664 | val_acc = 0.724 |\n",
            "Epoch 17: loss = 1.655 | acc = 0.731 | val_loss = 1.663 | val_acc = 0.722 |\n",
            "Epoch 18: loss = 1.651 | acc = 0.737 | val_loss = 1.659 | val_acc = 0.723 |\n",
            "Epoch 19: loss = 1.649 | acc = 0.737 | val_loss = 1.657 | val_acc = 0.735 |\n",
            "Epoch 20: loss = 1.648 | acc = 0.744 | val_loss = 1.655 | val_acc = 0.734 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.93 | acc = 0.493 | val_loss = 1.869 | val_acc = 0.549 |\n",
            "Epoch 2: loss = 1.852 | acc = 0.575 | val_loss = 1.863 | val_acc = 0.551 |\n",
            "Epoch 3: loss = 1.853 | acc = 0.561 | val_loss = 1.854 | val_acc = 0.542 |\n",
            "Epoch 4: loss = 1.837 | acc = 0.553 | val_loss = 1.763 | val_acc = 0.592 |\n",
            "Epoch 5: loss = 1.781 | acc = 0.589 | val_loss = 1.758 | val_acc = 0.606 |\n",
            "Epoch 6: loss = 1.771 | acc = 0.601 | val_loss = 1.798 | val_acc = 0.565 |\n",
            "Epoch 7: loss = 1.774 | acc = 0.582 | val_loss = 1.734 | val_acc = 0.64 |\n",
            "Epoch 8: loss = 1.736 | acc = 0.631 | val_loss = 1.733 | val_acc = 0.624 |\n",
            "Epoch 9: loss = 1.73 | acc = 0.627 | val_loss = 1.736 | val_acc = 0.63 |\n",
            "Epoch 10: loss = 1.742 | acc = 0.598 | val_loss = 1.758 | val_acc = 0.615 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.17 | acc = 0.151 | val_loss = 2.077 | val_acc = 0.156 |\n",
            "Epoch 2: loss = 2.202 | acc = 0.163 | val_loss = 2.397 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.322 | acc = 0.147 | val_loss = 2.303 | val_acc = 0.118 |\n",
            "Epoch 4: loss = 2.305 | acc = 0.158 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.184 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.184 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 7: loss = 2.303 | acc = 0.184 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 8: loss = 2.303 | acc = 0.184 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 9: loss = 2.303 | acc = 0.184 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 10: loss = 2.303 | acc = 0.184 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 11: loss = 2.303 | acc = 0.145 | val_loss = 2.303 | val_acc = 0.136 |\n",
            "Epoch 12: loss = 2.303 | acc = 0.134 | val_loss = 2.303 | val_acc = 0.134 |\n",
            "Epoch 13: loss = 2.303 | acc = 0.131 | val_loss = 2.303 | val_acc = 0.13 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.398 | acc = 0.038 | val_loss = 2.364 | val_acc = 0.026 |\n",
            "Epoch 2: loss = 2.341 | acc = 0.031 | val_loss = 2.321 | val_acc = 0.036 |\n",
            "Epoch 3: loss = 2.3 | acc = 0.045 | val_loss = 2.285 | val_acc = 0.053 |\n",
            "Epoch 4: loss = 2.266 | acc = 0.062 | val_loss = 2.254 | val_acc = 0.07 |\n",
            "Epoch 5: loss = 2.237 | acc = 0.081 | val_loss = 2.226 | val_acc = 0.089 |\n",
            "Epoch 6: loss = 2.209 | acc = 0.095 | val_loss = 2.199 | val_acc = 0.099 |\n",
            "Epoch 7: loss = 2.182 | acc = 0.099 | val_loss = 2.175 | val_acc = 0.1 |\n",
            "Epoch 8: loss = 2.16 | acc = 0.1 | val_loss = 2.154 | val_acc = 0.1 |\n",
            "Epoch 9: loss = 2.141 | acc = 0.1 | val_loss = 2.137 | val_acc = 0.1 |\n",
            "Epoch 10: loss = 2.126 | acc = 0.101 | val_loss = 2.123 | val_acc = 0.101 |\n",
            "Epoch 11: loss = 2.112 | acc = 0.101 | val_loss = 2.111 | val_acc = 0.101 |\n",
            "Epoch 12: loss = 2.1 | acc = 0.102 | val_loss = 2.097 | val_acc = 0.102 |\n",
            "Epoch 13: loss = 2.081 | acc = 0.102 | val_loss = 2.074 | val_acc = 0.102 |\n",
            "Epoch 14: loss = 2.063 | acc = 0.103 | val_loss = 2.061 | val_acc = 0.103 |\n",
            "Epoch 15: loss = 2.053 | acc = 0.104 | val_loss = 2.053 | val_acc = 0.104 |\n",
            "Epoch 16: loss = 2.045 | acc = 0.105 | val_loss = 2.046 | val_acc = 0.106 |\n",
            "Epoch 17: loss = 2.038 | acc = 0.107 | val_loss = 2.039 | val_acc = 0.108 |\n",
            "Epoch 18: loss = 2.031 | acc = 0.111 | val_loss = 2.033 | val_acc = 0.114 |\n",
            "Epoch 19: loss = 2.025 | acc = 0.115 | val_loss = 2.027 | val_acc = 0.121 |\n",
            "Epoch 20: loss = 2.02 | acc = 0.124 | val_loss = 2.022 | val_acc = 0.132 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.105 | acc = 0.343 | val_loss = 1.947 | val_acc = 0.507 |\n",
            "Epoch 2: loss = 1.894 | acc = 0.555 | val_loss = 1.867 | val_acc = 0.579 |\n",
            "Epoch 3: loss = 1.842 | acc = 0.604 | val_loss = 1.832 | val_acc = 0.608 |\n",
            "Epoch 4: loss = 1.813 | acc = 0.627 | val_loss = 1.804 | val_acc = 0.629 |\n",
            "Epoch 5: loss = 1.752 | acc = 0.678 | val_loss = 1.731 | val_acc = 0.694 |\n",
            "Epoch 6: loss = 1.718 | acc = 0.707 | val_loss = 1.714 | val_acc = 0.711 |\n",
            "Epoch 7: loss = 1.705 | acc = 0.715 | val_loss = 1.704 | val_acc = 0.714 |\n",
            "Epoch 8: loss = 1.695 | acc = 0.725 | val_loss = 1.696 | val_acc = 0.719 |\n",
            "Epoch 9: loss = 1.687 | acc = 0.728 | val_loss = 1.689 | val_acc = 0.716 |\n",
            "Epoch 10: loss = 1.682 | acc = 0.731 | val_loss = 1.685 | val_acc = 0.722 |\n",
            "Epoch 11: loss = 1.677 | acc = 0.733 | val_loss = 1.681 | val_acc = 0.725 |\n",
            "Epoch 12: loss = 1.672 | acc = 0.736 | val_loss = 1.675 | val_acc = 0.734 |\n",
            "Epoch 13: loss = 1.667 | acc = 0.74 | val_loss = 1.673 | val_acc = 0.738 |\n",
            "Epoch 14: loss = 1.664 | acc = 0.743 | val_loss = 1.67 | val_acc = 0.737 |\n",
            "Epoch 15: loss = 1.661 | acc = 0.745 | val_loss = 1.668 | val_acc = 0.736 |\n",
            "Epoch 16: loss = 1.659 | acc = 0.748 | val_loss = 1.665 | val_acc = 0.736 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.896 | acc = 0.525 | val_loss = 1.818 | val_acc = 0.617 |\n",
            "Epoch 2: loss = 1.763 | acc = 0.603 | val_loss = 1.755 | val_acc = 0.654 |\n",
            "Epoch 3: loss = 1.734 | acc = 0.62 | val_loss = 1.723 | val_acc = 0.616 |\n",
            "Epoch 4: loss = 1.718 | acc = 0.595 | val_loss = 1.704 | val_acc = 0.577 |\n",
            "Epoch 5: loss = 1.716 | acc = 0.595 | val_loss = 1.726 | val_acc = 0.552 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.969 | acc = 0.268 | val_loss = 2.002 | val_acc = 0.27 |\n",
            "Epoch 2: loss = 1.967 | acc = 0.261 | val_loss = 2.071 | val_acc = 0.213 |\n",
            "Epoch 3: loss = 2.08 | acc = 0.192 | val_loss = 2.165 | val_acc = 0.172 |\n",
            "Epoch 4: loss = 2.131 | acc = 0.154 | val_loss = 2.236 | val_acc = 0.103 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.151 | val_loss = 2.303 | val_acc = 0.144 |\n",
            "Epoch 2: loss = 2.302 | acc = 0.147 | val_loss = 2.319 | val_acc = 0.15 |\n",
            "Epoch 3: loss = 2.32 | acc = 0.13 | val_loss = 2.234 | val_acc = 0.148 |\n",
            "Epoch 4: loss = 2.333 | acc = 0.101 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.337 | acc = 0.1 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.386 | acc = 0.102 | val_loss = 2.366 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.345 | acc = 0.101 | val_loss = 2.327 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 2.305 | acc = 0.1 | val_loss = 2.288 | val_acc = 0.101 |\n",
            "Epoch 4: loss = 2.269 | acc = 0.1 | val_loss = 2.254 | val_acc = 0.102 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.171 | acc = 0.262 | val_loss = 2.054 | val_acc = 0.388 |\n",
            "Epoch 2: loss = 2.005 | acc = 0.432 | val_loss = 1.974 | val_acc = 0.461 |\n",
            "Epoch 3: loss = 1.942 | acc = 0.496 | val_loss = 1.915 | val_acc = 0.53 |\n",
            "Epoch 4: loss = 1.885 | acc = 0.564 | val_loss = 1.871 | val_acc = 0.574 |\n",
            "Epoch 5: loss = 1.853 | acc = 0.59 | val_loss = 1.847 | val_acc = 0.592 |\n",
            "Epoch 6: loss = 1.834 | acc = 0.606 | val_loss = 1.831 | val_acc = 0.606 |\n",
            "Epoch 7: loss = 1.82 | acc = 0.619 | val_loss = 1.819 | val_acc = 0.618 |\n",
            "Epoch 8: loss = 1.809 | acc = 0.627 | val_loss = 1.81 | val_acc = 0.62 |\n",
            "Epoch 9: loss = 1.799 | acc = 0.635 | val_loss = 1.801 | val_acc = 0.632 |\n",
            "Epoch 10: loss = 1.791 | acc = 0.643 | val_loss = 1.795 | val_acc = 0.635 |\n",
            "Epoch 11: loss = 1.784 | acc = 0.649 | val_loss = 1.788 | val_acc = 0.641 |\n",
            "Epoch 12: loss = 1.779 | acc = 0.654 | val_loss = 1.784 | val_acc = 0.647 |\n",
            "Epoch 13: loss = 1.774 | acc = 0.659 | val_loss = 1.779 | val_acc = 0.652 |\n",
            "Epoch 14: loss = 1.77 | acc = 0.662 | val_loss = 1.775 | val_acc = 0.654 |\n",
            "Epoch 15: loss = 1.765 | acc = 0.664 | val_loss = 1.771 | val_acc = 0.656 |\n",
            "Epoch 16: loss = 1.763 | acc = 0.665 | val_loss = 1.769 | val_acc = 0.661 |\n",
            "Epoch 17: loss = 1.759 | acc = 0.67 | val_loss = 1.764 | val_acc = 0.659 |\n",
            "Epoch 18: loss = 1.756 | acc = 0.674 | val_loss = 1.763 | val_acc = 0.658 |\n",
            "Epoch 19: loss = 1.753 | acc = 0.674 | val_loss = 1.761 | val_acc = 0.665 |\n",
            "Epoch 20: loss = 1.751 | acc = 0.677 | val_loss = 1.758 | val_acc = 0.666 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.86 | acc = 0.58 | val_loss = 1.754 | val_acc = 0.646 |\n",
            "Epoch 2: loss = 1.728 | acc = 0.673 | val_loss = 1.711 | val_acc = 0.688 |\n",
            "Epoch 3: loss = 1.706 | acc = 0.694 | val_loss = 1.702 | val_acc = 0.696 |\n",
            "Epoch 4: loss = 1.692 | acc = 0.698 | val_loss = 1.691 | val_acc = 0.7 |\n",
            "Epoch 5: loss = 1.681 | acc = 0.707 | val_loss = 1.677 | val_acc = 0.702 |\n",
            "Epoch 6: loss = 1.67 | acc = 0.714 | val_loss = 1.688 | val_acc = 0.697 |\n",
            "Epoch 7: loss = 1.668 | acc = 0.72 | val_loss = 1.666 | val_acc = 0.709 |\n",
            "Epoch 8: loss = 1.663 | acc = 0.717 | val_loss = 1.668 | val_acc = 0.696 |\n",
            "Epoch 9: loss = 1.66 | acc = 0.711 | val_loss = 1.667 | val_acc = 0.689 |\n",
            "Epoch 10: loss = 1.656 | acc = 0.716 | val_loss = 1.659 | val_acc = 0.713 |\n",
            "Epoch 11: loss = 1.653 | acc = 0.725 | val_loss = 1.67 | val_acc = 0.714 |\n",
            "Epoch 12: loss = 1.653 | acc = 0.72 | val_loss = 1.655 | val_acc = 0.7 |\n",
            "Epoch 13: loss = 1.651 | acc = 0.722 | val_loss = 1.658 | val_acc = 0.709 |\n",
            "Epoch 14: loss = 1.655 | acc = 0.716 | val_loss = 1.658 | val_acc = 0.713 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.874 | acc = 0.39 | val_loss = 1.812 | val_acc = 0.349 |\n",
            "Epoch 2: loss = 1.868 | acc = 0.359 | val_loss = 1.83 | val_acc = 0.314 |\n",
            "Epoch 3: loss = 1.86 | acc = 0.346 | val_loss = 1.918 | val_acc = 0.241 |\n",
            "Epoch 4: loss = 1.875 | acc = 0.295 | val_loss = 1.865 | val_acc = 0.342 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.275 | acc = 0.144 | val_loss = 2.303 | val_acc = 0.186 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.187 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.186 | val_loss = 2.303 | val_acc = 0.185 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.192 | val_loss = 2.303 | val_acc = 0.194 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.195 | val_loss = 2.303 | val_acc = 0.194 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.195 | val_loss = 2.303 | val_acc = 0.194 |\n",
            "Epoch 7: loss = 2.303 | acc = 0.195 | val_loss = 2.303 | val_acc = 0.194 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.397 | acc = 0.094 | val_loss = 2.392 | val_acc = 0.095 |\n",
            "Epoch 2: loss = 2.389 | acc = 0.093 | val_loss = 2.384 | val_acc = 0.096 |\n",
            "Epoch 3: loss = 2.381 | acc = 0.092 | val_loss = 2.376 | val_acc = 0.095 |\n",
            "Epoch 4: loss = 2.373 | acc = 0.092 | val_loss = 2.367 | val_acc = 0.095 |\n",
            "Epoch 5: loss = 2.363 | acc = 0.091 | val_loss = 2.356 | val_acc = 0.094 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.262 | acc = 0.134 | val_loss = 2.187 | val_acc = 0.136 |\n",
            "Epoch 2: loss = 2.145 | acc = 0.15 | val_loss = 2.104 | val_acc = 0.168 |\n",
            "Epoch 3: loss = 2.048 | acc = 0.215 | val_loss = 2.011 | val_acc = 0.286 |\n",
            "Epoch 4: loss = 1.975 | acc = 0.395 | val_loss = 1.951 | val_acc = 0.461 |\n",
            "Epoch 5: loss = 1.921 | acc = 0.501 | val_loss = 1.906 | val_acc = 0.512 |\n",
            "Epoch 6: loss = 1.881 | acc = 0.546 | val_loss = 1.874 | val_acc = 0.551 |\n",
            "Epoch 7: loss = 1.853 | acc = 0.574 | val_loss = 1.85 | val_acc = 0.562 |\n",
            "Epoch 8: loss = 1.831 | acc = 0.592 | val_loss = 1.831 | val_acc = 0.584 |\n",
            "Epoch 9: loss = 1.816 | acc = 0.605 | val_loss = 1.818 | val_acc = 0.595 |\n",
            "Epoch 10: loss = 1.802 | acc = 0.613 | val_loss = 1.804 | val_acc = 0.603 |\n",
            "Epoch 11: loss = 1.791 | acc = 0.624 | val_loss = 1.794 | val_acc = 0.613 |\n",
            "Epoch 12: loss = 1.781 | acc = 0.633 | val_loss = 1.785 | val_acc = 0.625 |\n",
            "Epoch 13: loss = 1.772 | acc = 0.64 | val_loss = 1.776 | val_acc = 0.627 |\n",
            "Epoch 14: loss = 1.765 | acc = 0.645 | val_loss = 1.77 | val_acc = 0.635 |\n",
            "Epoch 15: loss = 1.758 | acc = 0.653 | val_loss = 1.763 | val_acc = 0.643 |\n",
            "Epoch 16: loss = 1.752 | acc = 0.656 | val_loss = 1.758 | val_acc = 0.649 |\n",
            "Epoch 17: loss = 1.747 | acc = 0.663 | val_loss = 1.752 | val_acc = 0.651 |\n",
            "Epoch 18: loss = 1.741 | acc = 0.665 | val_loss = 1.747 | val_acc = 0.655 |\n",
            "Epoch 19: loss = 1.737 | acc = 0.668 | val_loss = 1.743 | val_acc = 0.659 |\n",
            "Epoch 20: loss = 1.733 | acc = 0.669 | val_loss = 1.739 | val_acc = 0.66 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.0 | acc = 0.238 | val_loss = 1.866 | val_acc = 0.326 |\n",
            "Epoch 2: loss = 1.8 | acc = 0.534 | val_loss = 1.755 | val_acc = 0.624 |\n",
            "Epoch 3: loss = 1.735 | acc = 0.628 | val_loss = 1.723 | val_acc = 0.636 |\n",
            "Epoch 4: loss = 1.709 | acc = 0.651 | val_loss = 1.704 | val_acc = 0.65 |\n",
            "Epoch 5: loss = 1.696 | acc = 0.659 | val_loss = 1.699 | val_acc = 0.65 |\n",
            "Epoch 6: loss = 1.693 | acc = 0.663 | val_loss = 1.692 | val_acc = 0.658 |\n",
            "Epoch 7: loss = 1.685 | acc = 0.647 | val_loss = 1.693 | val_acc = 0.623 |\n",
            "Epoch 8: loss = 1.682 | acc = 0.633 | val_loss = 1.69 | val_acc = 0.613 |\n",
            "Epoch 9: loss = 1.678 | acc = 0.635 | val_loss = 1.68 | val_acc = 0.634 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.863 | acc = 0.471 | val_loss = 1.784 | val_acc = 0.522 |\n",
            "Epoch 2: loss = 1.817 | acc = 0.476 | val_loss = 1.834 | val_acc = 0.448 |\n",
            "Epoch 3: loss = 1.828 | acc = 0.416 | val_loss = 1.825 | val_acc = 0.421 |\n",
            "Epoch 4: loss = 1.84 | acc = 0.427 | val_loss = 1.851 | val_acc = 0.4 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.249 | acc = 0.122 | val_loss = 2.263 | val_acc = 0.203 |\n",
            "Epoch 2: loss = 2.279 | acc = 0.187 | val_loss = 2.302 | val_acc = 0.109 |\n",
            "Epoch 3: loss = 2.261 | acc = 0.176 | val_loss = 2.215 | val_acc = 0.199 |\n",
            "Epoch 4: loss = 2.213 | acc = 0.197 | val_loss = 2.15 | val_acc = 0.148 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.437 | acc = 0.1 | val_loss = 2.435 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.432 | acc = 0.1 | val_loss = 2.43 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 2.426 | acc = 0.1 | val_loss = 2.423 | val_acc = 0.101 |\n",
            "Epoch 4: loss = 2.42 | acc = 0.099 | val_loss = 2.418 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.342 | acc = 0.093 | val_loss = 2.306 | val_acc = 0.093 |\n",
            "Epoch 2: loss = 2.273 | acc = 0.093 | val_loss = 2.244 | val_acc = 0.094 |\n",
            "Epoch 3: loss = 2.216 | acc = 0.096 | val_loss = 2.194 | val_acc = 0.096 |\n",
            "Epoch 4: loss = 2.167 | acc = 0.1 | val_loss = 2.145 | val_acc = 0.103 |\n",
            "Epoch 5: loss = 2.124 | acc = 0.107 | val_loss = 2.108 | val_acc = 0.113 |\n",
            "Epoch 6: loss = 2.09 | acc = 0.115 | val_loss = 2.075 | val_acc = 0.122 |\n",
            "Epoch 7: loss = 2.058 | acc = 0.126 | val_loss = 2.047 | val_acc = 0.136 |\n",
            "Epoch 8: loss = 2.03 | acc = 0.14 | val_loss = 2.019 | val_acc = 0.152 |\n",
            "Epoch 9: loss = 2.004 | acc = 0.163 | val_loss = 1.996 | val_acc = 0.179 |\n",
            "Epoch 10: loss = 1.983 | acc = 0.198 | val_loss = 1.975 | val_acc = 0.227 |\n",
            "Epoch 11: loss = 1.962 | acc = 0.276 | val_loss = 1.952 | val_acc = 0.341 |\n",
            "Epoch 12: loss = 1.929 | acc = 0.437 | val_loss = 1.911 | val_acc = 0.5 |\n",
            "Epoch 13: loss = 1.891 | acc = 0.518 | val_loss = 1.882 | val_acc = 0.524 |\n",
            "Epoch 14: loss = 1.867 | acc = 0.541 | val_loss = 1.861 | val_acc = 0.543 |\n",
            "Epoch 15: loss = 1.85 | acc = 0.558 | val_loss = 1.847 | val_acc = 0.557 |\n",
            "Epoch 16: loss = 1.836 | acc = 0.574 | val_loss = 1.835 | val_acc = 0.566 |\n",
            "Epoch 17: loss = 1.826 | acc = 0.583 | val_loss = 1.825 | val_acc = 0.578 |\n",
            "Epoch 18: loss = 1.816 | acc = 0.591 | val_loss = 1.817 | val_acc = 0.586 |\n",
            "Epoch 19: loss = 1.808 | acc = 0.598 | val_loss = 1.809 | val_acc = 0.59 |\n",
            "Epoch 20: loss = 1.801 | acc = 0.604 | val_loss = 1.802 | val_acc = 0.597 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.087 | acc = 0.1 | val_loss = 1.978 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 1.941 | acc = 0.101 | val_loss = 1.921 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 1.901 | acc = 0.102 | val_loss = 1.897 | val_acc = 0.103 |\n",
            "Epoch 4: loss = 1.883 | acc = 0.104 | val_loss = 1.882 | val_acc = 0.105 |\n",
            "Epoch 5: loss = 1.862 | acc = 0.106 | val_loss = 1.837 | val_acc = 0.109 |\n",
            "Epoch 6: loss = 1.817 | acc = 0.115 | val_loss = 1.81 | val_acc = 0.136 |\n",
            "Epoch 7: loss = 1.797 | acc = 0.14 | val_loss = 1.797 | val_acc = 0.14 |\n",
            "Epoch 8: loss = 1.784 | acc = 0.152 | val_loss = 1.787 | val_acc = 0.156 |\n",
            "Epoch 9: loss = 1.776 | acc = 0.177 | val_loss = 1.777 | val_acc = 0.2 |\n",
            "Epoch 10: loss = 1.767 | acc = 0.245 | val_loss = 1.768 | val_acc = 0.342 |\n",
            "Epoch 11: loss = 1.724 | acc = 0.626 | val_loss = 1.698 | val_acc = 0.695 |\n",
            "Epoch 12: loss = 1.686 | acc = 0.707 | val_loss = 1.687 | val_acc = 0.702 |\n",
            "Epoch 13: loss = 1.678 | acc = 0.712 | val_loss = 1.681 | val_acc = 0.706 |\n",
            "Epoch 14: loss = 1.672 | acc = 0.715 | val_loss = 1.674 | val_acc = 0.712 |\n",
            "Epoch 15: loss = 1.667 | acc = 0.72 | val_loss = 1.673 | val_acc = 0.722 |\n",
            "Epoch 16: loss = 1.662 | acc = 0.726 | val_loss = 1.666 | val_acc = 0.727 |\n",
            "Epoch 17: loss = 1.659 | acc = 0.731 | val_loss = 1.665 | val_acc = 0.721 |\n",
            "Epoch 18: loss = 1.658 | acc = 0.729 | val_loss = 1.661 | val_acc = 0.726 |\n",
            "Epoch 19: loss = 1.653 | acc = 0.735 | val_loss = 1.66 | val_acc = 0.729 |\n",
            "Epoch 20: loss = 1.652 | acc = 0.735 | val_loss = 1.656 | val_acc = 0.723 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.947 | acc = 0.267 | val_loss = 1.829 | val_acc = 0.551 |\n",
            "Epoch 2: loss = 1.785 | acc = 0.574 | val_loss = 1.771 | val_acc = 0.552 |\n",
            "Epoch 3: loss = 1.744 | acc = 0.558 | val_loss = 1.722 | val_acc = 0.544 |\n",
            "Epoch 4: loss = 1.731 | acc = 0.563 | val_loss = 1.732 | val_acc = 0.541 |\n",
            "Epoch 5: loss = 1.73 | acc = 0.558 | val_loss = 1.732 | val_acc = 0.57 |\n",
            "Epoch 6: loss = 1.736 | acc = 0.568 | val_loss = 1.718 | val_acc = 0.565 |\n",
            "Epoch 7: loss = 1.726 | acc = 0.557 | val_loss = 1.739 | val_acc = 0.521 |\n",
            "Epoch 8: loss = 1.724 | acc = 0.559 | val_loss = 1.719 | val_acc = 0.572 |\n",
            "Epoch 9: loss = 1.715 | acc = 0.567 | val_loss = 1.725 | val_acc = 0.522 |\n",
            "Epoch 10: loss = 1.73 | acc = 0.541 | val_loss = 1.731 | val_acc = 0.55 |\n",
            "Epoch 11: loss = 1.723 | acc = 0.572 | val_loss = 1.728 | val_acc = 0.538 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.226 | acc = 0.197 | val_loss = 2.279 | val_acc = 0.109 |\n",
            "Epoch 2: loss = 2.312 | acc = 0.133 | val_loss = 2.203 | val_acc = 0.3 |\n",
            "Epoch 3: loss = 2.203 | acc = 0.198 | val_loss = 2.194 | val_acc = 0.188 |\n",
            "Epoch 4: loss = 2.236 | acc = 0.166 | val_loss = 2.302 | val_acc = 0.103 |\n",
            "Epoch 5: loss = 2.272 | acc = 0.133 | val_loss = 2.265 | val_acc = 0.101 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.344 | acc = 0.089 | val_loss = 2.316 | val_acc = 0.091 |\n",
            "Epoch 2: loss = 2.294 | acc = 0.093 | val_loss = 2.27 | val_acc = 0.094 |\n",
            "Epoch 3: loss = 2.257 | acc = 0.097 | val_loss = 2.241 | val_acc = 0.098 |\n",
            "Epoch 4: loss = 2.231 | acc = 0.1 | val_loss = 2.22 | val_acc = 0.102 |\n",
            "Epoch 5: loss = 2.212 | acc = 0.103 | val_loss = 2.203 | val_acc = 0.104 |\n",
            "Epoch 6: loss = 2.195 | acc = 0.104 | val_loss = 2.188 | val_acc = 0.105 |\n",
            "Epoch 7: loss = 2.181 | acc = 0.105 | val_loss = 2.173 | val_acc = 0.106 |\n",
            "Epoch 8: loss = 2.163 | acc = 0.104 | val_loss = 2.151 | val_acc = 0.103 |\n",
            "Epoch 9: loss = 2.133 | acc = 0.102 | val_loss = 2.112 | val_acc = 0.102 |\n",
            "Epoch 10: loss = 2.097 | acc = 0.106 | val_loss = 2.086 | val_acc = 0.12 |\n",
            "Epoch 11: loss = 2.076 | acc = 0.163 | val_loss = 2.068 | val_acc = 0.248 |\n",
            "Epoch 12: loss = 2.057 | acc = 0.394 | val_loss = 2.05 | val_acc = 0.507 |\n",
            "Epoch 13: loss = 2.043 | acc = 0.538 | val_loss = 2.042 | val_acc = 0.55 |\n",
            "Epoch 14: loss = 2.038 | acc = 0.569 | val_loss = 2.039 | val_acc = 0.573 |\n",
            "Epoch 15: loss = 2.036 | acc = 0.59 | val_loss = 2.039 | val_acc = 0.591 |\n",
            "Epoch 16: loss = 2.037 | acc = 0.606 | val_loss = 2.04 | val_acc = 0.606 |\n",
            "Epoch 17: loss = 2.039 | acc = 0.623 | val_loss = 2.042 | val_acc = 0.613 |\n",
            "Epoch 18: loss = 2.042 | acc = 0.631 | val_loss = 2.045 | val_acc = 0.631 |\n",
            "Epoch 19: loss = 2.045 | acc = 0.64 | val_loss = 2.049 | val_acc = 0.629 |\n",
            "Epoch 20: loss = 2.05 | acc = 0.642 | val_loss = 2.054 | val_acc = 0.633 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.149 | acc = 0.067 | val_loss = 2.024 | val_acc = 0.074 |\n",
            "Epoch 2: loss = 1.973 | acc = 0.114 | val_loss = 1.929 | val_acc = 0.17 |\n",
            "Epoch 3: loss = 1.865 | acc = 0.435 | val_loss = 1.803 | val_acc = 0.66 |\n",
            "Epoch 4: loss = 1.777 | acc = 0.686 | val_loss = 1.767 | val_acc = 0.68 |\n",
            "Epoch 5: loss = 1.753 | acc = 0.703 | val_loss = 1.75 | val_acc = 0.697 |\n",
            "Epoch 6: loss = 1.739 | acc = 0.722 | val_loss = 1.737 | val_acc = 0.717 |\n",
            "Epoch 7: loss = 1.73 | acc = 0.727 | val_loss = 1.731 | val_acc = 0.72 |\n",
            "Epoch 8: loss = 1.723 | acc = 0.727 | val_loss = 1.724 | val_acc = 0.717 |\n",
            "Epoch 9: loss = 1.719 | acc = 0.731 | val_loss = 1.723 | val_acc = 0.72 |\n",
            "Epoch 10: loss = 1.715 | acc = 0.735 | val_loss = 1.718 | val_acc = 0.728 |\n",
            "Epoch 11: loss = 1.71 | acc = 0.742 | val_loss = 1.714 | val_acc = 0.724 |\n",
            "Epoch 12: loss = 1.708 | acc = 0.742 | val_loss = 1.713 | val_acc = 0.735 |\n",
            "Epoch 13: loss = 1.705 | acc = 0.745 | val_loss = 1.71 | val_acc = 0.729 |\n",
            "Epoch 14: loss = 1.703 | acc = 0.744 | val_loss = 1.708 | val_acc = 0.736 |\n",
            "Epoch 15: loss = 1.702 | acc = 0.748 | val_loss = 1.705 | val_acc = 0.737 |\n",
            "Epoch 16: loss = 1.7 | acc = 0.747 | val_loss = 1.704 | val_acc = 0.736 |\n",
            "Epoch 17: loss = 1.698 | acc = 0.745 | val_loss = 1.702 | val_acc = 0.733 |\n",
            "Epoch 18: loss = 1.697 | acc = 0.747 | val_loss = 1.701 | val_acc = 0.738 |\n",
            "Epoch 19: loss = 1.695 | acc = 0.748 | val_loss = 1.698 | val_acc = 0.745 |\n",
            "Epoch 20: loss = 1.693 | acc = 0.75 | val_loss = 1.697 | val_acc = 0.735 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.881 | acc = 0.522 | val_loss = 1.806 | val_acc = 0.647 |\n",
            "Epoch 2: loss = 1.783 | acc = 0.658 | val_loss = 1.756 | val_acc = 0.668 |\n",
            "Epoch 3: loss = 1.769 | acc = 0.663 | val_loss = 1.786 | val_acc = 0.658 |\n",
            "Epoch 4: loss = 1.781 | acc = 0.655 | val_loss = 1.777 | val_acc = 0.649 |\n",
            "Epoch 5: loss = 1.737 | acc = 0.648 | val_loss = 1.703 | val_acc = 0.669 |\n",
            "Epoch 6: loss = 1.718 | acc = 0.67 | val_loss = 1.723 | val_acc = 0.665 |\n",
            "Epoch 7: loss = 1.733 | acc = 0.644 | val_loss = 1.721 | val_acc = 0.618 |\n",
            "Epoch 8: loss = 1.731 | acc = 0.643 | val_loss = 1.742 | val_acc = 0.615 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.995 | acc = 0.288 | val_loss = 1.89 | val_acc = 0.288 |\n",
            "Epoch 2: loss = 1.997 | acc = 0.215 | val_loss = 2.129 | val_acc = 0.124 |\n",
            "Epoch 3: loss = 2.005 | acc = 0.209 | val_loss = 2.039 | val_acc = 0.235 |\n",
            "Epoch 4: loss = 2.088 | acc = 0.161 | val_loss = 2.036 | val_acc = 0.194 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.35 | acc = 0.169 | val_loss = 2.303 | val_acc = 0.14 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.151 | val_loss = 2.303 | val_acc = 0.157 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.116 | val_loss = 2.303 | val_acc = 0.106 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.153 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.38 | acc = 0.063 | val_loss = 2.355 | val_acc = 0.066 |\n",
            "Epoch 2: loss = 2.334 | acc = 0.065 | val_loss = 2.316 | val_acc = 0.067 |\n",
            "Epoch 3: loss = 2.299 | acc = 0.065 | val_loss = 2.286 | val_acc = 0.069 |\n",
            "Epoch 4: loss = 2.268 | acc = 0.066 | val_loss = 2.255 | val_acc = 0.067 |\n",
            "Epoch 5: loss = 2.238 | acc = 0.067 | val_loss = 2.226 | val_acc = 0.069 |\n",
            "Epoch 6: loss = 2.212 | acc = 0.066 | val_loss = 2.202 | val_acc = 0.071 |\n",
            "Epoch 7: loss = 2.191 | acc = 0.071 | val_loss = 2.182 | val_acc = 0.076 |\n",
            "Epoch 8: loss = 2.173 | acc = 0.077 | val_loss = 2.167 | val_acc = 0.084 |\n",
            "Epoch 9: loss = 2.159 | acc = 0.084 | val_loss = 2.153 | val_acc = 0.092 |\n",
            "Epoch 10: loss = 2.145 | acc = 0.096 | val_loss = 2.138 | val_acc = 0.104 |\n",
            "Epoch 11: loss = 2.132 | acc = 0.112 | val_loss = 2.125 | val_acc = 0.118 |\n",
            "Epoch 12: loss = 2.119 | acc = 0.128 | val_loss = 2.112 | val_acc = 0.135 |\n",
            "Epoch 13: loss = 2.106 | acc = 0.148 | val_loss = 2.1 | val_acc = 0.164 |\n",
            "Epoch 14: loss = 2.093 | acc = 0.179 | val_loss = 2.087 | val_acc = 0.205 |\n",
            "Epoch 15: loss = 2.08 | acc = 0.231 | val_loss = 2.073 | val_acc = 0.269 |\n",
            "Epoch 16: loss = 2.067 | acc = 0.308 | val_loss = 2.061 | val_acc = 0.36 |\n",
            "Epoch 17: loss = 2.054 | acc = 0.415 | val_loss = 2.047 | val_acc = 0.475 |\n",
            "Epoch 18: loss = 2.041 | acc = 0.506 | val_loss = 2.035 | val_acc = 0.536 |\n",
            "Epoch 19: loss = 2.03 | acc = 0.551 | val_loss = 2.026 | val_acc = 0.564 |\n",
            "Epoch 20: loss = 2.022 | acc = 0.572 | val_loss = 2.019 | val_acc = 0.583 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.176 | acc = 0.156 | val_loss = 2.047 | val_acc = 0.274 |\n",
            "Epoch 2: loss = 1.973 | acc = 0.353 | val_loss = 1.92 | val_acc = 0.426 |\n",
            "Epoch 3: loss = 1.886 | acc = 0.478 | val_loss = 1.864 | val_acc = 0.526 |\n",
            "Epoch 4: loss = 1.841 | acc = 0.548 | val_loss = 1.827 | val_acc = 0.572 |\n",
            "Epoch 5: loss = 1.811 | acc = 0.586 | val_loss = 1.805 | val_acc = 0.582 |\n",
            "Epoch 6: loss = 1.791 | acc = 0.601 | val_loss = 1.789 | val_acc = 0.609 |\n",
            "Epoch 7: loss = 1.778 | acc = 0.614 | val_loss = 1.776 | val_acc = 0.608 |\n",
            "Epoch 8: loss = 1.766 | acc = 0.626 | val_loss = 1.767 | val_acc = 0.626 |\n",
            "Epoch 9: loss = 1.758 | acc = 0.634 | val_loss = 1.759 | val_acc = 0.629 |\n",
            "Epoch 10: loss = 1.75 | acc = 0.638 | val_loss = 1.753 | val_acc = 0.631 |\n",
            "Epoch 11: loss = 1.744 | acc = 0.644 | val_loss = 1.746 | val_acc = 0.632 |\n",
            "Epoch 12: loss = 1.74 | acc = 0.647 | val_loss = 1.743 | val_acc = 0.64 |\n",
            "Epoch 13: loss = 1.735 | acc = 0.65 | val_loss = 1.738 | val_acc = 0.644 |\n",
            "Epoch 14: loss = 1.731 | acc = 0.654 | val_loss = 1.735 | val_acc = 0.645 |\n",
            "Epoch 15: loss = 1.728 | acc = 0.657 | val_loss = 1.733 | val_acc = 0.646 |\n",
            "Epoch 16: loss = 1.726 | acc = 0.655 | val_loss = 1.731 | val_acc = 0.647 |\n",
            "Epoch 17: loss = 1.724 | acc = 0.658 | val_loss = 1.728 | val_acc = 0.656 |\n",
            "Epoch 18: loss = 1.722 | acc = 0.661 | val_loss = 1.727 | val_acc = 0.653 |\n",
            "Epoch 19: loss = 1.72 | acc = 0.665 | val_loss = 1.725 | val_acc = 0.654 |\n",
            "Epoch 20: loss = 1.719 | acc = 0.661 | val_loss = 1.724 | val_acc = 0.65 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.983 | acc = 0.107 | val_loss = 1.864 | val_acc = 0.132 |\n",
            "Epoch 2: loss = 1.832 | acc = 0.153 | val_loss = 1.82 | val_acc = 0.197 |\n",
            "Epoch 3: loss = 1.742 | acc = 0.588 | val_loss = 1.708 | val_acc = 0.687 |\n",
            "Epoch 4: loss = 1.695 | acc = 0.697 | val_loss = 1.701 | val_acc = 0.689 |\n",
            "Epoch 5: loss = 1.685 | acc = 0.703 | val_loss = 1.677 | val_acc = 0.7 |\n",
            "Epoch 6: loss = 1.676 | acc = 0.712 | val_loss = 1.685 | val_acc = 0.708 |\n",
            "Epoch 7: loss = 1.684 | acc = 0.705 | val_loss = 1.675 | val_acc = 0.703 |\n",
            "Epoch 8: loss = 1.668 | acc = 0.716 | val_loss = 1.673 | val_acc = 0.712 |\n",
            "Epoch 9: loss = 1.667 | acc = 0.72 | val_loss = 1.673 | val_acc = 0.7 |\n",
            "Epoch 10: loss = 1.664 | acc = 0.727 | val_loss = 1.671 | val_acc = 0.72 |\n",
            "Epoch 11: loss = 1.666 | acc = 0.722 | val_loss = 1.672 | val_acc = 0.714 |\n",
            "Epoch 12: loss = 1.667 | acc = 0.728 | val_loss = 1.673 | val_acc = 0.715 |\n",
            "Epoch 13: loss = 1.655 | acc = 0.733 | val_loss = 1.653 | val_acc = 0.719 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.981 | acc = 0.111 | val_loss = 1.921 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 1.976 | acc = 0.127 | val_loss = 2.076 | val_acc = 0.231 |\n",
            "Epoch 3: loss = 1.997 | acc = 0.269 | val_loss = 1.921 | val_acc = 0.273 |\n",
            "Epoch 4: loss = 2.01 | acc = 0.222 | val_loss = 1.933 | val_acc = 0.321 |\n",
            "Epoch 5: loss = 1.941 | acc = 0.266 | val_loss = 1.901 | val_acc = 0.303 |\n",
            "Epoch 6: loss = 1.933 | acc = 0.243 | val_loss = 1.907 | val_acc = 0.196 |\n",
            "Epoch 7: loss = 2.044 | acc = 0.216 | val_loss = 2.054 | val_acc = 0.181 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.341 | acc = 0.179 | val_loss = 2.337 | val_acc = 0.175 |\n",
            "Epoch 2: loss = 2.307 | acc = 0.178 | val_loss = 2.303 | val_acc = 0.178 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.191 | val_loss = 2.303 | val_acc = 0.263 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.266 | val_loss = 2.303 | val_acc = 0.261 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.265 | val_loss = 2.303 | val_acc = 0.264 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.266 | val_loss = 2.303 | val_acc = 0.265 |\n",
            "Epoch 7: loss = 2.303 | acc = 0.18 | val_loss = 2.303 | val_acc = 0.182 |\n",
            "Epoch 8: loss = 2.303 | acc = 0.104 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 9: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.324 | acc = 0.078 | val_loss = 2.317 | val_acc = 0.081 |\n",
            "Epoch 2: loss = 2.309 | acc = 0.087 | val_loss = 2.303 | val_acc = 0.095 |\n",
            "Epoch 3: loss = 2.293 | acc = 0.107 | val_loss = 2.286 | val_acc = 0.12 |\n",
            "Epoch 4: loss = 2.276 | acc = 0.137 | val_loss = 2.269 | val_acc = 0.154 |\n",
            "Epoch 5: loss = 2.258 | acc = 0.177 | val_loss = 2.251 | val_acc = 0.197 |\n",
            "Epoch 6: loss = 2.24 | acc = 0.221 | val_loss = 2.232 | val_acc = 0.238 |\n",
            "Epoch 7: loss = 2.222 | acc = 0.26 | val_loss = 2.214 | val_acc = 0.272 |\n",
            "Epoch 8: loss = 2.204 | acc = 0.294 | val_loss = 2.198 | val_acc = 0.301 |\n",
            "Epoch 9: loss = 2.188 | acc = 0.32 | val_loss = 2.184 | val_acc = 0.324 |\n",
            "Epoch 10: loss = 2.175 | acc = 0.341 | val_loss = 2.172 | val_acc = 0.338 |\n",
            "Epoch 11: loss = 2.163 | acc = 0.355 | val_loss = 2.161 | val_acc = 0.355 |\n",
            "Epoch 12: loss = 2.153 | acc = 0.368 | val_loss = 2.152 | val_acc = 0.37 |\n",
            "Epoch 13: loss = 2.145 | acc = 0.381 | val_loss = 2.144 | val_acc = 0.383 |\n",
            "Epoch 14: loss = 2.137 | acc = 0.392 | val_loss = 2.136 | val_acc = 0.392 |\n",
            "Epoch 15: loss = 2.13 | acc = 0.402 | val_loss = 2.13 | val_acc = 0.4 |\n",
            "Epoch 16: loss = 2.124 | acc = 0.411 | val_loss = 2.124 | val_acc = 0.409 |\n",
            "Epoch 17: loss = 2.119 | acc = 0.419 | val_loss = 2.119 | val_acc = 0.417 |\n",
            "Epoch 18: loss = 2.114 | acc = 0.425 | val_loss = 2.114 | val_acc = 0.423 |\n",
            "Epoch 19: loss = 2.11 | acc = 0.431 | val_loss = 2.11 | val_acc = 0.428 |\n",
            "Epoch 20: loss = 2.106 | acc = 0.437 | val_loss = 2.106 | val_acc = 0.432 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.326 | acc = 0.095 | val_loss = 2.234 | val_acc = 0.098 |\n",
            "Epoch 2: loss = 2.164 | acc = 0.1 | val_loss = 2.111 | val_acc = 0.106 |\n",
            "Epoch 3: loss = 2.074 | acc = 0.112 | val_loss = 2.047 | val_acc = 0.125 |\n",
            "Epoch 4: loss = 2.023 | acc = 0.147 | val_loss = 2.007 | val_acc = 0.172 |\n",
            "Epoch 5: loss = 1.988 | acc = 0.209 | val_loss = 1.976 | val_acc = 0.24 |\n",
            "Epoch 6: loss = 1.958 | acc = 0.283 | val_loss = 1.947 | val_acc = 0.316 |\n",
            "Epoch 7: loss = 1.934 | acc = 0.35 | val_loss = 1.929 | val_acc = 0.381 |\n",
            "Epoch 8: loss = 1.916 | acc = 0.423 | val_loss = 1.911 | val_acc = 0.452 |\n",
            "Epoch 9: loss = 1.898 | acc = 0.486 | val_loss = 1.893 | val_acc = 0.508 |\n",
            "Epoch 10: loss = 1.879 | acc = 0.53 | val_loss = 1.874 | val_acc = 0.532 |\n",
            "Epoch 11: loss = 1.862 | acc = 0.55 | val_loss = 1.86 | val_acc = 0.544 |\n",
            "Epoch 12: loss = 1.849 | acc = 0.562 | val_loss = 1.848 | val_acc = 0.551 |\n",
            "Epoch 13: loss = 1.838 | acc = 0.569 | val_loss = 1.839 | val_acc = 0.562 |\n",
            "Epoch 14: loss = 1.83 | acc = 0.573 | val_loss = 1.831 | val_acc = 0.568 |\n",
            "Epoch 15: loss = 1.822 | acc = 0.581 | val_loss = 1.825 | val_acc = 0.57 |\n",
            "Epoch 16: loss = 1.817 | acc = 0.588 | val_loss = 1.82 | val_acc = 0.579 |\n",
            "Epoch 17: loss = 1.812 | acc = 0.595 | val_loss = 1.815 | val_acc = 0.586 |\n",
            "Epoch 18: loss = 1.807 | acc = 0.6 | val_loss = 1.811 | val_acc = 0.59 |\n",
            "Epoch 19: loss = 1.804 | acc = 0.6 | val_loss = 1.808 | val_acc = 0.595 |\n",
            "Epoch 20: loss = 1.8 | acc = 0.604 | val_loss = 1.805 | val_acc = 0.592 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.073 | acc = 0.116 | val_loss = 1.955 | val_acc = 0.149 |\n",
            "Epoch 2: loss = 1.891 | acc = 0.379 | val_loss = 1.815 | val_acc = 0.645 |\n",
            "Epoch 3: loss = 1.789 | acc = 0.673 | val_loss = 1.78 | val_acc = 0.673 |\n",
            "Epoch 4: loss = 1.765 | acc = 0.691 | val_loss = 1.766 | val_acc = 0.687 |\n",
            "Epoch 5: loss = 1.751 | acc = 0.697 | val_loss = 1.754 | val_acc = 0.694 |\n",
            "Epoch 6: loss = 1.743 | acc = 0.705 | val_loss = 1.741 | val_acc = 0.703 |\n",
            "Epoch 7: loss = 1.735 | acc = 0.709 | val_loss = 1.742 | val_acc = 0.7 |\n",
            "Epoch 8: loss = 1.732 | acc = 0.706 | val_loss = 1.735 | val_acc = 0.705 |\n",
            "Epoch 9: loss = 1.725 | acc = 0.712 | val_loss = 1.73 | val_acc = 0.708 |\n",
            "Epoch 10: loss = 1.714 | acc = 0.72 | val_loss = 1.691 | val_acc = 0.721 |\n",
            "Epoch 11: loss = 1.673 | acc = 0.727 | val_loss = 1.673 | val_acc = 0.716 |\n",
            "Epoch 12: loss = 1.663 | acc = 0.732 | val_loss = 1.663 | val_acc = 0.725 |\n",
            "Epoch 13: loss = 1.655 | acc = 0.738 | val_loss = 1.658 | val_acc = 0.735 |\n",
            "Epoch 14: loss = 1.653 | acc = 0.737 | val_loss = 1.655 | val_acc = 0.732 |\n",
            "Epoch 15: loss = 1.648 | acc = 0.74 | val_loss = 1.654 | val_acc = 0.723 |\n",
            "Epoch 16: loss = 1.648 | acc = 0.739 | val_loss = 1.65 | val_acc = 0.728 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.872 | acc = 0.507 | val_loss = 1.84 | val_acc = 0.374 |\n",
            "Epoch 2: loss = 1.828 | acc = 0.445 | val_loss = 1.818 | val_acc = 0.418 |\n",
            "Epoch 3: loss = 1.8 | acc = 0.435 | val_loss = 1.791 | val_acc = 0.413 |\n",
            "Epoch 4: loss = 1.808 | acc = 0.417 | val_loss = 1.811 | val_acc = 0.433 |\n",
            "Epoch 5: loss = 1.801 | acc = 0.413 | val_loss = 1.805 | val_acc = 0.35 |\n",
            "Epoch 6: loss = 1.801 | acc = 0.415 | val_loss = 1.795 | val_acc = 0.438 |\n",
            "Epoch 7: loss = 1.784 | acc = 0.447 | val_loss = 1.834 | val_acc = 0.38 |\n",
            "Epoch 8: loss = 1.814 | acc = 0.397 | val_loss = 1.796 | val_acc = 0.344 |\n",
            "Epoch 9: loss = 1.772 | acc = 0.393 | val_loss = 1.793 | val_acc = 0.369 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.273 | acc = 0.16 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.326 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.111 |\n",
            "Epoch 3: loss = 2.302 | acc = 0.106 | val_loss = 2.302 | val_acc = 0.104 |\n",
            "Epoch 4: loss = 2.292 | acc = 0.123 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.336 | acc = 0.104 | val_loss = 2.333 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.328 | acc = 0.104 | val_loss = 2.326 | val_acc = 0.102 |\n",
            "Epoch 3: loss = 2.321 | acc = 0.104 | val_loss = 2.319 | val_acc = 0.103 |\n",
            "Epoch 4: loss = 2.314 | acc = 0.104 | val_loss = 2.314 | val_acc = 0.103 |\n",
            "Epoch 5: loss = 2.308 | acc = 0.104 | val_loss = 2.308 | val_acc = 0.102 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.104 | val_loss = 2.303 | val_acc = 0.103 |\n",
            "Epoch 7: loss = 2.298 | acc = 0.104 | val_loss = 2.298 | val_acc = 0.103 |\n",
            "Epoch 8: loss = 2.292 | acc = 0.104 | val_loss = 2.292 | val_acc = 0.103 |\n",
            "Epoch 9: loss = 2.287 | acc = 0.105 | val_loss = 2.286 | val_acc = 0.104 |\n",
            "Epoch 10: loss = 2.281 | acc = 0.105 | val_loss = 2.281 | val_acc = 0.105 |\n",
            "Epoch 11: loss = 2.275 | acc = 0.105 | val_loss = 2.275 | val_acc = 0.105 |\n",
            "Epoch 12: loss = 2.268 | acc = 0.106 | val_loss = 2.268 | val_acc = 0.105 |\n",
            "Epoch 13: loss = 2.261 | acc = 0.106 | val_loss = 2.26 | val_acc = 0.105 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.354 | acc = 0.098 | val_loss = 2.328 | val_acc = 0.094 |\n",
            "Epoch 2: loss = 2.3 | acc = 0.094 | val_loss = 2.276 | val_acc = 0.096 |\n",
            "Epoch 3: loss = 2.252 | acc = 0.096 | val_loss = 2.232 | val_acc = 0.097 |\n",
            "Epoch 4: loss = 2.208 | acc = 0.098 | val_loss = 2.19 | val_acc = 0.098 |\n",
            "Epoch 5: loss = 2.167 | acc = 0.099 | val_loss = 2.153 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.135 | acc = 0.1 | val_loss = 2.123 | val_acc = 0.101 |\n",
            "Epoch 7: loss = 2.103 | acc = 0.101 | val_loss = 2.087 | val_acc = 0.101 |\n",
            "Epoch 8: loss = 2.067 | acc = 0.101 | val_loss = 2.054 | val_acc = 0.101 |\n",
            "Epoch 9: loss = 2.04 | acc = 0.101 | val_loss = 2.031 | val_acc = 0.101 |\n",
            "Epoch 10: loss = 2.02 | acc = 0.102 | val_loss = 2.013 | val_acc = 0.102 |\n",
            "Epoch 11: loss = 2.004 | acc = 0.103 | val_loss = 2.0 | val_acc = 0.102 |\n",
            "Epoch 12: loss = 1.992 | acc = 0.104 | val_loss = 1.989 | val_acc = 0.104 |\n",
            "Epoch 13: loss = 1.982 | acc = 0.105 | val_loss = 1.98 | val_acc = 0.105 |\n",
            "Epoch 14: loss = 1.973 | acc = 0.106 | val_loss = 1.972 | val_acc = 0.106 |\n",
            "Epoch 15: loss = 1.966 | acc = 0.107 | val_loss = 1.966 | val_acc = 0.108 |\n",
            "Epoch 16: loss = 1.96 | acc = 0.109 | val_loss = 1.959 | val_acc = 0.11 |\n",
            "Epoch 17: loss = 1.954 | acc = 0.112 | val_loss = 1.954 | val_acc = 0.113 |\n",
            "Epoch 18: loss = 1.949 | acc = 0.116 | val_loss = 1.95 | val_acc = 0.116 |\n",
            "Epoch 19: loss = 1.944 | acc = 0.121 | val_loss = 1.946 | val_acc = 0.123 |\n",
            "Epoch 20: loss = 1.94 | acc = 0.131 | val_loss = 1.941 | val_acc = 0.136 |\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.055 | acc = 0.321 | val_loss = 1.912 | val_acc = 0.555 |\n",
            "Epoch 2: loss = 1.865 | acc = 0.603 | val_loss = 1.835 | val_acc = 0.626 |\n",
            "Epoch 3: loss = 1.787 | acc = 0.654 | val_loss = 1.759 | val_acc = 0.661 |\n",
            "Epoch 4: loss = 1.738 | acc = 0.683 | val_loss = 1.73 | val_acc = 0.683 |\n",
            "Epoch 5: loss = 1.717 | acc = 0.701 | val_loss = 1.716 | val_acc = 0.691 |\n",
            "Epoch 6: loss = 1.705 | acc = 0.709 | val_loss = 1.705 | val_acc = 0.699 |\n",
            "Epoch 7: loss = 1.694 | acc = 0.719 | val_loss = 1.696 | val_acc = 0.704 |\n",
            "Epoch 8: loss = 1.686 | acc = 0.728 | val_loss = 1.689 | val_acc = 0.719 |\n",
            "Epoch 9: loss = 1.681 | acc = 0.728 | val_loss = 1.684 | val_acc = 0.713 |\n",
            "Epoch 10: loss = 1.674 | acc = 0.726 | val_loss = 1.68 | val_acc = 0.722 |\n",
            "Epoch 11: loss = 1.669 | acc = 0.736 | val_loss = 1.676 | val_acc = 0.728 |\n",
            "Epoch 12: loss = 1.666 | acc = 0.738 | val_loss = 1.673 | val_acc = 0.729 |\n",
            "Epoch 13: loss = 1.663 | acc = 0.743 | val_loss = 1.668 | val_acc = 0.732 |\n",
            "Epoch 14: loss = 1.659 | acc = 0.74 | val_loss = 1.665 | val_acc = 0.722 |\n",
            "Epoch 15: loss = 1.658 | acc = 0.736 | val_loss = 1.666 | val_acc = 0.722 |\n",
            "Epoch 16: loss = 1.657 | acc = 0.737 | val_loss = 1.662 | val_acc = 0.727 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.869 | acc = 0.536 | val_loss = 1.763 | val_acc = 0.621 |\n",
            "Epoch 2: loss = 1.744 | acc = 0.651 | val_loss = 1.739 | val_acc = 0.64 |\n",
            "Epoch 3: loss = 1.746 | acc = 0.631 | val_loss = 1.752 | val_acc = 0.613 |\n",
            "Epoch 4: loss = 1.747 | acc = 0.63 | val_loss = 1.739 | val_acc = 0.628 |\n",
            "Epoch 5: loss = 1.72 | acc = 0.639 | val_loss = 1.721 | val_acc = 0.634 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1e-05~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.212 | acc = 0.141 | val_loss = 2.28 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.229 | acc = 0.153 | val_loss = 2.228 | val_acc = 0.176 |\n",
            "Epoch 3: loss = 2.236 | acc = 0.163 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.265 | acc = 0.114 | val_loss = 2.189 | val_acc = 0.191 |\n",
            "Epoch 6: loss = 2.218 | acc = 0.157 | val_loss = 2.349 | val_acc = 0.101 |\n",
            "Epoch 7: loss = 2.236 | acc = 0.139 | val_loss = 2.213 | val_acc = 0.156 |\n",
            "Epoch 8: loss = 2.227 | acc = 0.156 | val_loss = 2.212 | val_acc = 0.175 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.346 | acc = 0.098 | val_loss = 2.302 | val_acc = 0.098 |\n",
            "Epoch 2: loss = 2.278 | acc = 0.148 | val_loss = 2.268 | val_acc = 0.23 |\n",
            "Epoch 3: loss = 2.275 | acc = 0.266 | val_loss = 2.284 | val_acc = 0.267 |\n",
            "Epoch 4: loss = 2.291 | acc = 0.25 | val_loss = 2.296 | val_acc = 0.229 |\n",
            "Epoch 5: loss = 2.299 | acc = 0.209 | val_loss = 2.301 | val_acc = 0.258 |\n",
            "Epoch 6: loss = 2.302 | acc = 0.136 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.101 | acc = 0.34 | val_loss = 1.982 | val_acc = 0.592 |\n",
            "Epoch 2: loss = 1.977 | acc = 0.652 | val_loss = 1.988 | val_acc = 0.622 |\n",
            "Epoch 3: loss = 1.994 | acc = 0.634 | val_loss = 2.002 | val_acc = 0.604 |\n",
            "Epoch 4: loss = 1.998 | acc = 0.539 | val_loss = 1.994 | val_acc = 0.513 |\n",
            "Epoch 5: loss = 1.986 | acc = 0.505 | val_loss = 1.979 | val_acc = 0.519 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.906 | acc = 0.39 | val_loss = 1.795 | val_acc = 0.489 |\n",
            "Epoch 2: loss = 1.772 | acc = 0.551 | val_loss = 1.776 | val_acc = 0.524 |\n",
            "Epoch 3: loss = 1.782 | acc = 0.518 | val_loss = 1.787 | val_acc = 0.528 |\n",
            "Epoch 4: loss = 1.809 | acc = 0.483 | val_loss = 1.87 | val_acc = 0.388 |\n",
            "Epoch 5: loss = 1.847 | acc = 0.448 | val_loss = 1.84 | val_acc = 0.409 |\n",
            "Epoch 6: loss = 1.844 | acc = 0.434 | val_loss = 1.819 | val_acc = 0.422 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.02 | acc = 0.227 | val_loss = 2.098 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.106 | acc = 0.165 | val_loss = 1.986 | val_acc = 0.191 |\n",
            "Epoch 3: loss = 2.112 | acc = 0.176 | val_loss = 2.052 | val_acc = 0.232 |\n",
            "Epoch 4: loss = 2.12 | acc = 0.167 | val_loss = 2.271 | val_acc = 0.124 |\n",
            "Epoch 5: loss = 2.195 | acc = 0.119 | val_loss = 2.074 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.234 | acc = 0.12 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.341 | acc = 0.126 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.102 |\n",
            "Epoch 3: loss = 2.335 | acc = 0.118 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.31 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.338 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.391 | acc = 0.1 | val_loss = 2.371 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.345 | acc = 0.1 | val_loss = 2.322 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.289 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.279 | acc = 0.101 | val_loss = 2.275 | val_acc = 0.102 |\n",
            "Epoch 5: loss = 2.275 | acc = 0.119 | val_loss = 2.278 | val_acc = 0.139 |\n",
            "Epoch 6: loss = 2.282 | acc = 0.164 | val_loss = 2.286 | val_acc = 0.18 |\n",
            "Epoch 7: loss = 2.289 | acc = 0.194 | val_loss = 2.292 | val_acc = 0.204 |\n",
            "Epoch 8: loss = 2.295 | acc = 0.239 | val_loss = 2.297 | val_acc = 0.212 |\n",
            "Epoch 9: loss = 2.298 | acc = 0.202 | val_loss = 2.3 | val_acc = 0.198 |\n",
            "Epoch 10: loss = 2.301 | acc = 0.128 | val_loss = 2.301 | val_acc = 0.1 |\n",
            "Epoch 11: loss = 2.302 | acc = 0.106 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.208 | acc = 0.235 | val_loss = 2.089 | val_acc = 0.402 |\n",
            "Epoch 2: loss = 2.021 | acc = 0.527 | val_loss = 1.99 | val_acc = 0.599 |\n",
            "Epoch 3: loss = 1.982 | acc = 0.637 | val_loss = 1.982 | val_acc = 0.66 |\n",
            "Epoch 4: loss = 1.983 | acc = 0.675 | val_loss = 1.991 | val_acc = 0.669 |\n",
            "Epoch 5: loss = 1.996 | acc = 0.677 | val_loss = 2.004 | val_acc = 0.662 |\n",
            "Epoch 6: loss = 2.005 | acc = 0.663 | val_loss = 2.008 | val_acc = 0.63 |\n",
            "Epoch 7: loss = 2.005 | acc = 0.606 | val_loss = 2.004 | val_acc = 0.536 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.883 | acc = 0.549 | val_loss = 1.771 | val_acc = 0.679 |\n",
            "Epoch 2: loss = 1.749 | acc = 0.699 | val_loss = 1.743 | val_acc = 0.689 |\n",
            "Epoch 3: loss = 1.726 | acc = 0.709 | val_loss = 1.722 | val_acc = 0.703 |\n",
            "Epoch 4: loss = 1.719 | acc = 0.719 | val_loss = 1.727 | val_acc = 0.712 |\n",
            "Epoch 5: loss = 1.72 | acc = 0.717 | val_loss = 1.725 | val_acc = 0.699 |\n",
            "Epoch 6: loss = 1.714 | acc = 0.716 | val_loss = 1.728 | val_acc = 0.693 |\n",
            "Epoch 7: loss = 1.719 | acc = 0.716 | val_loss = 1.724 | val_acc = 0.703 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.934 | acc = 0.359 | val_loss = 1.932 | val_acc = 0.368 |\n",
            "Epoch 2: loss = 1.98 | acc = 0.284 | val_loss = 2.026 | val_acc = 0.293 |\n",
            "Epoch 3: loss = 1.994 | acc = 0.257 | val_loss = 2.248 | val_acc = 0.116 |\n",
            "Epoch 4: loss = 2.077 | acc = 0.189 | val_loss = 2.012 | val_acc = 0.224 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.289 | acc = 0.14 | val_loss = 2.303 | val_acc = 0.187 |\n",
            "Epoch 2: loss = 2.306 | acc = 0.118 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.163 | val_loss = 2.303 | val_acc = 0.154 |\n",
            "Epoch 4: loss = 2.311 | acc = 0.161 | val_loss = 2.303 | val_acc = 0.121 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.358 | acc = 0.102 | val_loss = 2.342 | val_acc = 0.102 |\n",
            "Epoch 2: loss = 2.321 | acc = 0.107 | val_loss = 2.308 | val_acc = 0.11 |\n",
            "Epoch 3: loss = 2.292 | acc = 0.113 | val_loss = 2.282 | val_acc = 0.119 |\n",
            "Epoch 4: loss = 2.27 | acc = 0.124 | val_loss = 2.264 | val_acc = 0.135 |\n",
            "Epoch 5: loss = 2.257 | acc = 0.143 | val_loss = 2.253 | val_acc = 0.155 |\n",
            "Epoch 6: loss = 2.25 | acc = 0.173 | val_loss = 2.25 | val_acc = 0.188 |\n",
            "Epoch 7: loss = 2.249 | acc = 0.209 | val_loss = 2.251 | val_acc = 0.224 |\n",
            "Epoch 8: loss = 2.253 | acc = 0.245 | val_loss = 2.256 | val_acc = 0.264 |\n",
            "Epoch 9: loss = 2.258 | acc = 0.282 | val_loss = 2.262 | val_acc = 0.296 |\n",
            "Epoch 10: loss = 2.265 | acc = 0.317 | val_loss = 2.269 | val_acc = 0.322 |\n",
            "Epoch 11: loss = 2.271 | acc = 0.342 | val_loss = 2.275 | val_acc = 0.351 |\n",
            "Epoch 12: loss = 2.278 | acc = 0.362 | val_loss = 2.281 | val_acc = 0.368 |\n",
            "Epoch 13: loss = 2.283 | acc = 0.378 | val_loss = 2.285 | val_acc = 0.378 |\n",
            "Epoch 14: loss = 2.287 | acc = 0.379 | val_loss = 2.289 | val_acc = 0.386 |\n",
            "Epoch 15: loss = 2.291 | acc = 0.395 | val_loss = 2.293 | val_acc = 0.401 |\n",
            "Epoch 16: loss = 2.294 | acc = 0.407 | val_loss = 2.295 | val_acc = 0.436 |\n",
            "Epoch 17: loss = 2.296 | acc = 0.438 | val_loss = 2.297 | val_acc = 0.466 |\n",
            "Epoch 18: loss = 2.298 | acc = 0.422 | val_loss = 2.299 | val_acc = 0.45 |\n",
            "Epoch 19: loss = 2.3 | acc = 0.419 | val_loss = 2.3 | val_acc = 0.316 |\n",
            "Epoch 20: loss = 2.301 | acc = 0.26 | val_loss = 2.301 | val_acc = 0.284 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.288 | acc = 0.108 | val_loss = 2.232 | val_acc = 0.107 |\n",
            "Epoch 2: loss = 2.181 | acc = 0.109 | val_loss = 2.123 | val_acc = 0.104 |\n",
            "Epoch 3: loss = 2.074 | acc = 0.112 | val_loss = 2.041 | val_acc = 0.141 |\n",
            "Epoch 4: loss = 2.012 | acc = 0.398 | val_loss = 1.992 | val_acc = 0.57 |\n",
            "Epoch 5: loss = 1.979 | acc = 0.598 | val_loss = 1.975 | val_acc = 0.612 |\n",
            "Epoch 6: loss = 1.97 | acc = 0.63 | val_loss = 1.971 | val_acc = 0.633 |\n",
            "Epoch 7: loss = 1.97 | acc = 0.644 | val_loss = 1.973 | val_acc = 0.639 |\n",
            "Epoch 8: loss = 1.973 | acc = 0.649 | val_loss = 1.978 | val_acc = 0.644 |\n",
            "Epoch 9: loss = 1.979 | acc = 0.653 | val_loss = 1.984 | val_acc = 0.628 |\n",
            "Epoch 10: loss = 1.984 | acc = 0.649 | val_loss = 1.989 | val_acc = 0.627 |\n",
            "Epoch 11: loss = 1.988 | acc = 0.636 | val_loss = 1.992 | val_acc = 0.598 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.06 | acc = 0.096 | val_loss = 1.948 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 1.875 | acc = 0.112 | val_loss = 1.821 | val_acc = 0.141 |\n",
            "Epoch 3: loss = 1.782 | acc = 0.529 | val_loss = 1.745 | val_acc = 0.721 |\n",
            "Epoch 4: loss = 1.73 | acc = 0.731 | val_loss = 1.722 | val_acc = 0.731 |\n",
            "Epoch 5: loss = 1.718 | acc = 0.74 | val_loss = 1.717 | val_acc = 0.726 |\n",
            "Epoch 6: loss = 1.71 | acc = 0.739 | val_loss = 1.711 | val_acc = 0.737 |\n",
            "Epoch 7: loss = 1.702 | acc = 0.74 | val_loss = 1.705 | val_acc = 0.728 |\n",
            "Epoch 8: loss = 1.698 | acc = 0.738 | val_loss = 1.699 | val_acc = 0.719 |\n",
            "Epoch 9: loss = 1.693 | acc = 0.733 | val_loss = 1.699 | val_acc = 0.724 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.968 | acc = 0.36 | val_loss = 1.92 | val_acc = 0.447 |\n",
            "Epoch 2: loss = 1.939 | acc = 0.426 | val_loss = 1.962 | val_acc = 0.402 |\n",
            "Epoch 3: loss = 1.939 | acc = 0.393 | val_loss = 1.835 | val_acc = 0.428 |\n",
            "Epoch 4: loss = 1.864 | acc = 0.405 | val_loss = 1.962 | val_acc = 0.286 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.294 | acc = 0.167 | val_loss = 2.303 | val_acc = 0.198 |\n",
            "Epoch 2: loss = 2.253 | acc = 0.185 | val_loss = 2.278 | val_acc = 0.223 |\n",
            "Epoch 3: loss = 2.293 | acc = 0.135 | val_loss = 2.303 | val_acc = 0.162 |\n",
            "Epoch 4: loss = 2.298 | acc = 0.14 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.292 | acc = 0.121 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.326 | acc = 0.1 | val_loss = 2.317 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 2.313 | acc = 0.1 | val_loss = 2.304 | val_acc = 0.101 |\n",
            "Epoch 3: loss = 2.301 | acc = 0.1 | val_loss = 2.293 | val_acc = 0.101 |\n",
            "Epoch 4: loss = 2.29 | acc = 0.1 | val_loss = 2.283 | val_acc = 0.102 |\n",
            "Epoch 5: loss = 2.28 | acc = 0.1 | val_loss = 2.274 | val_acc = 0.101 |\n",
            "Epoch 6: loss = 2.272 | acc = 0.1 | val_loss = 2.267 | val_acc = 0.101 |\n",
            "Epoch 7: loss = 2.265 | acc = 0.1 | val_loss = 2.261 | val_acc = 0.101 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.378 | acc = 0.082 | val_loss = 2.341 | val_acc = 0.083 |\n",
            "Epoch 2: loss = 2.301 | acc = 0.094 | val_loss = 2.265 | val_acc = 0.104 |\n",
            "Epoch 3: loss = 2.223 | acc = 0.169 | val_loss = 2.187 | val_acc = 0.242 |\n",
            "Epoch 4: loss = 2.151 | acc = 0.282 | val_loss = 2.123 | val_acc = 0.314 |\n",
            "Epoch 5: loss = 2.083 | acc = 0.377 | val_loss = 2.06 | val_acc = 0.426 |\n",
            "Epoch 6: loss = 2.038 | acc = 0.469 | val_loss = 2.028 | val_acc = 0.486 |\n",
            "Epoch 7: loss = 2.013 | acc = 0.52 | val_loss = 2.008 | val_acc = 0.531 |\n",
            "Epoch 8: loss = 2.0 | acc = 0.553 | val_loss = 1.998 | val_acc = 0.557 |\n",
            "Epoch 9: loss = 1.991 | acc = 0.582 | val_loss = 1.991 | val_acc = 0.582 |\n",
            "Epoch 10: loss = 1.987 | acc = 0.602 | val_loss = 1.989 | val_acc = 0.602 |\n",
            "Epoch 11: loss = 1.985 | acc = 0.617 | val_loss = 1.988 | val_acc = 0.611 |\n",
            "Epoch 12: loss = 1.986 | acc = 0.627 | val_loss = 1.989 | val_acc = 0.623 |\n",
            "Epoch 13: loss = 1.987 | acc = 0.635 | val_loss = 1.991 | val_acc = 0.632 |\n",
            "Epoch 14: loss = 1.989 | acc = 0.642 | val_loss = 1.993 | val_acc = 0.636 |\n",
            "Epoch 15: loss = 1.992 | acc = 0.646 | val_loss = 1.996 | val_acc = 0.632 |\n",
            "Epoch 16: loss = 1.995 | acc = 0.647 | val_loss = 1.998 | val_acc = 0.634 |\n",
            "Epoch 17: loss = 1.997 | acc = 0.644 | val_loss = 2.001 | val_acc = 0.633 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.119 | acc = 0.35 | val_loss = 1.994 | val_acc = 0.474 |\n",
            "Epoch 2: loss = 1.941 | acc = 0.526 | val_loss = 1.911 | val_acc = 0.554 |\n",
            "Epoch 3: loss = 1.859 | acc = 0.61 | val_loss = 1.826 | val_acc = 0.643 |\n",
            "Epoch 4: loss = 1.809 | acc = 0.659 | val_loss = 1.805 | val_acc = 0.658 |\n",
            "Epoch 5: loss = 1.792 | acc = 0.675 | val_loss = 1.793 | val_acc = 0.667 |\n",
            "Epoch 6: loss = 1.782 | acc = 0.688 | val_loss = 1.784 | val_acc = 0.682 |\n",
            "Epoch 7: loss = 1.762 | acc = 0.695 | val_loss = 1.741 | val_acc = 0.685 |\n",
            "Epoch 8: loss = 1.728 | acc = 0.705 | val_loss = 1.726 | val_acc = 0.702 |\n",
            "Epoch 9: loss = 1.718 | acc = 0.716 | val_loss = 1.72 | val_acc = 0.709 |\n",
            "Epoch 10: loss = 1.712 | acc = 0.719 | val_loss = 1.714 | val_acc = 0.716 |\n",
            "Epoch 11: loss = 1.707 | acc = 0.724 | val_loss = 1.709 | val_acc = 0.711 |\n",
            "Epoch 12: loss = 1.704 | acc = 0.727 | val_loss = 1.708 | val_acc = 0.719 |\n",
            "Epoch 13: loss = 1.701 | acc = 0.733 | val_loss = 1.704 | val_acc = 0.73 |\n",
            "Epoch 14: loss = 1.699 | acc = 0.733 | val_loss = 1.704 | val_acc = 0.728 |\n",
            "Epoch 15: loss = 1.695 | acc = 0.735 | val_loss = 1.699 | val_acc = 0.724 |\n",
            "Epoch 16: loss = 1.694 | acc = 0.736 | val_loss = 1.699 | val_acc = 0.725 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.848 | acc = 0.518 | val_loss = 1.792 | val_acc = 0.595 |\n",
            "Epoch 2: loss = 1.773 | acc = 0.571 | val_loss = 1.77 | val_acc = 0.602 |\n",
            "Epoch 3: loss = 1.763 | acc = 0.584 | val_loss = 1.754 | val_acc = 0.587 |\n",
            "Epoch 4: loss = 1.772 | acc = 0.535 | val_loss = 1.842 | val_acc = 0.525 |\n",
            "Epoch 5: loss = 1.798 | acc = 0.496 | val_loss = 1.761 | val_acc = 0.502 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.0001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.247 | acc = 0.184 | val_loss = 2.26 | val_acc = 0.198 |\n",
            "Epoch 2: loss = 2.314 | acc = 0.126 | val_loss = 2.318 | val_acc = 0.105 |\n",
            "Epoch 3: loss = 2.312 | acc = 0.126 | val_loss = 2.367 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.364 | acc = 0.104 | val_loss = 2.337 | val_acc = 0.101 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.291 | acc = 0.178 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.029 | acc = 0.403 | val_loss = 2.0 | val_acc = 0.351 |\n",
            "Epoch 2: loss = 1.991 | acc = 0.396 | val_loss = 2.001 | val_acc = 0.431 |\n",
            "Epoch 3: loss = 1.99 | acc = 0.398 | val_loss = 1.984 | val_acc = 0.388 |\n",
            "Epoch 4: loss = 1.991 | acc = 0.394 | val_loss = 1.988 | val_acc = 0.411 |\n",
            "Epoch 5: loss = 1.987 | acc = 0.395 | val_loss = 1.979 | val_acc = 0.419 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.18 | acc = 0.168 | val_loss = 2.269 | val_acc = 0.101 |\n",
            "Epoch 2: loss = 2.245 | acc = 0.134 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.258 | acc = 0.133 | val_loss = 2.296 | val_acc = 0.11 |\n",
            "Epoch 4: loss = 2.263 | acc = 0.132 | val_loss = 2.197 | val_acc = 0.167 |\n",
            "Epoch 5: loss = 2.246 | acc = 0.135 | val_loss = 2.348 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.251 | acc = 0.135 | val_loss = 2.141 | val_acc = 0.166 |\n",
            "Epoch 7: loss = 2.238 | acc = 0.14 | val_loss = 2.302 | val_acc = 0.102 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.351 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.339 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.369 | acc = 0.1 | val_loss = 2.41 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.326 | acc = 0.102 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.323 | acc = 0.111 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.288 | acc = 0.223 | val_loss = 2.3 | val_acc = 0.19 |\n",
            "Epoch 2: loss = 2.302 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.049 | acc = 0.417 | val_loss = 1.98 | val_acc = 0.337 |\n",
            "Epoch 2: loss = 1.968 | acc = 0.351 | val_loss = 1.963 | val_acc = 0.322 |\n",
            "Epoch 3: loss = 1.96 | acc = 0.312 | val_loss = 1.957 | val_acc = 0.365 |\n",
            "Epoch 4: loss = 1.96 | acc = 0.306 | val_loss = 1.957 | val_acc = 0.322 |\n",
            "Epoch 5: loss = 1.96 | acc = 0.303 | val_loss = 1.968 | val_acc = 0.277 |\n",
            "Epoch 6: loss = 1.959 | acc = 0.303 | val_loss = 1.964 | val_acc = 0.286 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.037 | acc = 0.281 | val_loss = 2.239 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 2.184 | acc = 0.154 | val_loss = 2.105 | val_acc = 0.2 |\n",
            "Epoch 3: loss = 2.183 | acc = 0.15 | val_loss = 2.295 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.172 | acc = 0.158 | val_loss = 2.244 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.19 | acc = 0.153 | val_loss = 2.334 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.315 | acc = 0.12 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.311 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.31 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.327 | acc = 0.099 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.351 | acc = 0.131 | val_loss = 2.31 | val_acc = 0.114 |\n",
            "Epoch 2: loss = 2.304 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.298 | acc = 0.137 | val_loss = 2.274 | val_acc = 0.301 |\n",
            "Epoch 2: loss = 2.29 | acc = 0.326 | val_loss = 2.3 | val_acc = 0.16 |\n",
            "Epoch 3: loss = 2.302 | acc = 0.115 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.085 | acc = 0.445 | val_loss = 2.008 | val_acc = 0.582 |\n",
            "Epoch 2: loss = 1.995 | acc = 0.47 | val_loss = 1.976 | val_acc = 0.377 |\n",
            "Epoch 3: loss = 1.964 | acc = 0.397 | val_loss = 1.96 | val_acc = 0.37 |\n",
            "Epoch 4: loss = 1.954 | acc = 0.347 | val_loss = 1.954 | val_acc = 0.306 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.923 | acc = 0.391 | val_loss = 1.902 | val_acc = 0.348 |\n",
            "Epoch 2: loss = 1.98 | acc = 0.291 | val_loss = 2.037 | val_acc = 0.192 |\n",
            "Epoch 3: loss = 2.063 | acc = 0.179 | val_loss = 2.036 | val_acc = 0.178 |\n",
            "Epoch 4: loss = 2.095 | acc = 0.168 | val_loss = 2.319 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.322 | acc = 0.138 | val_loss = 2.329 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.31 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.304 | acc = 0.1 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.354 | acc = 0.065 | val_loss = 2.324 | val_acc = 0.065 |\n",
            "Epoch 2: loss = 2.31 | acc = 0.074 | val_loss = 2.303 | val_acc = 0.094 |\n",
            "Epoch 3: loss = 2.302 | acc = 0.099 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.302 | acc = 0.1 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.302 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.348 | acc = 0.07 | val_loss = 2.296 | val_acc = 0.096 |\n",
            "Epoch 2: loss = 2.277 | acc = 0.189 | val_loss = 2.276 | val_acc = 0.32 |\n",
            "Epoch 3: loss = 2.285 | acc = 0.363 | val_loss = 2.293 | val_acc = 0.318 |\n",
            "Epoch 4: loss = 2.297 | acc = 0.301 | val_loss = 2.3 | val_acc = 0.323 |\n",
            "Epoch 5: loss = 2.301 | acc = 0.168 | val_loss = 2.302 | val_acc = 0.195 |\n",
            "Epoch 6: loss = 2.302 | acc = 0.118 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 7: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.214 | acc = 0.089 | val_loss = 2.078 | val_acc = 0.12 |\n",
            "Epoch 2: loss = 2.028 | acc = 0.572 | val_loss = 2.013 | val_acc = 0.612 |\n",
            "Epoch 3: loss = 2.003 | acc = 0.524 | val_loss = 1.994 | val_acc = 0.446 |\n",
            "Epoch 4: loss = 1.98 | acc = 0.416 | val_loss = 1.971 | val_acc = 0.413 |\n",
            "Epoch 5: loss = 1.962 | acc = 0.402 | val_loss = 1.958 | val_acc = 0.367 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 1.921 | acc = 0.525 | val_loss = 1.826 | val_acc = 0.64 |\n",
            "Epoch 2: loss = 1.831 | acc = 0.578 | val_loss = 1.803 | val_acc = 0.513 |\n",
            "Epoch 3: loss = 1.835 | acc = 0.482 | val_loss = 1.852 | val_acc = 0.382 |\n",
            "Epoch 4: loss = 1.872 | acc = 0.436 | val_loss = 1.857 | val_acc = 0.434 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.001~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.283 | acc = 0.18 | val_loss = 2.327 | val_acc = 0.199 |\n",
            "Epoch 2: loss = 2.283 | acc = 0.155 | val_loss = 2.263 | val_acc = 0.194 |\n",
            "Epoch 3: loss = 2.299 | acc = 0.138 | val_loss = 2.285 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.295 | acc = 0.129 | val_loss = 2.302 | val_acc = 0.196 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.102 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.304 | acc = 0.101 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.304 | acc = 0.098 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.304 | acc = 0.098 | val_loss = 2.306 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.112 | val_loss = 2.314 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.314 | acc = 0.101 | val_loss = 2.307 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.313 | acc = 0.1 | val_loss = 2.329 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.314 | acc = 0.098 | val_loss = 2.321 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.361 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.364 | acc = 0.102 | val_loss = 2.367 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.357 | acc = 0.103 | val_loss = 2.421 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.359 | acc = 0.098 | val_loss = 2.337 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.212 | acc = 0.181 | val_loss = 2.226 | val_acc = 0.103 |\n",
            "Epoch 2: loss = 2.219 | acc = 0.182 | val_loss = 2.219 | val_acc = 0.214 |\n",
            "Epoch 3: loss = 2.219 | acc = 0.184 | val_loss = 2.242 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.22 | acc = 0.178 | val_loss = 2.226 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.219 | acc = 0.178 | val_loss = 2.194 | val_acc = 0.197 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.33 | acc = 0.102 | val_loss = 2.389 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.322 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.331 | acc = 0.099 | val_loss = 2.41 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.324 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.308 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.31 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.299 | acc = 0.107 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.116 | acc = 0.216 | val_loss = 2.136 | val_acc = 0.179 |\n",
            "Epoch 2: loss = 2.116 | acc = 0.209 | val_loss = 2.133 | val_acc = 0.198 |\n",
            "Epoch 3: loss = 2.116 | acc = 0.207 | val_loss = 2.125 | val_acc = 0.198 |\n",
            "Epoch 4: loss = 2.115 | acc = 0.206 | val_loss = 2.112 | val_acc = 0.199 |\n",
            "Epoch 5: loss = 2.103 | acc = 0.204 | val_loss = 2.114 | val_acc = 0.204 |\n",
            "Epoch 6: loss = 2.115 | acc = 0.203 | val_loss = 2.128 | val_acc = 0.204 |\n",
            "Epoch 7: loss = 2.116 | acc = 0.205 | val_loss = 2.08 | val_acc = 0.29 |\n",
            "Epoch 8: loss = 2.114 | acc = 0.207 | val_loss = 2.124 | val_acc = 0.197 |\n",
            "Epoch 9: loss = 2.115 | acc = 0.207 | val_loss = 2.123 | val_acc = 0.198 |\n",
            "Epoch 10: loss = 2.11 | acc = 0.208 | val_loss = 2.094 | val_acc = 0.224 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.312 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.308 | acc = 0.103 | val_loss = 2.336 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.306 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.307 | acc = 0.099 | val_loss = 2.358 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.31 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.309 | acc = 0.085 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.109 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.049 | acc = 0.318 | val_loss = 2.05 | val_acc = 0.191 |\n",
            "Epoch 2: loss = 2.04 | acc = 0.195 | val_loss = 2.047 | val_acc = 0.19 |\n",
            "Epoch 3: loss = 2.038 | acc = 0.194 | val_loss = 2.055 | val_acc = 0.194 |\n",
            "Epoch 4: loss = 2.04 | acc = 0.196 | val_loss = 2.048 | val_acc = 0.188 |\n",
            "Epoch 5: loss = 2.04 | acc = 0.2 | val_loss = 2.062 | val_acc = 0.196 |\n",
            "Epoch 6: loss = 2.036 | acc = 0.194 | val_loss = 2.029 | val_acc = 0.189 |\n",
            "Epoch 7: loss = 2.038 | acc = 0.197 | val_loss = 2.057 | val_acc = 0.283 |\n",
            "Epoch 8: loss = 2.041 | acc = 0.197 | val_loss = 2.04 | val_acc = 0.19 |\n",
            "Epoch 9: loss = 2.039 | acc = 0.198 | val_loss = 2.042 | val_acc = 0.19 |\n",
            "Epoch 10: loss = 2.041 | acc = 0.191 | val_loss = 2.027 | val_acc = 0.194 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.01~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.315 | acc = 0.115 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.305 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.304 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.305 | acc = 0.103 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.318 | acc = 0.101 | val_loss = 2.308 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.319 | acc = 0.101 | val_loss = 2.32 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.318 | acc = 0.1 | val_loss = 2.321 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.319 | acc = 0.101 | val_loss = 2.324 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.384 | acc = 0.101 | val_loss = 2.411 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.381 | acc = 0.103 | val_loss = 2.407 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.385 | acc = 0.1 | val_loss = 2.349 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.385 | acc = 0.101 | val_loss = 2.415 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.311 | acc = 0.101 | val_loss = 2.309 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.311 | acc = 0.101 | val_loss = 2.311 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.311 | acc = 0.101 | val_loss = 2.308 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.31 | acc = 0.102 | val_loss = 2.307 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 32~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.37 | acc = 0.099 | val_loss = 2.38 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.369 | acc = 0.101 | val_loss = 2.384 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.37 | acc = 0.099 | val_loss = 2.348 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.367 | acc = 0.102 | val_loss = 2.391 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.306 | acc = 0.102 | val_loss = 2.307 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.307 | acc = 0.101 | val_loss = 2.309 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.307 | acc = 0.101 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.307 | acc = 0.099 | val_loss = 2.308 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 64~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.344 | acc = 0.099 | val_loss = 2.326 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.341 | acc = 0.099 | val_loss = 2.318 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.342 | acc = 0.102 | val_loss = 2.366 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.346 | acc = 0.098 | val_loss = 2.331 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.097 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.304 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.101 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.303 | acc = 0.098 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.303 | acc = 0.099 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.305 | acc = 0.101 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.305 | acc = 0.101 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.305 | acc = 0.1 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.305 | acc = 0.1 | val_loss = 2.304 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 0.1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 128~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 10~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.324 | acc = 0.101 | val_loss = 2.328 | val_acc = 0.1 |\n",
            "Epoch 2: loss = 2.324 | acc = 0.099 | val_loss = 2.332 | val_acc = 0.1 |\n",
            "Epoch 3: loss = 2.321 | acc = 0.099 | val_loss = 2.305 | val_acc = 0.1 |\n",
            "Epoch 4: loss = 2.322 | acc = 0.098 | val_loss = 2.307 | val_acc = 0.1 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.001~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.384 | acc = 0.091 | val_loss = 2.376 | val_acc = 0.104 |\n",
            "Epoch 2: loss = 2.384 | acc = 0.093 | val_loss = 2.384 | val_acc = 0.106 |\n",
            "Epoch 3: loss = 2.39 | acc = 0.092 | val_loss = 2.39 | val_acc = 0.109 |\n",
            "Epoch 4: loss = 2.395 | acc = 0.092 | val_loss = 2.393 | val_acc = 0.11 |\n",
            "Epoch 5: loss = 2.404 | acc = 0.091 | val_loss = 2.399 | val_acc = 0.111 |\n",
            "Epoch 6: loss = 2.404 | acc = 0.092 | val_loss = 2.401 | val_acc = 0.112 |\n",
            "Epoch 7: loss = 2.408 | acc = 0.089 | val_loss = 2.403 | val_acc = 0.106 |\n",
            "Epoch 8: loss = 2.408 | acc = 0.089 | val_loss = 2.405 | val_acc = 0.105 |\n",
            "Epoch 9: loss = 2.411 | acc = 0.086 | val_loss = 2.407 | val_acc = 0.099 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.01~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.39 | acc = 0.099 | val_loss = 2.408 | val_acc = 0.122 |\n",
            "Epoch 2: loss = 2.418 | acc = 0.128 | val_loss = 2.429 | val_acc = 0.132 |\n",
            "Epoch 3: loss = 2.429 | acc = 0.141 | val_loss = 2.447 | val_acc = 0.153 |\n",
            "Epoch 4: loss = 2.433 | acc = 0.149 | val_loss = 2.45 | val_acc = 0.16 |\n",
            "Epoch 5: loss = 2.431 | acc = 0.145 | val_loss = 2.451 | val_acc = 0.138 |\n",
            "Epoch 6: loss = 2.428 | acc = 0.122 | val_loss = 2.451 | val_acc = 0.106 |\n",
            "Epoch 7: loss = 2.426 | acc = 0.114 | val_loss = 2.45 | val_acc = 0.102 |\n",
            "Early stopping after 3 epochs without improvement.\n",
            "~~~~~~~~~~~~~~~~~~L1 Lambda: 1e-06~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~L2 Lambda: 1~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Batch Size: 16~~~~~~~~~~~~~~~~~~\n",
            "~~~~~~~~~~~~~~~~~~Learning Rate: 0.1~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1: loss = 2.418 | acc = 0.118 | val_loss = 2.421 | val_acc = 0.098 |\n",
            "Epoch 2: loss = 2.424 | acc = 0.116 | val_loss = 2.422 | val_acc = 0.095 |\n",
            "Epoch 3: loss = 2.419 | acc = 0.12 | val_loss = 2.422 | val_acc = 0.095 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR_10\n",
        "l1_lambdas = [0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "l2_lambdas = [0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "learning_rates = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "transform_CIFAR_10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "    # Numbers from normalization comes from this link:\n",
        "    # https://www.kaggle.com/code/fanbyprinciple/cifar10-explanation-with-pytorch\n",
        "])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "CIFAR_10_training_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_CIFAR_10)\n",
        "CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "\n",
        "CIFAR_10_testing_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_CIFAR_10)\n",
        "CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_testing_set,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=2)\n",
        "\n",
        "find_best_parameters(np.asarray(CIFAR_10_training_set.data.reshape(-1,32*32*3)) ,\n",
        "            np.asarray(CIFAR_10_training_set.targets),\n",
        "            np.asarray(CIFAR_10_testing_set.data.reshape(-1,32*32*3)),\n",
        "                     np.asarray(CIFAR_10_testing_set.targets),\n",
        "            num_hidden_layers=2,num_neurons=128, l1_lambdas=l1_lambdas, l2_lambdas=l2_lambdas, batch_sizes=batch_sizes, learning_rates=learning_rates, epochs=20)\n"
      ],
      "metadata": {
        "id": "ZPmKdUoGZi-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doesn't seem to improve alot. Maybe there is a mistake in how we implemented the bias."
      ],
      "metadata": {
        "id": "CTr9hBS3Qqep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist_x_train, fashion_mnist_x_test, fashion_mnist_y_train, fashion_mnist_y_test = load_and_preprocess_dataset(\"fashion_mnist\", normalize=False)\n",
        "model = MLP(fashion_mnist_x_train ,\n",
        "            fashion_mnist_y_train,\n",
        "            fashion_mnist_x_test,\n",
        "            fashion_mnist_y_test,\n",
        "            num_hidden_layers=2,num_neurons=128)\n",
        "\n",
        "model.train(batch_size=64,epochs=10,lr=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "clZXGJ0k1klR",
        "outputId": "9c85567f-de18-4594-d058-2008e83b25be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-875a3115494d>:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_1d = labels.astype(np.int).reshape(-1)\n",
            "<ipython-input-21-875a3115494d>:233: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.weights = np.asarray(self.weights)\n",
            "<ipython-input-21-875a3115494d>:101: RuntimeWarning: overflow encountered in exp\n",
            "  return 1./(1. + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 2.19 | acc = 0.137 | val_loss = 2.221 | val_acc = 0.118 |\n",
            "Epoch 2: loss = 2.262 | acc = 0.122 | val_loss = 2.302 | val_acc = 0.098 |\n",
            "Epoch 3: loss = 2.289 | acc = 0.115 | val_loss = 2.302 | val_acc = 0.102 |\n",
            "Epoch 4: loss = 2.297 | acc = 0.109 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 5: loss = 2.303 | acc = 0.1 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 6: loss = 2.303 | acc = 0.102 | val_loss = 2.303 | val_acc = 0.105 |\n",
            "Epoch 7: loss = 2.303 | acc = 0.111 | val_loss = 2.303 | val_acc = 0.1 |\n",
            "Epoch 8: loss = 2.303 | acc = 0.1 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 9: loss = 2.303 | acc = 0.1 | val_loss = 2.302 | val_acc = 0.1 |\n",
            "Epoch 10: loss = 2.303 | acc = 0.106 | val_loss = 2.302 | val_acc = 0.1 |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ccbcdc7d9b74>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             num_hidden_layers=2,num_neurons=128)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-875a3115494d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m    467\u001b[0m           \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# Computes and store average training loss and accuracy over all batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-875a3115494d>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, batch_y)\u001b[0m\n\u001b[1;32m    391\u001b[0m           \u001b[0;31m# Mini-step 5. Update the weights of the current layer by subtracting the scaled gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mregularization_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mregularized_term_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# L1 regularization term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr_scaled_gradient\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mregularization_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 6: Introducing CNN\n",
        "\n",
        "Using PyTorch, create a convolutional neural network (CNN) with 2 convolutional and 2 fully connected layers.\n",
        "Although you are free in your choice of the hyperparameters of the convolutional layers, set the number of units\n",
        "in the fully connected layers to be 128. Also, set the activations in all of the layers to be ReLU. Train this CNN\n",
        "on the Fashion MNIST dataset. Does using a CNN increase/decrease the accuracy compared to using MLPs?\n",
        "Provide comments on your results."
      ],
      "metadata": {
        "id": "5DyJmXkJ10U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_output_dimensions(image_dimension, kernel_size, stride, padding):\n",
        "    # Calculate the output dimensions\n",
        "    output = math.floor((image_dimension + 2 * padding - kernel_size) / stride) + 1\n",
        "\n",
        "    output = math.floor(output / 2)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "Ys6A7JnxCjAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, image_dimesions = 28, channels=1):\n",
        "        super(CNN, self).__init__()\n",
        "        self.image_dimesions = image_dimesions\n",
        "        # Define the convolutional layer\n",
        "        self.conv1 = nn.Conv2d(channels, 512, kernel_size=7, stride=2, padding=3)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Define the pooling layer\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.new_dimesions = calculate_output_dimensions(self.image_dimesions, 7, 2, 3)\n",
        "\n",
        "        # Define the second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        # Define the BatchNorm2d layer\n",
        "        self.bn2 = nn.BatchNorm2d(1024)\n",
        "\n",
        "        # Define the second pooling layer\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.new_dimesions = calculate_output_dimensions(self.new_dimesions, 5, 2, 1)\n",
        "\n",
        "        # Define first fully connected layer\n",
        "        self.fc1 = nn.Linear(int(1024 * self.new_dimesions * self.new_dimesions), 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Define second fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        # Output\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the convolutional layer\n",
        "        x = self.conv1(x)\n",
        "        # Apply the ReLU activation function\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        # Apply the first pooling layer\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Apply the second convolutional layer\n",
        "        x = self.conv2(x)\n",
        "        # Apply the ReLU activation function\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        # Apply the BatchNorm2d layer\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        # Apply the second pooling layer\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the output of the pooling layer\n",
        "        x = x.view(-1, int(1024 * self.new_dimesions * self.new_dimesions ))\n",
        "\n",
        "        # Apply the first fully connected layer\n",
        "        x = self.fc1(x)\n",
        "        # Apply the ReLU activation function\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        # Apply the second fully connected layer\n",
        "        x = self.fc2(x)\n",
        "        # Apply the ReLU activation function\n",
        "        x = self.relu4(x)\n",
        "\n",
        "        # Apply the final fully connected layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Apply the softmax layer\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        # Return the output of the fully connected layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "IxUVHvUPHQcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def train_CNN(batch_size, training_set, trainloader, test_set, testloader, optimizer, num_epochs, cnn):\n",
        "    train_accuracies = []\n",
        "    # Define the loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        cnn.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in trainloader:\n",
        "            inputs, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            inputs = inputs.to('cuda')\n",
        "            labels = labels.to('cuda')\n",
        "            outputs = cnn(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        average_loss = running_loss / len(trainloader)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        cnn.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                inputs, labels = batch\n",
        "                inputs = inputs.to('cuda')\n",
        "                labels = labels.to('cuda')\n",
        "                outputs = cnn(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        average_val_loss = val_loss / len(testloader)\n",
        "\n",
        "        # Print the training and validation metrics for this epoch\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {average_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
        "              f\"Validation Loss: {average_val_loss:.4f}, Validation Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return train_accuracies"
      ],
      "metadata": {
        "id": "P47m72tLkwex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST using ADAM optimizer."
      ],
      "metadata": {
        "id": "nsiCdExtCM6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transform to convert images to tensors and normalize\n",
        "transform_FMNIST = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "FMNIST_training_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                            download=True,\n",
        "                                            train=True,\n",
        "                                            transform=transform_FMNIST)\n",
        "\n",
        "FMINIST_trainloader = torch.utils.data.DataLoader(FMNIST_training_set,\n",
        "                                                 batch_size=batch_size,\n",
        "                                                 shuffle=True,\n",
        "                                                 num_workers=2)\n",
        "\n",
        "FMNIST_test_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                        download=True,\n",
        "                                        train=False,\n",
        "                                        transform=transform_FMNIST)\n",
        "\n",
        "FMINIST_testloader = torch.utils.data.DataLoader(FMNIST_test_set,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=2)\n",
        "\n",
        "# Define the CNN\n",
        "cnn = CNN(channels=1, image_dimesions=28)\n",
        "cnn = cnn.to('cuda')\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=0.001)  # Make sure to create the optimizer after defining the CNN\n",
        "\n",
        "num_epochs = 25  # You can change this as needed\n",
        "\n",
        "# Call the train_CNN function\n",
        "trained_cnn = train_CNN(batch_size, FMNIST_training_set, FMINIST_trainloader, FMNIST_test_set, FMINIST_testloader, optimizer, num_epochs, cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQobfDNFn0Tv",
        "outputId": "2b148a5d-f38e-4a21-95d2-243bfa7058e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 - Train Loss: 1.6462, Train Acc: 81.76%, Validation Loss: 1.6451, Validation Acc: 81.62%\n",
            "Epoch 2/25 - Train Loss: 1.6103, Train Acc: 85.08%, Validation Loss: 1.6128, Validation Acc: 84.75%\n",
            "Epoch 3/25 - Train Loss: 1.6031, Train Acc: 85.77%, Validation Loss: 1.6122, Validation Acc: 84.82%\n",
            "Epoch 4/25 - Train Loss: 1.5881, Train Acc: 87.27%, Validation Loss: 1.6030, Validation Acc: 85.71%\n",
            "Epoch 5/25 - Train Loss: 1.5887, Train Acc: 87.18%, Validation Loss: 1.6025, Validation Acc: 85.76%\n",
            "Epoch 6/25 - Train Loss: 1.5848, Train Acc: 87.56%, Validation Loss: 1.6016, Validation Acc: 85.94%\n",
            "Epoch 7/25 - Train Loss: 1.5914, Train Acc: 86.96%, Validation Loss: 1.6021, Validation Acc: 85.86%\n",
            "Epoch 8/25 - Train Loss: 1.5782, Train Acc: 88.25%, Validation Loss: 1.5955, Validation Acc: 86.51%\n",
            "Epoch 9/25 - Train Loss: 1.5733, Train Acc: 88.75%, Validation Loss: 1.5945, Validation Acc: 86.71%\n",
            "Epoch 10/25 - Train Loss: 1.5758, Train Acc: 88.48%, Validation Loss: 1.5921, Validation Acc: 86.90%\n",
            "Epoch 11/25 - Train Loss: 1.5741, Train Acc: 88.66%, Validation Loss: 1.5910, Validation Acc: 86.93%\n",
            "Epoch 12/25 - Train Loss: 1.5707, Train Acc: 89.00%, Validation Loss: 1.5828, Validation Acc: 87.67%\n",
            "Epoch 13/25 - Train Loss: 1.5735, Train Acc: 88.74%, Validation Loss: 1.5906, Validation Acc: 87.02%\n",
            "Epoch 14/25 - Train Loss: 1.5721, Train Acc: 88.88%, Validation Loss: 1.6074, Validation Acc: 85.30%\n",
            "Epoch 15/25 - Train Loss: 1.5691, Train Acc: 89.16%, Validation Loss: 1.5815, Validation Acc: 87.89%\n",
            "Epoch 16/25 - Train Loss: 1.5676, Train Acc: 89.33%, Validation Loss: 1.5913, Validation Acc: 86.94%\n",
            "Epoch 17/25 - Train Loss: 1.5749, Train Acc: 88.59%, Validation Loss: 1.5872, Validation Acc: 87.40%\n",
            "Epoch 18/25 - Train Loss: 1.5740, Train Acc: 88.68%, Validation Loss: 1.5810, Validation Acc: 88.00%\n",
            "Epoch 19/25 - Train Loss: 1.5688, Train Acc: 89.19%, Validation Loss: 1.6131, Validation Acc: 84.73%\n",
            "Epoch 20/25 - Train Loss: 1.5691, Train Acc: 89.19%, Validation Loss: 1.5912, Validation Acc: 86.96%\n",
            "Epoch 21/25 - Train Loss: 1.5659, Train Acc: 89.52%, Validation Loss: 1.5943, Validation Acc: 86.64%\n",
            "Epoch 22/25 - Train Loss: 1.5687, Train Acc: 89.22%, Validation Loss: 1.5846, Validation Acc: 87.67%\n",
            "Epoch 23/25 - Train Loss: 1.5709, Train Acc: 89.00%, Validation Loss: 1.5886, Validation Acc: 87.21%\n",
            "Epoch 24/25 - Train Loss: 1.5686, Train Acc: 89.23%, Validation Loss: 1.5816, Validation Acc: 88.03%\n",
            "Epoch 25/25 - Train Loss: 1.5675, Train Acc: 89.33%, Validation Loss: 1.5882, Validation Acc: 87.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = range(1, 26)\n",
        "train_acc = [81.71, 84.99, 86.54, 87.15, 87.65, 87.65, 87.49, 88.46, 88.20, 88.49, 88.88, 88.75, 88.72, 89.02, 88.91, 89.35, 89.24, 89.20, 89.41, 89.55, 89.02, 88.31, 89.07, 89.48, 89.08]\n",
        "val_acc = [82.31, 85.62, 83.62, 86.37, 85.00, 85.73, 85.20, 86.75, 86.66, 87.02, 86.06, 86.54, 87.74, 86.37, 87.98, 87.10, 86.99, 85.60, 87.44, 88.01, 87.42, 86.09, 87.58, 86.84, 86.77]\n",
        "\n",
        "# Create the line graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_acc, label='Train Accuracy', marker='o', color='blue')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy', marker='o', color='orange')\n",
        "\n",
        "# Customize the graph\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Validation Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the graph\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "CTB5Lf7aJi8u",
        "outputId": "6e379481-034f-4d43-e0ed-47310281ef43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR using ADAM optimizer."
      ],
      "metadata": {
        "id": "3Dh9OK1kCGOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transform to convert images to tensors and normalize\n",
        "transform_CIFAR_10 = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "    # Numbers from normalization comes from this link:\n",
        "    # https://www.kaggle.com/code/fanbyprinciple/cifar10-explanation-with-pytorch\n",
        "        ])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "CIFAR_10_training_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_CIFAR_10)\n",
        "CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "CIFAR_10_testing_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_CIFAR_10)\n",
        "CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_testing_set,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=2)\n",
        "\n",
        "# Define the CNN\n",
        "cnn = CNN(channels=3, image_dimesions=32)\n",
        "cnn = cnn.to('cuda')\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=0.001)  # Make sure to create the optimizer after defining the CNN\n",
        "\n",
        "num_epochs = 25  # You can change this as needed\n",
        "\n",
        "# Call the train_CNN function\n",
        "trained_cnn = train_CNN(batch_size, CIFAR_10_training_set, CIFAR_10_trainloader, CIFAR_10_testing_set, CIFAR_10_testloader, optimizer, num_epochs, cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs_sWOUIAvb6",
        "outputId": "1bf80dac-71a8-446c-f57c-b45782845b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/25 - Train Loss: 2.0480, Train Acc: 40.76%, Validation Loss: 1.9880, Validation Acc: 47.14%\n",
            "Epoch 2/25 - Train Loss: 1.9572, Train Acc: 49.92%, Validation Loss: 1.9340, Validation Acc: 52.31%\n",
            "Epoch 3/25 - Train Loss: 1.9368, Train Acc: 51.93%, Validation Loss: 1.9797, Validation Acc: 47.75%\n",
            "Epoch 4/25 - Train Loss: 1.9042, Train Acc: 55.40%, Validation Loss: 1.8955, Validation Acc: 56.35%\n",
            "Epoch 5/25 - Train Loss: 1.9138, Train Acc: 54.39%, Validation Loss: 1.9140, Validation Acc: 54.45%\n",
            "Epoch 6/25 - Train Loss: 1.8739, Train Acc: 58.50%, Validation Loss: 1.8915, Validation Acc: 56.71%\n",
            "Epoch 7/25 - Train Loss: 1.8738, Train Acc: 58.41%, Validation Loss: 1.8778, Validation Acc: 58.09%\n",
            "Epoch 8/25 - Train Loss: 1.8513, Train Acc: 60.77%, Validation Loss: 1.8774, Validation Acc: 57.97%\n",
            "Epoch 9/25 - Train Loss: 1.8422, Train Acc: 61.62%, Validation Loss: 1.8692, Validation Acc: 59.07%\n",
            "Epoch 10/25 - Train Loss: 1.8220, Train Acc: 63.67%, Validation Loss: 1.8571, Validation Acc: 60.12%\n",
            "Epoch 11/25 - Train Loss: 1.8245, Train Acc: 63.49%, Validation Loss: 1.8710, Validation Acc: 58.63%\n",
            "Epoch 12/25 - Train Loss: 1.8118, Train Acc: 64.74%, Validation Loss: 1.8569, Validation Acc: 60.12%\n",
            "Epoch 13/25 - Train Loss: 1.8094, Train Acc: 65.00%, Validation Loss: 1.8544, Validation Acc: 60.33%\n",
            "Epoch 14/25 - Train Loss: 1.8046, Train Acc: 65.48%, Validation Loss: 1.8537, Validation Acc: 60.53%\n",
            "Epoch 15/25 - Train Loss: 1.7991, Train Acc: 65.99%, Validation Loss: 1.8496, Validation Acc: 60.98%\n",
            "Epoch 16/25 - Train Loss: 1.7988, Train Acc: 66.13%, Validation Loss: 1.8497, Validation Acc: 60.89%\n",
            "Epoch 17/25 - Train Loss: 1.7895, Train Acc: 67.00%, Validation Loss: 1.8421, Validation Acc: 61.53%\n",
            "Epoch 18/25 - Train Loss: 1.7868, Train Acc: 67.28%, Validation Loss: 1.8527, Validation Acc: 60.49%\n",
            "Epoch 19/25 - Train Loss: 1.7875, Train Acc: 67.15%, Validation Loss: 1.8410, Validation Acc: 61.77%\n",
            "Epoch 20/25 - Train Loss: 1.7840, Train Acc: 67.56%, Validation Loss: 1.8408, Validation Acc: 61.76%\n",
            "Epoch 21/25 - Train Loss: 1.7769, Train Acc: 68.27%, Validation Loss: 1.8309, Validation Acc: 62.78%\n",
            "Epoch 22/25 - Train Loss: 1.7778, Train Acc: 68.14%, Validation Loss: 1.8368, Validation Acc: 62.23%\n",
            "Epoch 23/25 - Train Loss: 1.7760, Train Acc: 68.35%, Validation Loss: 1.8700, Validation Acc: 58.82%\n",
            "Epoch 24/25 - Train Loss: 1.7800, Train Acc: 67.93%, Validation Loss: 1.8484, Validation Acc: 61.01%\n",
            "Epoch 25/25 - Train Loss: 1.7615, Train Acc: 69.86%, Validation Loss: 1.8269, Validation Acc: 63.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transform to convert images to tensors and normalize\n",
        "transform_CIFAR_10 = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "    # Numbers from normalization comes from this link:\n",
        "    # https://www.kaggle.com/code/fanbyprinciple/cifar10-explanation-with-pytorch\n",
        "        ])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "CIFAR_10_training_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform_CIFAR_10)\n",
        "CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "CIFAR_10_testing_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_CIFAR_10)\n",
        "CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_testing_set,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=2)\n",
        "\n",
        "# Define the CNN\n",
        "cnn = CNN(channels=3, image_dimesions=32)\n",
        "cnn = cnn.to('cuda')\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.8)  # Make sure to create the optimizer after defining the CNN\n",
        "\n",
        "num_epochs = 100  # You can change this as needed\n",
        "\n",
        "# Call the train_CNN function\n",
        "trained_cnn = train_CNN(batch_size, CIFAR_10_training_set, CIFAR_10_trainloader, CIFAR_10_testing_set, CIFAR_10_testloader, optimizer, num_epochs, cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-u3H3L12kN2",
        "outputId": "11bc8b51-239b-42f9-8fb2-49b5de936130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 59512626.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 1/100 - Train Loss: 2.2561, Train Acc: 21.26%, Validation Loss: 2.2005, Validation Acc: 27.82%\n",
            "Epoch 2/100 - Train Loss: 2.1492, Train Acc: 33.12%, Validation Loss: 2.0943, Validation Acc: 39.36%\n",
            "Epoch 3/100 - Train Loss: 2.0602, Train Acc: 42.67%, Validation Loss: 2.0301, Validation Acc: 46.17%\n",
            "Epoch 4/100 - Train Loss: 2.0074, Train Acc: 47.94%, Validation Loss: 2.0007, Validation Acc: 48.33%\n",
            "Epoch 5/100 - Train Loss: 1.9721, Train Acc: 51.26%, Validation Loss: 1.9803, Validation Acc: 50.10%\n",
            "Epoch 6/100 - Train Loss: 1.9458, Train Acc: 54.02%, Validation Loss: 1.9654, Validation Acc: 51.20%\n",
            "Epoch 7/100 - Train Loss: 1.9249, Train Acc: 56.04%, Validation Loss: 1.9510, Validation Acc: 52.73%\n",
            "Epoch 8/100 - Train Loss: 1.9065, Train Acc: 57.77%, Validation Loss: 1.9487, Validation Acc: 52.31%\n",
            "Epoch 9/100 - Train Loss: 1.8923, Train Acc: 59.52%, Validation Loss: 1.9398, Validation Acc: 53.71%\n",
            "Epoch 10/100 - Train Loss: 1.8779, Train Acc: 61.39%, Validation Loss: 1.9315, Validation Acc: 54.70%\n",
            "Epoch 11/100 - Train Loss: 1.8634, Train Acc: 62.57%, Validation Loss: 1.9171, Validation Acc: 56.49%\n",
            "Epoch 12/100 - Train Loss: 1.8525, Train Acc: 63.70%, Validation Loss: 1.9175, Validation Acc: 55.91%\n",
            "Epoch 13/100 - Train Loss: 1.8397, Train Acc: 65.02%, Validation Loss: 1.9063, Validation Acc: 56.82%\n",
            "Epoch 14/100 - Train Loss: 1.8304, Train Acc: 65.90%, Validation Loss: 1.9116, Validation Acc: 56.69%\n",
            "Epoch 15/100 - Train Loss: 1.8282, Train Acc: 66.37%, Validation Loss: 1.9004, Validation Acc: 57.57%\n",
            "Epoch 16/100 - Train Loss: 1.8242, Train Acc: 66.67%, Validation Loss: 1.9031, Validation Acc: 57.31%\n",
            "Epoch 17/100 - Train Loss: 1.8158, Train Acc: 67.25%, Validation Loss: 1.8930, Validation Acc: 58.47%\n",
            "Epoch 18/100 - Train Loss: 1.8116, Train Acc: 67.66%, Validation Loss: 1.8917, Validation Acc: 58.03%\n",
            "Epoch 19/100 - Train Loss: 1.8065, Train Acc: 67.95%, Validation Loss: 1.8974, Validation Acc: 57.26%\n",
            "Epoch 20/100 - Train Loss: 1.8039, Train Acc: 68.26%, Validation Loss: 1.9016, Validation Acc: 56.73%\n",
            "Epoch 21/100 - Train Loss: 1.7969, Train Acc: 68.92%, Validation Loss: 1.8959, Validation Acc: 57.59%\n",
            "Epoch 22/100 - Train Loss: 1.7991, Train Acc: 68.71%, Validation Loss: 1.8911, Validation Acc: 58.48%\n",
            "Epoch 23/100 - Train Loss: 1.7974, Train Acc: 68.91%, Validation Loss: 1.8965, Validation Acc: 57.77%\n",
            "Epoch 24/100 - Train Loss: 1.7929, Train Acc: 69.23%, Validation Loss: 1.8994, Validation Acc: 57.14%\n",
            "Epoch 25/100 - Train Loss: 1.7989, Train Acc: 68.67%, Validation Loss: 1.8972, Validation Acc: 57.77%\n",
            "Epoch 26/100 - Train Loss: 1.7987, Train Acc: 68.53%, Validation Loss: 1.8938, Validation Acc: 57.77%\n",
            "Epoch 27/100 - Train Loss: 1.7994, Train Acc: 68.51%, Validation Loss: 1.8979, Validation Acc: 57.28%\n",
            "Epoch 28/100 - Train Loss: 1.8031, Train Acc: 68.19%, Validation Loss: 1.8970, Validation Acc: 57.80%\n",
            "Epoch 29/100 - Train Loss: 1.7930, Train Acc: 68.83%, Validation Loss: 1.9032, Validation Acc: 56.33%\n",
            "Epoch 30/100 - Train Loss: 1.7839, Train Acc: 69.46%, Validation Loss: 1.9012, Validation Acc: 56.50%\n",
            "Epoch 31/100 - Train Loss: 1.7787, Train Acc: 70.11%, Validation Loss: 1.9006, Validation Acc: 56.69%\n",
            "Epoch 32/100 - Train Loss: 1.7867, Train Acc: 69.44%, Validation Loss: 1.8874, Validation Acc: 58.04%\n",
            "Epoch 33/100 - Train Loss: 1.7851, Train Acc: 69.45%, Validation Loss: 1.8978, Validation Acc: 57.01%\n",
            "Epoch 34/100 - Train Loss: 1.7825, Train Acc: 69.53%, Validation Loss: 1.8990, Validation Acc: 56.76%\n",
            "Epoch 35/100 - Train Loss: 1.7786, Train Acc: 69.89%, Validation Loss: 1.8907, Validation Acc: 57.42%\n",
            "Epoch 36/100 - Train Loss: 1.7683, Train Acc: 70.67%, Validation Loss: 1.8889, Validation Acc: 57.75%\n",
            "Epoch 37/100 - Train Loss: 1.7696, Train Acc: 70.58%, Validation Loss: 1.8795, Validation Acc: 58.41%\n",
            "Epoch 38/100 - Train Loss: 1.7661, Train Acc: 70.95%, Validation Loss: 1.8786, Validation Acc: 58.68%\n",
            "Epoch 39/100 - Train Loss: 1.7628, Train Acc: 71.27%, Validation Loss: 1.8865, Validation Acc: 57.74%\n",
            "Epoch 40/100 - Train Loss: 1.7558, Train Acc: 71.87%, Validation Loss: 1.8728, Validation Acc: 59.14%\n",
            "Epoch 41/100 - Train Loss: 1.7452, Train Acc: 72.88%, Validation Loss: 1.8846, Validation Acc: 58.08%\n",
            "Epoch 42/100 - Train Loss: 1.7445, Train Acc: 72.96%, Validation Loss: 1.8810, Validation Acc: 58.67%\n",
            "Epoch 43/100 - Train Loss: 1.7541, Train Acc: 72.17%, Validation Loss: 1.8869, Validation Acc: 57.75%\n",
            "Epoch 44/100 - Train Loss: 1.7468, Train Acc: 72.66%, Validation Loss: 1.9587, Validation Acc: 50.08%\n",
            "Epoch 45/100 - Train Loss: 1.7418, Train Acc: 73.31%, Validation Loss: 1.8828, Validation Acc: 58.27%\n",
            "Epoch 46/100 - Train Loss: 1.7307, Train Acc: 74.34%, Validation Loss: 1.8619, Validation Acc: 60.12%\n",
            "Epoch 47/100 - Train Loss: 1.7261, Train Acc: 74.58%, Validation Loss: 1.8805, Validation Acc: 58.05%\n",
            "Epoch 48/100 - Train Loss: 1.7234, Train Acc: 74.87%, Validation Loss: 1.8824, Validation Acc: 57.84%\n",
            "Epoch 49/100 - Train Loss: 1.7253, Train Acc: 74.76%, Validation Loss: 1.8667, Validation Acc: 59.58%\n",
            "Epoch 50/100 - Train Loss: 1.7210, Train Acc: 75.14%, Validation Loss: 1.8864, Validation Acc: 57.56%\n",
            "Epoch 51/100 - Train Loss: 1.7102, Train Acc: 76.12%, Validation Loss: 1.8692, Validation Acc: 59.30%\n",
            "Epoch 52/100 - Train Loss: 1.7117, Train Acc: 76.04%, Validation Loss: 1.8812, Validation Acc: 58.08%\n",
            "Epoch 53/100 - Train Loss: 1.7113, Train Acc: 76.06%, Validation Loss: 1.8728, Validation Acc: 58.87%\n",
            "Epoch 54/100 - Train Loss: 1.7083, Train Acc: 76.31%, Validation Loss: 1.8718, Validation Acc: 59.04%\n",
            "Epoch 55/100 - Train Loss: 1.7031, Train Acc: 76.72%, Validation Loss: 1.8866, Validation Acc: 57.63%\n",
            "Epoch 56/100 - Train Loss: 1.6970, Train Acc: 77.42%, Validation Loss: 1.8787, Validation Acc: 58.23%\n",
            "Epoch 57/100 - Train Loss: 1.6906, Train Acc: 77.96%, Validation Loss: 1.8695, Validation Acc: 59.01%\n",
            "Epoch 58/100 - Train Loss: 1.6845, Train Acc: 78.60%, Validation Loss: 1.8752, Validation Acc: 58.74%\n",
            "Epoch 59/100 - Train Loss: 1.6769, Train Acc: 79.32%, Validation Loss: 1.8731, Validation Acc: 58.61%\n",
            "Epoch 60/100 - Train Loss: 1.6787, Train Acc: 79.01%, Validation Loss: 1.8659, Validation Acc: 59.52%\n",
            "Epoch 61/100 - Train Loss: 1.6684, Train Acc: 79.98%, Validation Loss: 1.8711, Validation Acc: 58.78%\n",
            "Epoch 62/100 - Train Loss: 1.6674, Train Acc: 80.10%, Validation Loss: 1.8562, Validation Acc: 60.52%\n",
            "Epoch 63/100 - Train Loss: 1.6587, Train Acc: 80.87%, Validation Loss: 1.8612, Validation Acc: 59.77%\n",
            "Epoch 64/100 - Train Loss: 1.6598, Train Acc: 80.89%, Validation Loss: 1.8794, Validation Acc: 58.11%\n",
            "Epoch 65/100 - Train Loss: 1.6690, Train Acc: 79.99%, Validation Loss: 1.8664, Validation Acc: 59.39%\n",
            "Epoch 66/100 - Train Loss: 1.6641, Train Acc: 80.34%, Validation Loss: 1.8742, Validation Acc: 58.61%\n",
            "Epoch 67/100 - Train Loss: 1.6599, Train Acc: 80.78%, Validation Loss: 1.8598, Validation Acc: 59.94%\n",
            "Epoch 68/100 - Train Loss: 1.6527, Train Acc: 81.39%, Validation Loss: 1.8700, Validation Acc: 58.72%\n",
            "Epoch 69/100 - Train Loss: 1.6495, Train Acc: 81.77%, Validation Loss: 1.8744, Validation Acc: 58.33%\n",
            "Epoch 70/100 - Train Loss: 1.6496, Train Acc: 81.69%, Validation Loss: 1.8687, Validation Acc: 59.01%\n",
            "Epoch 71/100 - Train Loss: 1.6395, Train Acc: 82.67%, Validation Loss: 1.8658, Validation Acc: 59.57%\n",
            "Epoch 72/100 - Train Loss: 1.6394, Train Acc: 82.69%, Validation Loss: 1.8612, Validation Acc: 59.88%\n",
            "Epoch 73/100 - Train Loss: 1.6424, Train Acc: 82.36%, Validation Loss: 1.8697, Validation Acc: 58.92%\n",
            "Epoch 74/100 - Train Loss: 1.6451, Train Acc: 82.19%, Validation Loss: 1.8585, Validation Acc: 60.18%\n",
            "Epoch 75/100 - Train Loss: 1.6418, Train Acc: 82.53%, Validation Loss: 1.8566, Validation Acc: 60.22%\n",
            "Epoch 76/100 - Train Loss: 1.6355, Train Acc: 83.04%, Validation Loss: 1.8689, Validation Acc: 59.04%\n",
            "Epoch 77/100 - Train Loss: 1.6328, Train Acc: 83.34%, Validation Loss: 1.8574, Validation Acc: 60.32%\n",
            "Epoch 78/100 - Train Loss: 1.6299, Train Acc: 83.61%, Validation Loss: 1.8613, Validation Acc: 59.67%\n",
            "Epoch 79/100 - Train Loss: 1.6279, Train Acc: 83.87%, Validation Loss: 1.8614, Validation Acc: 59.72%\n",
            "Epoch 80/100 - Train Loss: 1.6255, Train Acc: 83.97%, Validation Loss: 1.8637, Validation Acc: 59.65%\n",
            "Epoch 81/100 - Train Loss: 1.6219, Train Acc: 84.36%, Validation Loss: 1.8552, Validation Acc: 60.58%\n",
            "Epoch 82/100 - Train Loss: 1.6163, Train Acc: 84.80%, Validation Loss: 1.8647, Validation Acc: 59.45%\n",
            "Epoch 83/100 - Train Loss: 1.6154, Train Acc: 84.86%, Validation Loss: 1.8615, Validation Acc: 59.66%\n",
            "Epoch 84/100 - Train Loss: 1.6123, Train Acc: 85.14%, Validation Loss: 1.8580, Validation Acc: 60.16%\n",
            "Epoch 85/100 - Train Loss: 1.6095, Train Acc: 85.40%, Validation Loss: 1.8583, Validation Acc: 60.18%\n",
            "Epoch 86/100 - Train Loss: 1.6081, Train Acc: 85.54%, Validation Loss: 1.8529, Validation Acc: 60.61%\n",
            "Epoch 87/100 - Train Loss: 1.6080, Train Acc: 85.55%, Validation Loss: 1.8563, Validation Acc: 60.32%\n",
            "Epoch 88/100 - Train Loss: 1.6062, Train Acc: 85.69%, Validation Loss: 1.8524, Validation Acc: 60.84%\n",
            "Epoch 89/100 - Train Loss: 1.6041, Train Acc: 85.84%, Validation Loss: 1.8526, Validation Acc: 60.38%\n",
            "Epoch 90/100 - Train Loss: 1.6034, Train Acc: 85.93%, Validation Loss: 1.8613, Validation Acc: 59.69%\n",
            "Epoch 91/100 - Train Loss: 1.6013, Train Acc: 86.10%, Validation Loss: 1.8522, Validation Acc: 60.56%\n",
            "Epoch 92/100 - Train Loss: 1.6002, Train Acc: 86.17%, Validation Loss: 1.8554, Validation Acc: 60.44%\n",
            "Epoch 93/100 - Train Loss: 1.5993, Train Acc: 86.23%, Validation Loss: 1.8486, Validation Acc: 61.10%\n",
            "Epoch 94/100 - Train Loss: 1.6003, Train Acc: 86.21%, Validation Loss: 1.8538, Validation Acc: 60.42%\n",
            "Epoch 95/100 - Train Loss: 1.5981, Train Acc: 86.36%, Validation Loss: 1.8512, Validation Acc: 60.88%\n",
            "Epoch 96/100 - Train Loss: 1.5976, Train Acc: 86.37%, Validation Loss: 1.8526, Validation Acc: 60.60%\n",
            "Epoch 97/100 - Train Loss: 1.5968, Train Acc: 86.46%, Validation Loss: 1.8491, Validation Acc: 61.04%\n",
            "Epoch 98/100 - Train Loss: 1.5968, Train Acc: 86.45%, Validation Loss: 1.8542, Validation Acc: 60.57%\n",
            "Epoch 99/100 - Train Loss: 1.5968, Train Acc: 86.49%, Validation Loss: 1.8509, Validation Acc: 60.84%\n",
            "Epoch 100/100 - Train Loss: 1.5956, Train Acc: 86.55%, Validation Loss: 1.8473, Validation Acc: 61.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_accuracy = [40.76, 49.92, 51.93, 55.40, 54.39, 58.50, 58.41, 60.77, 61.62, 63.67, 63.49, 64.74, 65.00, 65.48, 65.99, 66.13, 67.00, 67.28, 67.15, 67.56, 68.27, 68.14, 68.35, 67.93, 69.86]\n",
        "validation_accuracy = [47.14, 52.31, 47.75, 56.35, 54.45, 56.71, 58.09, 57.97, 59.07, 60.12, 58.63, 60.12, 60.33, 60.53, 60.98, 60.89, 61.53, 60.49, 61.77, 61.76, 62.78, 62.23, 58.82, 61.01, 63.24]\n",
        "\n",
        "# Create a list of epoch numbers (1 to 25)\n",
        "epochs = list(range(1, 26))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_accuracy, label=\"Train Accuracy\", marker='o')\n",
        "plt.plot(epochs, validation_accuracy, label=\"Validation Accuracy\", marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Training and Validation Accuracy Over 25 Epochs\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "6e2RTmSvLx5O",
        "outputId": "b5dabd40-e435-41b7-f89e-8f59eb559de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def test_sgd_momentumFMNIST(num_epochs, cnn):\n",
        "    # Loop over different momentum values\n",
        "    momentum_accuracies = {}\n",
        "    for momentum in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "\n",
        "      # Load the Fashion MNIST dataset\n",
        "        FMNIST_training_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                                    download=True,\n",
        "                                                    train=True,\n",
        "                                                    transform=transform_FMNIST)\n",
        "\n",
        "        FMINIST_trainloader = torch.utils.data.DataLoader(FMNIST_training_set,\n",
        "                                                         batch_size=batch_size,\n",
        "                                                         shuffle=True,\n",
        "                                                         num_workers=2)\n",
        "\n",
        "        FMNIST_test_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data',\n",
        "                                                download=True,\n",
        "                                                train=False,\n",
        "                                                transform=transform_FMNIST)\n",
        "\n",
        "        FMINIST_testloader = torch.utils.data.DataLoader(FMNIST_test_set,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        num_workers=2)\n",
        "        # Define the SGD optimizer with the current momentum\n",
        "        optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=momentum)\n",
        "\n",
        "        print(f\"Testing SGD optimizer with momentum {momentum}...\")\n",
        "\n",
        "        # Train the CNN with the current optimizer\n",
        "        train_accuracies = train_CNN(batch_size, FMNIST_training_set, FMINIST_trainloader, FMNIST_test_set, FMINIST_testloader, optimizer, num_epochs, cnn)\n",
        "\n",
        "        momentum_accuracies[momentum] = train_accuracies\n",
        "\n",
        "    for momentum in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "        plt.plot(range(1, num_epochs + 1), momentum_accuracies[momentum], label=f'Momentum {momentum}')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Accuracy (%)')\n",
        "    plt.title('Training Accuracy vs. Epoch for Different Momentums')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vSsnCtiRiM3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_FMNIST = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "cnn = CNN(channels=1, image_dimesions=28)\n",
        "cnn = cnn.to('cuda')\n",
        "test_sgd_momentumFMNIST(25, cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "D7XjAQVjipBQ",
        "outputId": "22c34834-fa09-4fcb-85fd-3b6296d77e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing SGD optimizer with momentum 0.0...\n",
            "Epoch 1/25 - Train Loss: 2.2974, Train Acc: 21.93%, Validation Loss: 2.2941, Validation Acc: 25.60%\n",
            "Epoch 2/25 - Train Loss: 2.2898, Train Acc: 29.12%, Validation Loss: 2.2854, Validation Acc: 31.18%\n",
            "Epoch 3/25 - Train Loss: 2.2791, Train Acc: 33.50%, Validation Loss: 2.2726, Validation Acc: 34.16%\n",
            "Epoch 4/25 - Train Loss: 2.2603, Train Acc: 35.13%, Validation Loss: 2.2453, Validation Acc: 35.57%\n",
            "Epoch 5/25 - Train Loss: 2.2204, Train Acc: 37.12%, Validation Loss: 2.1885, Validation Acc: 38.31%\n",
            "Epoch 6/25 - Train Loss: 2.1485, Train Acc: 40.14%, Validation Loss: 2.1142, Validation Acc: 41.58%\n",
            "Epoch 7/25 - Train Loss: 2.0765, Train Acc: 47.56%, Validation Loss: 2.0431, Validation Acc: 53.41%\n",
            "Epoch 8/25 - Train Loss: 1.9967, Train Acc: 59.28%, Validation Loss: 1.9538, Validation Acc: 64.88%\n",
            "Epoch 9/25 - Train Loss: 1.9119, Train Acc: 67.40%, Validation Loss: 1.8799, Validation Acc: 68.84%\n",
            "Epoch 10/25 - Train Loss: 1.8507, Train Acc: 70.26%, Validation Loss: 1.8311, Validation Acc: 71.08%\n",
            "Epoch 11/25 - Train Loss: 1.8089, Train Acc: 72.39%, Validation Loss: 1.7931, Validation Acc: 73.19%\n",
            "Epoch 12/25 - Train Loss: 1.7789, Train Acc: 73.90%, Validation Loss: 1.7702, Validation Acc: 74.54%\n",
            "Epoch 13/25 - Train Loss: 1.7568, Train Acc: 75.18%, Validation Loss: 1.7521, Validation Acc: 75.49%\n",
            "Epoch 14/25 - Train Loss: 1.7402, Train Acc: 76.04%, Validation Loss: 1.7368, Validation Acc: 76.08%\n",
            "Epoch 15/25 - Train Loss: 1.7273, Train Acc: 76.75%, Validation Loss: 1.7247, Validation Acc: 76.58%\n",
            "Epoch 16/25 - Train Loss: 1.7172, Train Acc: 77.25%, Validation Loss: 1.7161, Validation Acc: 77.24%\n",
            "Epoch 17/25 - Train Loss: 1.7092, Train Acc: 77.72%, Validation Loss: 1.7091, Validation Acc: 77.60%\n",
            "Epoch 18/25 - Train Loss: 1.7025, Train Acc: 78.15%, Validation Loss: 1.7029, Validation Acc: 77.99%\n",
            "Epoch 19/25 - Train Loss: 1.6968, Train Acc: 78.50%, Validation Loss: 1.6979, Validation Acc: 78.34%\n",
            "Epoch 20/25 - Train Loss: 1.6919, Train Acc: 78.83%, Validation Loss: 1.6928, Validation Acc: 78.47%\n",
            "Epoch 21/25 - Train Loss: 1.6877, Train Acc: 79.17%, Validation Loss: 1.6886, Validation Acc: 78.89%\n",
            "Epoch 22/25 - Train Loss: 1.6838, Train Acc: 79.42%, Validation Loss: 1.6855, Validation Acc: 79.20%\n",
            "Epoch 23/25 - Train Loss: 1.6802, Train Acc: 79.72%, Validation Loss: 1.6829, Validation Acc: 79.45%\n",
            "Epoch 24/25 - Train Loss: 1.6771, Train Acc: 79.91%, Validation Loss: 1.6797, Validation Acc: 79.77%\n",
            "Epoch 25/25 - Train Loss: 1.6742, Train Acc: 80.15%, Validation Loss: 1.6761, Validation Acc: 79.88%\n",
            "Testing SGD optimizer with momentum 0.1...\n",
            "Epoch 1/25 - Train Loss: 1.6714, Train Acc: 80.33%, Validation Loss: 1.6732, Validation Acc: 80.16%\n",
            "Epoch 2/25 - Train Loss: 1.6685, Train Acc: 80.62%, Validation Loss: 1.6714, Validation Acc: 80.30%\n",
            "Epoch 3/25 - Train Loss: 1.6661, Train Acc: 80.80%, Validation Loss: 1.6690, Validation Acc: 80.45%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f152caa52510>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dimesions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_sgd_momentum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFMNIST_training_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFMNIST_test_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFMINIST_trainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFMINIST_testloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-de52e3542dd6>\u001b[0m in \u001b[0;36mtest_sgd_momentum\u001b[0;34m(trainset, testset, trainloader, testloader, num_epochs, cnn)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Train the CNN with the current optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mmomentum_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6267e413c405>\u001b[0m in \u001b[0;36mtrain_CNN\u001b[0;34m(batch_size, training_set, trainloader, test_set, testloader, optimizer, num_epochs, cnn)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def test_sgd_momentumCIFAR(num_epochs, cnn):\n",
        "    # Loop over different momentum values\n",
        "    momentum_accuracies = {}\n",
        "    for momentum in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "\n",
        "        # Load the Fashion MNIST dataset\n",
        "        transform_CIFAR_10 = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                         (0.2470, 0.2435, 0.2616))\n",
        "    # Numbers from normalization comes from this link:\n",
        "    # https://www.kaggle.com/code/fanbyprinciple/cifar10-explanation-with-pytorch\n",
        "        ])\n",
        "\n",
        "        batch_size = 64\n",
        "\n",
        "        CIFAR_10_training_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                                train=True,\n",
        "                                                download=True,\n",
        "                                                transform=transform_CIFAR_10)\n",
        "        CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set,\n",
        "                                                           batch_size=batch_size,\n",
        "                                                           shuffle=True,\n",
        "                                                           num_workers=2)\n",
        "\n",
        "        CIFAR_10_testing_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                               download=True, transform=transform_CIFAR_10)\n",
        "        CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_testing_set,\n",
        "                                                          batch_size=batch_size,\n",
        "                                                          shuffle=False,\n",
        "                                                          num_workers=2)\n",
        "\n",
        "        cnn = CNN(channels=3, image_dimesions=32)\n",
        "        cnn = cnn.to('cuda')\n",
        "        # Define the SGD optimizer with the current momentum\n",
        "        optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=momentum)\n",
        "\n",
        "        print(f\"Testing SGD optimizer with momentum {momentum}...\")\n",
        "\n",
        "        # Train the CNN with the current optimizer\n",
        "        train_accuracies = train_CNN(batch_size, CIFAR_10_training_set, CIFAR_10_trainloader, CIFAR_10_testing_set, CIFAR_10_testloader, optimizer, num_epochs, cnn)\n",
        "\n",
        "        momentum_accuracies[momentum] = train_accuracies\n",
        "\n",
        "    for momentum in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "        plt.plot(range(1, num_epochs + 1), momentum_accuracies[momentum], label=f'Momentum {momentum}')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Accuracy (%)')\n",
        "    plt.title('Training Accuracy vs. Epoch for Different Momentums')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "W6ET4UsH4eE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the CNN\n",
        "cnn = CNN(channels=3, image_dimesions=32)\n",
        "cnn = cnn.to('cuda')\n",
        "\n",
        "test_sgd_momentumCIFAR(20, cnn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a5cmRl_joHuD",
        "outputId": "ba2e71d3-660e-42af-98de-8a6f3aadfcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 71933185.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.0...\n",
            "Epoch 1/20 - Train Loss: 2.2991, Train Acc: 11.99%, Validation Loss: 2.2943, Validation Acc: 17.23%\n",
            "Epoch 2/20 - Train Loss: 2.2898, Train Acc: 19.44%, Validation Loss: 2.2837, Validation Acc: 21.06%\n",
            "Epoch 3/20 - Train Loss: 2.2765, Train Acc: 21.41%, Validation Loss: 2.2642, Validation Acc: 21.97%\n",
            "Epoch 4/20 - Train Loss: 2.2574, Train Acc: 22.30%, Validation Loss: 2.2445, Validation Acc: 22.86%\n",
            "Epoch 5/20 - Train Loss: 2.2374, Train Acc: 23.55%, Validation Loss: 2.2248, Validation Acc: 24.37%\n",
            "Epoch 6/20 - Train Loss: 2.2180, Train Acc: 25.07%, Validation Loss: 2.2055, Validation Acc: 26.52%\n",
            "Epoch 7/20 - Train Loss: 2.1996, Train Acc: 26.96%, Validation Loss: 2.1867, Validation Acc: 28.42%\n",
            "Epoch 8/20 - Train Loss: 2.1831, Train Acc: 28.51%, Validation Loss: 2.1721, Validation Acc: 30.11%\n",
            "Epoch 9/20 - Train Loss: 2.1686, Train Acc: 30.07%, Validation Loss: 2.1581, Validation Acc: 31.63%\n",
            "Epoch 10/20 - Train Loss: 2.1553, Train Acc: 31.51%, Validation Loss: 2.1460, Validation Acc: 32.72%\n",
            "Epoch 11/20 - Train Loss: 2.1431, Train Acc: 32.94%, Validation Loss: 2.1332, Validation Acc: 33.68%\n",
            "Epoch 12/20 - Train Loss: 2.1303, Train Acc: 34.38%, Validation Loss: 2.1222, Validation Acc: 35.54%\n",
            "Epoch 13/20 - Train Loss: 2.1168, Train Acc: 36.11%, Validation Loss: 2.1096, Validation Acc: 36.93%\n",
            "Epoch 14/20 - Train Loss: 2.1026, Train Acc: 38.00%, Validation Loss: 2.0967, Validation Acc: 38.37%\n",
            "Epoch 15/20 - Train Loss: 2.0885, Train Acc: 39.56%, Validation Loss: 2.0842, Validation Acc: 39.75%\n",
            "Epoch 16/20 - Train Loss: 2.0733, Train Acc: 41.22%, Validation Loss: 2.0713, Validation Acc: 41.22%\n",
            "Epoch 17/20 - Train Loss: 2.0585, Train Acc: 42.67%, Validation Loss: 2.0582, Validation Acc: 42.21%\n",
            "Epoch 18/20 - Train Loss: 2.0434, Train Acc: 44.12%, Validation Loss: 2.0458, Validation Acc: 43.20%\n",
            "Epoch 19/20 - Train Loss: 2.0289, Train Acc: 45.60%, Validation Loss: 2.0384, Validation Acc: 43.58%\n",
            "Epoch 20/20 - Train Loss: 2.0146, Train Acc: 47.21%, Validation Loss: 2.0237, Validation Acc: 45.67%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.1...\n",
            "Epoch 1/20 - Train Loss: 2.2977, Train Acc: 17.77%, Validation Loss: 2.2930, Validation Acc: 21.99%\n",
            "Epoch 2/20 - Train Loss: 2.2879, Train Acc: 23.75%, Validation Loss: 2.2819, Validation Acc: 23.97%\n",
            "Epoch 3/20 - Train Loss: 2.2736, Train Acc: 24.32%, Validation Loss: 2.2652, Validation Acc: 24.03%\n",
            "Epoch 4/20 - Train Loss: 2.2545, Train Acc: 24.45%, Validation Loss: 2.2439, Validation Acc: 23.93%\n",
            "Epoch 5/20 - Train Loss: 2.2332, Train Acc: 24.80%, Validation Loss: 2.2216, Validation Acc: 25.00%\n",
            "Epoch 6/20 - Train Loss: 2.2096, Train Acc: 26.92%, Validation Loss: 2.1965, Validation Acc: 28.05%\n",
            "Epoch 7/20 - Train Loss: 2.1876, Train Acc: 29.21%, Validation Loss: 2.1757, Validation Acc: 30.30%\n",
            "Epoch 8/20 - Train Loss: 2.1691, Train Acc: 31.13%, Validation Loss: 2.1570, Validation Acc: 32.21%\n",
            "Epoch 9/20 - Train Loss: 2.1515, Train Acc: 33.13%, Validation Loss: 2.1421, Validation Acc: 34.05%\n",
            "Epoch 10/20 - Train Loss: 2.1339, Train Acc: 34.95%, Validation Loss: 2.1257, Validation Acc: 35.25%\n",
            "Epoch 11/20 - Train Loss: 2.1171, Train Acc: 36.76%, Validation Loss: 2.1095, Validation Acc: 36.83%\n",
            "Epoch 12/20 - Train Loss: 2.1007, Train Acc: 38.39%, Validation Loss: 2.0921, Validation Acc: 38.82%\n",
            "Epoch 13/20 - Train Loss: 2.0836, Train Acc: 40.26%, Validation Loss: 2.0787, Validation Acc: 40.58%\n",
            "Epoch 14/20 - Train Loss: 2.0670, Train Acc: 42.04%, Validation Loss: 2.0632, Validation Acc: 42.00%\n",
            "Epoch 15/20 - Train Loss: 2.0506, Train Acc: 43.77%, Validation Loss: 2.0488, Validation Acc: 43.14%\n",
            "Epoch 16/20 - Train Loss: 2.0345, Train Acc: 45.61%, Validation Loss: 2.0376, Validation Acc: 44.70%\n",
            "Epoch 17/20 - Train Loss: 2.0187, Train Acc: 47.27%, Validation Loss: 2.0226, Validation Acc: 45.74%\n",
            "Epoch 18/20 - Train Loss: 2.0040, Train Acc: 48.73%, Validation Loss: 2.0150, Validation Acc: 46.72%\n",
            "Epoch 19/20 - Train Loss: 1.9895, Train Acc: 50.09%, Validation Loss: 2.0010, Validation Acc: 48.07%\n",
            "Epoch 20/20 - Train Loss: 1.9768, Train Acc: 51.34%, Validation Loss: 1.9935, Validation Acc: 48.96%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.2...\n",
            "Epoch 1/20 - Train Loss: 2.2973, Train Acc: 16.19%, Validation Loss: 2.2916, Validation Acc: 19.04%\n",
            "Epoch 2/20 - Train Loss: 2.2854, Train Acc: 19.82%, Validation Loss: 2.2773, Validation Acc: 21.87%\n",
            "Epoch 3/20 - Train Loss: 2.2687, Train Acc: 22.13%, Validation Loss: 2.2574, Validation Acc: 23.02%\n",
            "Epoch 4/20 - Train Loss: 2.2486, Train Acc: 23.18%, Validation Loss: 2.2374, Validation Acc: 24.12%\n",
            "Epoch 5/20 - Train Loss: 2.2295, Train Acc: 24.59%, Validation Loss: 2.2174, Validation Acc: 25.92%\n",
            "Epoch 6/20 - Train Loss: 2.2098, Train Acc: 26.62%, Validation Loss: 2.1977, Validation Acc: 27.95%\n",
            "Epoch 7/20 - Train Loss: 2.1900, Train Acc: 28.61%, Validation Loss: 2.1777, Validation Acc: 29.96%\n",
            "Epoch 8/20 - Train Loss: 2.1709, Train Acc: 30.64%, Validation Loss: 2.1594, Validation Acc: 32.09%\n",
            "Epoch 9/20 - Train Loss: 2.1533, Train Acc: 32.67%, Validation Loss: 2.1413, Validation Acc: 34.13%\n",
            "Epoch 10/20 - Train Loss: 2.1349, Train Acc: 34.99%, Validation Loss: 2.1235, Validation Acc: 36.04%\n",
            "Epoch 11/20 - Train Loss: 2.1148, Train Acc: 37.31%, Validation Loss: 2.1060, Validation Acc: 37.35%\n",
            "Epoch 12/20 - Train Loss: 2.0946, Train Acc: 39.35%, Validation Loss: 2.0890, Validation Acc: 39.36%\n",
            "Epoch 13/20 - Train Loss: 2.0755, Train Acc: 41.19%, Validation Loss: 2.0703, Validation Acc: 41.20%\n",
            "Epoch 14/20 - Train Loss: 2.0568, Train Acc: 43.04%, Validation Loss: 2.0564, Validation Acc: 42.61%\n",
            "Epoch 15/20 - Train Loss: 2.0397, Train Acc: 44.73%, Validation Loss: 2.0409, Validation Acc: 43.61%\n",
            "Epoch 16/20 - Train Loss: 2.0232, Train Acc: 46.37%, Validation Loss: 2.0292, Validation Acc: 45.27%\n",
            "Epoch 17/20 - Train Loss: 2.0072, Train Acc: 48.12%, Validation Loss: 2.0167, Validation Acc: 46.73%\n",
            "Epoch 18/20 - Train Loss: 1.9919, Train Acc: 49.71%, Validation Loss: 2.0055, Validation Acc: 47.36%\n",
            "Epoch 19/20 - Train Loss: 1.9781, Train Acc: 51.03%, Validation Loss: 1.9971, Validation Acc: 48.21%\n",
            "Epoch 20/20 - Train Loss: 1.9640, Train Acc: 52.36%, Validation Loss: 1.9884, Validation Acc: 48.93%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.3...\n",
            "Epoch 1/20 - Train Loss: 2.2974, Train Acc: 15.27%, Validation Loss: 2.2900, Validation Acc: 18.45%\n",
            "Epoch 2/20 - Train Loss: 2.2809, Train Acc: 20.03%, Validation Loss: 2.2685, Validation Acc: 21.34%\n",
            "Epoch 3/20 - Train Loss: 2.2589, Train Acc: 22.07%, Validation Loss: 2.2450, Validation Acc: 23.43%\n",
            "Epoch 4/20 - Train Loss: 2.2361, Train Acc: 24.02%, Validation Loss: 2.2207, Validation Acc: 25.60%\n",
            "Epoch 5/20 - Train Loss: 2.2107, Train Acc: 26.17%, Validation Loss: 2.1955, Validation Acc: 27.72%\n",
            "Epoch 6/20 - Train Loss: 2.1882, Train Acc: 28.25%, Validation Loss: 2.1752, Validation Acc: 29.22%\n",
            "Epoch 7/20 - Train Loss: 2.1689, Train Acc: 30.03%, Validation Loss: 2.1578, Validation Acc: 30.74%\n",
            "Epoch 8/20 - Train Loss: 2.1516, Train Acc: 31.89%, Validation Loss: 2.1412, Validation Acc: 32.48%\n",
            "Epoch 9/20 - Train Loss: 2.1361, Train Acc: 33.40%, Validation Loss: 2.1271, Validation Acc: 33.82%\n",
            "Epoch 10/20 - Train Loss: 2.1222, Train Acc: 34.63%, Validation Loss: 2.1149, Validation Acc: 35.21%\n",
            "Epoch 11/20 - Train Loss: 2.1081, Train Acc: 36.36%, Validation Loss: 2.1015, Validation Acc: 36.55%\n",
            "Epoch 12/20 - Train Loss: 2.0921, Train Acc: 38.19%, Validation Loss: 2.0876, Validation Acc: 38.31%\n",
            "Epoch 13/20 - Train Loss: 2.0740, Train Acc: 40.19%, Validation Loss: 2.0735, Validation Acc: 39.85%\n",
            "Epoch 14/20 - Train Loss: 2.0571, Train Acc: 42.01%, Validation Loss: 2.0596, Validation Acc: 41.06%\n",
            "Epoch 15/20 - Train Loss: 2.0408, Train Acc: 43.86%, Validation Loss: 2.0488, Validation Acc: 42.17%\n",
            "Epoch 16/20 - Train Loss: 2.0243, Train Acc: 45.95%, Validation Loss: 2.0395, Validation Acc: 43.26%\n",
            "Epoch 17/20 - Train Loss: 2.0079, Train Acc: 47.75%, Validation Loss: 2.0235, Validation Acc: 44.96%\n",
            "Epoch 18/20 - Train Loss: 1.9895, Train Acc: 50.04%, Validation Loss: 2.0091, Validation Acc: 46.89%\n",
            "Epoch 19/20 - Train Loss: 1.9706, Train Acc: 52.20%, Validation Loss: 1.9933, Validation Acc: 48.15%\n",
            "Epoch 20/20 - Train Loss: 1.9509, Train Acc: 54.47%, Validation Loss: 1.9818, Validation Acc: 50.18%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.4...\n",
            "Epoch 1/20 - Train Loss: 2.2978, Train Acc: 15.66%, Validation Loss: 2.2920, Validation Acc: 18.72%\n",
            "Epoch 2/20 - Train Loss: 2.2850, Train Acc: 20.86%, Validation Loss: 2.2757, Validation Acc: 21.97%\n",
            "Epoch 3/20 - Train Loss: 2.2659, Train Acc: 22.17%, Validation Loss: 2.2555, Validation Acc: 22.49%\n",
            "Epoch 4/20 - Train Loss: 2.2427, Train Acc: 23.06%, Validation Loss: 2.2276, Validation Acc: 23.90%\n",
            "Epoch 5/20 - Train Loss: 2.2152, Train Acc: 25.52%, Validation Loss: 2.2007, Validation Acc: 27.36%\n",
            "Epoch 6/20 - Train Loss: 2.1900, Train Acc: 28.59%, Validation Loss: 2.1743, Validation Acc: 30.05%\n",
            "Epoch 7/20 - Train Loss: 2.1658, Train Acc: 31.86%, Validation Loss: 2.1498, Validation Acc: 33.33%\n",
            "Epoch 8/20 - Train Loss: 2.1380, Train Acc: 34.87%, Validation Loss: 2.1218, Validation Acc: 35.81%\n",
            "Epoch 9/20 - Train Loss: 2.1109, Train Acc: 37.13%, Validation Loss: 2.0999, Validation Acc: 38.18%\n",
            "Epoch 10/20 - Train Loss: 2.0871, Train Acc: 39.30%, Validation Loss: 2.0758, Validation Acc: 40.40%\n",
            "Epoch 11/20 - Train Loss: 2.0647, Train Acc: 41.56%, Validation Loss: 2.0570, Validation Acc: 42.27%\n",
            "Epoch 12/20 - Train Loss: 2.0430, Train Acc: 43.98%, Validation Loss: 2.0377, Validation Acc: 43.73%\n",
            "Epoch 13/20 - Train Loss: 2.0232, Train Acc: 46.13%, Validation Loss: 2.0218, Validation Acc: 45.94%\n",
            "Epoch 14/20 - Train Loss: 2.0034, Train Acc: 48.17%, Validation Loss: 2.0055, Validation Acc: 47.64%\n",
            "Epoch 15/20 - Train Loss: 1.9847, Train Acc: 50.52%, Validation Loss: 1.9933, Validation Acc: 48.64%\n",
            "Epoch 16/20 - Train Loss: 1.9659, Train Acc: 52.65%, Validation Loss: 1.9885, Validation Acc: 49.07%\n",
            "Epoch 17/20 - Train Loss: 1.9482, Train Acc: 54.78%, Validation Loss: 1.9714, Validation Acc: 50.99%\n",
            "Epoch 18/20 - Train Loss: 1.9306, Train Acc: 56.50%, Validation Loss: 1.9577, Validation Acc: 52.53%\n",
            "Epoch 19/20 - Train Loss: 1.9133, Train Acc: 58.34%, Validation Loss: 1.9487, Validation Acc: 53.13%\n",
            "Epoch 20/20 - Train Loss: 1.8975, Train Acc: 59.88%, Validation Loss: 1.9362, Validation Acc: 54.53%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.5...\n",
            "Epoch 1/20 - Train Loss: 2.2888, Train Acc: 16.31%, Validation Loss: 2.2734, Validation Acc: 19.18%\n",
            "Epoch 2/20 - Train Loss: 2.2606, Train Acc: 21.60%, Validation Loss: 2.2442, Validation Acc: 23.57%\n",
            "Epoch 3/20 - Train Loss: 2.2335, Train Acc: 24.54%, Validation Loss: 2.2210, Validation Acc: 25.95%\n",
            "Epoch 4/20 - Train Loss: 2.2086, Train Acc: 27.00%, Validation Loss: 2.1927, Validation Acc: 28.52%\n",
            "Epoch 5/20 - Train Loss: 2.1816, Train Acc: 29.36%, Validation Loss: 2.1662, Validation Acc: 30.54%\n",
            "Epoch 6/20 - Train Loss: 2.1566, Train Acc: 32.29%, Validation Loss: 2.1407, Validation Acc: 34.35%\n",
            "Epoch 7/20 - Train Loss: 2.1282, Train Acc: 35.89%, Validation Loss: 2.1125, Validation Acc: 37.56%\n",
            "Epoch 8/20 - Train Loss: 2.0953, Train Acc: 39.30%, Validation Loss: 2.0825, Validation Acc: 40.19%\n",
            "Epoch 9/20 - Train Loss: 2.0666, Train Acc: 41.81%, Validation Loss: 2.0596, Validation Acc: 41.79%\n",
            "Epoch 10/20 - Train Loss: 2.0417, Train Acc: 44.00%, Validation Loss: 2.0414, Validation Acc: 43.39%\n",
            "Epoch 11/20 - Train Loss: 2.0218, Train Acc: 45.79%, Validation Loss: 2.0250, Validation Acc: 44.79%\n",
            "Epoch 12/20 - Train Loss: 2.0028, Train Acc: 47.43%, Validation Loss: 2.0119, Validation Acc: 45.80%\n",
            "Epoch 13/20 - Train Loss: 1.9858, Train Acc: 49.15%, Validation Loss: 2.0033, Validation Acc: 46.29%\n",
            "Epoch 14/20 - Train Loss: 1.9692, Train Acc: 50.62%, Validation Loss: 1.9881, Validation Acc: 48.03%\n",
            "Epoch 15/20 - Train Loss: 1.9531, Train Acc: 52.55%, Validation Loss: 1.9756, Validation Acc: 49.86%\n",
            "Epoch 16/20 - Train Loss: 1.9347, Train Acc: 55.02%, Validation Loss: 1.9665, Validation Acc: 51.29%\n",
            "Epoch 17/20 - Train Loss: 1.9161, Train Acc: 57.36%, Validation Loss: 1.9546, Validation Acc: 52.22%\n",
            "Epoch 18/20 - Train Loss: 1.9003, Train Acc: 58.94%, Validation Loss: 1.9415, Validation Acc: 53.33%\n",
            "Epoch 19/20 - Train Loss: 1.8829, Train Acc: 60.60%, Validation Loss: 1.9356, Validation Acc: 54.11%\n",
            "Epoch 20/20 - Train Loss: 1.8668, Train Acc: 62.21%, Validation Loss: 1.9279, Validation Acc: 54.52%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.6...\n",
            "Epoch 1/20 - Train Loss: 2.2927, Train Acc: 17.42%, Validation Loss: 2.2815, Validation Acc: 22.85%\n",
            "Epoch 2/20 - Train Loss: 2.2625, Train Acc: 23.56%, Validation Loss: 2.2379, Validation Acc: 24.13%\n",
            "Epoch 3/20 - Train Loss: 2.2133, Train Acc: 26.15%, Validation Loss: 2.1862, Validation Acc: 29.21%\n",
            "Epoch 4/20 - Train Loss: 2.1678, Train Acc: 30.37%, Validation Loss: 2.1452, Validation Acc: 33.56%\n",
            "Epoch 5/20 - Train Loss: 2.1289, Train Acc: 35.03%, Validation Loss: 2.1063, Validation Acc: 37.72%\n",
            "Epoch 6/20 - Train Loss: 2.0895, Train Acc: 39.72%, Validation Loss: 2.0726, Validation Acc: 41.60%\n",
            "Epoch 7/20 - Train Loss: 2.0529, Train Acc: 43.87%, Validation Loss: 2.0406, Validation Acc: 44.80%\n",
            "Epoch 8/20 - Train Loss: 2.0211, Train Acc: 46.87%, Validation Loss: 2.0177, Validation Acc: 46.72%\n",
            "Epoch 9/20 - Train Loss: 1.9949, Train Acc: 49.44%, Validation Loss: 2.0029, Validation Acc: 47.86%\n",
            "Epoch 10/20 - Train Loss: 1.9710, Train Acc: 51.50%, Validation Loss: 1.9849, Validation Acc: 49.74%\n",
            "Epoch 11/20 - Train Loss: 1.9513, Train Acc: 53.49%, Validation Loss: 1.9696, Validation Acc: 50.73%\n",
            "Epoch 12/20 - Train Loss: 1.9324, Train Acc: 55.33%, Validation Loss: 1.9646, Validation Acc: 51.12%\n",
            "Epoch 13/20 - Train Loss: 1.9182, Train Acc: 56.70%, Validation Loss: 1.9544, Validation Acc: 52.03%\n",
            "Epoch 14/20 - Train Loss: 1.9029, Train Acc: 58.29%, Validation Loss: 1.9471, Validation Acc: 52.71%\n",
            "Epoch 15/20 - Train Loss: 1.8882, Train Acc: 59.78%, Validation Loss: 1.9381, Validation Acc: 53.44%\n",
            "Epoch 16/20 - Train Loss: 1.8714, Train Acc: 61.29%, Validation Loss: 1.9327, Validation Acc: 53.96%\n",
            "Epoch 17/20 - Train Loss: 1.8567, Train Acc: 62.92%, Validation Loss: 1.9199, Validation Acc: 55.34%\n",
            "Epoch 18/20 - Train Loss: 1.8417, Train Acc: 64.59%, Validation Loss: 1.9204, Validation Acc: 55.43%\n",
            "Epoch 19/20 - Train Loss: 1.8271, Train Acc: 65.97%, Validation Loss: 1.9124, Validation Acc: 55.69%\n",
            "Epoch 20/20 - Train Loss: 1.8163, Train Acc: 67.14%, Validation Loss: 1.9023, Validation Acc: 56.95%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.7...\n",
            "Epoch 1/20 - Train Loss: 2.2907, Train Acc: 18.36%, Validation Loss: 2.2719, Validation Acc: 21.08%\n",
            "Epoch 2/20 - Train Loss: 2.2526, Train Acc: 22.13%, Validation Loss: 2.2258, Validation Acc: 23.41%\n",
            "Epoch 3/20 - Train Loss: 2.2059, Train Acc: 24.86%, Validation Loss: 2.1792, Validation Acc: 27.00%\n",
            "Epoch 4/20 - Train Loss: 2.1588, Train Acc: 30.83%, Validation Loss: 2.1313, Validation Acc: 34.87%\n",
            "Epoch 5/20 - Train Loss: 2.1075, Train Acc: 37.86%, Validation Loss: 2.0812, Validation Acc: 40.46%\n",
            "Epoch 6/20 - Train Loss: 2.0583, Train Acc: 42.41%, Validation Loss: 2.0443, Validation Acc: 43.32%\n",
            "Epoch 7/20 - Train Loss: 2.0190, Train Acc: 46.55%, Validation Loss: 2.0161, Validation Acc: 46.26%\n",
            "Epoch 8/20 - Train Loss: 1.9837, Train Acc: 50.66%, Validation Loss: 1.9877, Validation Acc: 50.02%\n",
            "Epoch 9/20 - Train Loss: 1.9535, Train Acc: 53.70%, Validation Loss: 1.9655, Validation Acc: 51.95%\n",
            "Epoch 10/20 - Train Loss: 1.9277, Train Acc: 56.30%, Validation Loss: 1.9601, Validation Acc: 51.42%\n",
            "Epoch 11/20 - Train Loss: 1.9048, Train Acc: 58.48%, Validation Loss: 1.9398, Validation Acc: 53.55%\n",
            "Epoch 12/20 - Train Loss: 1.8832, Train Acc: 60.61%, Validation Loss: 1.9317, Validation Acc: 54.21%\n",
            "Epoch 13/20 - Train Loss: 1.8668, Train Acc: 62.21%, Validation Loss: 1.9169, Validation Acc: 55.30%\n",
            "Epoch 14/20 - Train Loss: 1.8480, Train Acc: 64.00%, Validation Loss: 1.9148, Validation Acc: 55.54%\n",
            "Epoch 15/20 - Train Loss: 1.8343, Train Acc: 65.36%, Validation Loss: 1.9056, Validation Acc: 56.80%\n",
            "Epoch 16/20 - Train Loss: 1.8219, Train Acc: 66.60%, Validation Loss: 1.9070, Validation Acc: 56.49%\n",
            "Epoch 17/20 - Train Loss: 1.8086, Train Acc: 67.99%, Validation Loss: 1.8974, Validation Acc: 56.85%\n",
            "Epoch 18/20 - Train Loss: 1.7977, Train Acc: 68.91%, Validation Loss: 1.8944, Validation Acc: 57.55%\n",
            "Epoch 19/20 - Train Loss: 1.7857, Train Acc: 70.01%, Validation Loss: 1.8896, Validation Acc: 58.17%\n",
            "Epoch 20/20 - Train Loss: 1.7726, Train Acc: 71.25%, Validation Loss: 1.8873, Validation Acc: 58.15%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.8...\n",
            "Epoch 1/20 - Train Loss: 2.2690, Train Acc: 18.68%, Validation Loss: 2.2343, Validation Acc: 25.02%\n",
            "Epoch 2/20 - Train Loss: 2.2018, Train Acc: 27.98%, Validation Loss: 2.1625, Validation Acc: 31.03%\n",
            "Epoch 3/20 - Train Loss: 2.1425, Train Acc: 32.73%, Validation Loss: 2.1171, Validation Acc: 35.11%\n",
            "Epoch 4/20 - Train Loss: 2.0933, Train Acc: 38.69%, Validation Loss: 2.0631, Validation Acc: 41.58%\n",
            "Epoch 5/20 - Train Loss: 2.0358, Train Acc: 45.09%, Validation Loss: 2.0157, Validation Acc: 47.00%\n",
            "Epoch 6/20 - Train Loss: 1.9924, Train Acc: 49.45%, Validation Loss: 1.9897, Validation Acc: 49.05%\n",
            "Epoch 7/20 - Train Loss: 1.9617, Train Acc: 52.20%, Validation Loss: 1.9701, Validation Acc: 50.73%\n",
            "Epoch 8/20 - Train Loss: 1.9333, Train Acc: 55.01%, Validation Loss: 1.9528, Validation Acc: 52.10%\n",
            "Epoch 9/20 - Train Loss: 1.9072, Train Acc: 57.77%, Validation Loss: 1.9346, Validation Acc: 54.40%\n",
            "Epoch 10/20 - Train Loss: 1.8875, Train Acc: 60.02%, Validation Loss: 1.9309, Validation Acc: 54.76%\n",
            "Epoch 11/20 - Train Loss: 1.8707, Train Acc: 61.82%, Validation Loss: 1.9187, Validation Acc: 55.53%\n",
            "Epoch 12/20 - Train Loss: 1.8518, Train Acc: 63.40%, Validation Loss: 1.9178, Validation Acc: 55.41%\n",
            "Epoch 13/20 - Train Loss: 1.8391, Train Acc: 64.78%, Validation Loss: 1.9042, Validation Acc: 56.69%\n",
            "Epoch 14/20 - Train Loss: 1.8277, Train Acc: 65.83%, Validation Loss: 1.9014, Validation Acc: 57.23%\n",
            "Epoch 15/20 - Train Loss: 1.8128, Train Acc: 67.15%, Validation Loss: 1.8992, Validation Acc: 57.10%\n",
            "Epoch 16/20 - Train Loss: 1.8011, Train Acc: 68.23%, Validation Loss: 1.8927, Validation Acc: 57.82%\n",
            "Epoch 17/20 - Train Loss: 1.7917, Train Acc: 69.22%, Validation Loss: 1.8914, Validation Acc: 58.17%\n",
            "Epoch 18/20 - Train Loss: 1.7892, Train Acc: 69.48%, Validation Loss: 1.8867, Validation Acc: 58.16%\n",
            "Epoch 19/20 - Train Loss: 1.7853, Train Acc: 69.82%, Validation Loss: 1.8857, Validation Acc: 58.43%\n",
            "Epoch 20/20 - Train Loss: 1.7811, Train Acc: 70.33%, Validation Loss: 1.8972, Validation Acc: 57.34%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 0.9...\n",
            "Epoch 1/20 - Train Loss: 2.2370, Train Acc: 23.74%, Validation Loss: 2.1617, Validation Acc: 30.88%\n",
            "Epoch 2/20 - Train Loss: 2.1165, Train Acc: 35.86%, Validation Loss: 2.0635, Validation Acc: 41.47%\n",
            "Epoch 3/20 - Train Loss: 2.0310, Train Acc: 44.39%, Validation Loss: 2.0167, Validation Acc: 45.69%\n",
            "Epoch 4/20 - Train Loss: 1.9789, Train Acc: 50.32%, Validation Loss: 1.9681, Validation Acc: 51.42%\n",
            "Epoch 5/20 - Train Loss: 1.9367, Train Acc: 54.81%, Validation Loss: 1.9488, Validation Acc: 52.46%\n",
            "Epoch 6/20 - Train Loss: 1.9119, Train Acc: 57.38%, Validation Loss: 1.9348, Validation Acc: 53.70%\n",
            "Epoch 7/20 - Train Loss: 1.8918, Train Acc: 59.25%, Validation Loss: 1.9338, Validation Acc: 53.80%\n",
            "Epoch 8/20 - Train Loss: 1.8744, Train Acc: 60.77%, Validation Loss: 1.9170, Validation Acc: 55.90%\n",
            "Epoch 9/20 - Train Loss: 1.8646, Train Acc: 61.61%, Validation Loss: 1.9161, Validation Acc: 55.50%\n",
            "Epoch 10/20 - Train Loss: 1.8550, Train Acc: 62.55%, Validation Loss: 1.9113, Validation Acc: 55.74%\n",
            "Epoch 11/20 - Train Loss: 1.8505, Train Acc: 62.99%, Validation Loss: 1.9120, Validation Acc: 56.03%\n",
            "Epoch 12/20 - Train Loss: 1.8383, Train Acc: 63.94%, Validation Loss: 1.8978, Validation Acc: 56.99%\n",
            "Epoch 13/20 - Train Loss: 1.8248, Train Acc: 65.15%, Validation Loss: 1.8925, Validation Acc: 57.93%\n",
            "Epoch 14/20 - Train Loss: 1.8211, Train Acc: 65.60%, Validation Loss: 1.8902, Validation Acc: 57.56%\n",
            "Epoch 15/20 - Train Loss: 1.8129, Train Acc: 66.21%, Validation Loss: 1.8875, Validation Acc: 58.27%\n",
            "Epoch 16/20 - Train Loss: 1.8053, Train Acc: 67.04%, Validation Loss: 1.8854, Validation Acc: 58.06%\n",
            "Epoch 17/20 - Train Loss: 1.7999, Train Acc: 67.43%, Validation Loss: 1.8799, Validation Acc: 58.65%\n",
            "Epoch 18/20 - Train Loss: 1.7919, Train Acc: 68.16%, Validation Loss: 1.8755, Validation Acc: 59.00%\n",
            "Epoch 19/20 - Train Loss: 1.7830, Train Acc: 68.99%, Validation Loss: 1.8789, Validation Acc: 58.45%\n",
            "Epoch 20/20 - Train Loss: 1.7737, Train Acc: 69.83%, Validation Loss: 1.8650, Validation Acc: 60.14%\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing SGD optimizer with momentum 1.0...\n",
            "Epoch 1/20 - Train Loss: 2.1649, Train Acc: 28.88%, Validation Loss: 2.0995, Validation Acc: 35.71%\n",
            "Epoch 2/20 - Train Loss: 2.1575, Train Acc: 30.23%, Validation Loss: 2.1815, Validation Acc: 27.93%\n",
            "Epoch 3/20 - Train Loss: 2.2271, Train Acc: 23.38%, Validation Loss: 2.2287, Validation Acc: 23.18%\n",
            "Epoch 4/20 - Train Loss: 2.2366, Train Acc: 22.45%, Validation Loss: 2.2424, Validation Acc: 21.83%\n",
            "Epoch 5/20 - Train Loss: 2.2767, Train Acc: 18.46%, Validation Loss: 2.3072, Validation Acc: 15.35%\n",
            "Epoch 6/20 - Train Loss: 2.3146, Train Acc: 14.64%, Validation Loss: 2.3197, Validation Acc: 14.12%\n",
            "Epoch 7/20 - Train Loss: 2.3314, Train Acc: 12.97%, Validation Loss: 2.3524, Validation Acc: 10.84%\n",
            "Epoch 8/20 - Train Loss: 2.3595, Train Acc: 10.17%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 9/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 10/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 11/20 - Train Loss: 2.3611, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 12/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 13/20 - Train Loss: 2.3611, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 14/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 15/20 - Train Loss: 2.3611, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 16/20 - Train Loss: 2.3611, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 17/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 18/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 19/20 - Train Loss: 2.3612, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n",
            "Epoch 20/20 - Train Loss: 2.3610, Train Acc: 10.00%, Validation Loss: 2.3607, Validation Acc: 10.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number 9"
      ],
      "metadata": {
        "id": "3gapJXE55IUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Check if a GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Specify the ResNet version you want (e.g., ResNet-18, ResNet-50, etc.)\n",
        "model = models.resnet50(pretrained=True)\n",
        "model = nn.Sequential(*list(model.children())[:-1])  # Remove the last fully connected layer\n",
        "\n",
        "batch_size = 64\n",
        "CIFAR_10_training_set = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                                train=True,\n",
        "                                                download=True,\n",
        "                                                transform=transform_CIFAR_10)\n",
        "CIFAR_10_trainloader = torch.utils.data.DataLoader(CIFAR_10_training_set,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   num_workers=2)\n",
        "\n",
        "CIFAR_10_testing_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                                   download=True, transform=transform_CIFAR_10)\n",
        "CIFAR_10_testloader = torch.utils.data.DataLoader(CIFAR_10_testing_set,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=2)\n",
        "\n",
        "# Define a custom classifier with two fully connected layers\n",
        "classifier = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(2048, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# Combine the ResNet feature extractor and the custom classifier\n",
        "model.fc = classifier\n",
        "\n",
        "# Put the entire model on the GPU\n",
        "model.to(device)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 40  # You can adjust the number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(CIFAR_10_trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(CIFAR_10_trainloader)}, Train Accuracy: {(100 * correct / total):.2f}%\")\n",
        "\n",
        "    # Evaluation on the test set after each epoch\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in CIFAR_10_testloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on CIFAR-10 test set after epoch {epoch + 1}: {accuracy:.2f}%')\n",
        "\n",
        "# Final test accuracy\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in CIFAR_10_testloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Final accuracy on CIFAR-10 test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSBnrwQn84J2",
        "outputId": "1996cd71-2174-46ec-d723-0cd2d2c01079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 81.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 1.2244090189409378, Train Accuracy: 57.94%\n",
            "Accuracy on CIFAR-10 test set after epoch 1: 77.88%\n",
            "Epoch 2, Loss: 0.6198589304447784, Train Accuracy: 80.11%\n",
            "Accuracy on CIFAR-10 test set after epoch 2: 81.87%\n",
            "Epoch 3, Loss: 0.4337299216700637, Train Accuracy: 85.98%\n",
            "Accuracy on CIFAR-10 test set after epoch 3: 82.64%\n",
            "Epoch 4, Loss: 0.31487677347324694, Train Accuracy: 89.82%\n",
            "Accuracy on CIFAR-10 test set after epoch 4: 83.38%\n",
            "Epoch 5, Loss: 0.23715059997518653, Train Accuracy: 92.45%\n",
            "Accuracy on CIFAR-10 test set after epoch 5: 83.24%\n",
            "Epoch 6, Loss: 0.18558451162694056, Train Accuracy: 94.18%\n",
            "Accuracy on CIFAR-10 test set after epoch 6: 82.95%\n",
            "Epoch 7, Loss: 0.14867209320378197, Train Accuracy: 95.28%\n",
            "Accuracy on CIFAR-10 test set after epoch 7: 83.44%\n",
            "Epoch 8, Loss: 0.12153757818619651, Train Accuracy: 96.13%\n",
            "Accuracy on CIFAR-10 test set after epoch 8: 83.67%\n",
            "Epoch 9, Loss: 0.1114966936726266, Train Accuracy: 96.55%\n",
            "Accuracy on CIFAR-10 test set after epoch 9: 84.24%\n",
            "Epoch 10, Loss: 0.0892395104616261, Train Accuracy: 97.21%\n",
            "Accuracy on CIFAR-10 test set after epoch 10: 84.49%\n",
            "Epoch 11, Loss: 0.07248849900322668, Train Accuracy: 97.70%\n",
            "Accuracy on CIFAR-10 test set after epoch 11: 84.05%\n",
            "Epoch 12, Loss: 0.06525364067450719, Train Accuracy: 97.92%\n",
            "Accuracy on CIFAR-10 test set after epoch 12: 84.43%\n",
            "Epoch 13, Loss: 0.053542849446689744, Train Accuracy: 98.38%\n",
            "Accuracy on CIFAR-10 test set after epoch 13: 84.85%\n",
            "Epoch 14, Loss: 0.050107236437397104, Train Accuracy: 98.45%\n",
            "Accuracy on CIFAR-10 test set after epoch 14: 84.67%\n",
            "Epoch 15, Loss: 0.04196197661253221, Train Accuracy: 98.69%\n",
            "Accuracy on CIFAR-10 test set after epoch 15: 84.53%\n",
            "Epoch 16, Loss: 0.043525084614833516, Train Accuracy: 98.64%\n",
            "Accuracy on CIFAR-10 test set after epoch 16: 84.47%\n",
            "Epoch 17, Loss: 0.03425340804294028, Train Accuracy: 98.92%\n",
            "Accuracy on CIFAR-10 test set after epoch 17: 84.10%\n",
            "Epoch 18, Loss: 0.035285907014391604, Train Accuracy: 98.96%\n",
            "Accuracy on CIFAR-10 test set after epoch 18: 84.76%\n",
            "Epoch 19, Loss: 0.032685670060524835, Train Accuracy: 99.00%\n",
            "Accuracy on CIFAR-10 test set after epoch 19: 84.79%\n",
            "Epoch 20, Loss: 0.027483851143904506, Train Accuracy: 99.16%\n",
            "Accuracy on CIFAR-10 test set after epoch 20: 85.06%\n",
            "Epoch 21, Loss: 0.023353215156267742, Train Accuracy: 99.29%\n",
            "Accuracy on CIFAR-10 test set after epoch 21: 84.92%\n",
            "Epoch 22, Loss: 0.02743225790949686, Train Accuracy: 99.20%\n",
            "Accuracy on CIFAR-10 test set after epoch 22: 84.89%\n",
            "Epoch 23, Loss: 0.02281492828409952, Train Accuracy: 99.28%\n",
            "Accuracy on CIFAR-10 test set after epoch 23: 84.88%\n",
            "Epoch 24, Loss: 0.021872620270757214, Train Accuracy: 99.34%\n",
            "Accuracy on CIFAR-10 test set after epoch 24: 84.72%\n",
            "Epoch 25, Loss: 0.020210814633359955, Train Accuracy: 99.39%\n",
            "Accuracy on CIFAR-10 test set after epoch 25: 84.59%\n",
            "Epoch 26, Loss: 0.02245811166626144, Train Accuracy: 99.35%\n",
            "Accuracy on CIFAR-10 test set after epoch 26: 84.79%\n",
            "Epoch 27, Loss: 0.020883465861141516, Train Accuracy: 99.40%\n",
            "Accuracy on CIFAR-10 test set after epoch 27: 84.71%\n",
            "Epoch 28, Loss: 0.016856440683016064, Train Accuracy: 99.52%\n",
            "Accuracy on CIFAR-10 test set after epoch 28: 84.64%\n",
            "Epoch 29, Loss: 0.01895618487973912, Train Accuracy: 99.44%\n",
            "Accuracy on CIFAR-10 test set after epoch 29: 84.87%\n",
            "Epoch 30, Loss: 0.018612475947702014, Train Accuracy: 99.41%\n",
            "Accuracy on CIFAR-10 test set after epoch 30: 85.42%\n",
            "Epoch 31, Loss: 0.013940146727086393, Train Accuracy: 99.59%\n",
            "Accuracy on CIFAR-10 test set after epoch 31: 84.65%\n",
            "Epoch 32, Loss: 0.014539413934792666, Train Accuracy: 99.57%\n",
            "Accuracy on CIFAR-10 test set after epoch 32: 85.08%\n",
            "Epoch 33, Loss: 0.01107882394468883, Train Accuracy: 99.68%\n",
            "Accuracy on CIFAR-10 test set after epoch 33: 85.46%\n",
            "Epoch 34, Loss: 0.0173832684265324, Train Accuracy: 99.53%\n",
            "Accuracy on CIFAR-10 test set after epoch 34: 85.17%\n",
            "Epoch 35, Loss: 0.014732132200635505, Train Accuracy: 99.55%\n",
            "Accuracy on CIFAR-10 test set after epoch 35: 85.27%\n",
            "Epoch 36, Loss: 0.010201622906774122, Train Accuracy: 99.70%\n",
            "Accuracy on CIFAR-10 test set after epoch 36: 85.48%\n",
            "Epoch 37, Loss: 0.010877345585754192, Train Accuracy: 99.68%\n",
            "Accuracy on CIFAR-10 test set after epoch 37: 85.06%\n",
            "Epoch 38, Loss: 0.00900422182073906, Train Accuracy: 99.72%\n",
            "Accuracy on CIFAR-10 test set after epoch 38: 85.44%\n",
            "Epoch 39, Loss: 0.008514343108155626, Train Accuracy: 99.73%\n",
            "Accuracy on CIFAR-10 test set after epoch 39: 85.28%\n",
            "Epoch 40, Loss: 0.011938374662326555, Train Accuracy: 99.65%\n",
            "Accuracy on CIFAR-10 test set after epoch 40: 85.56%\n",
            "Final accuracy on CIFAR-10 test set: 85.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = list(range(1, 41))  # Epochs 1 to 40\n",
        "train_accuracy = [\n",
        "    57.94, 80.11, 85.98, 89.82, 92.45, 94.18, 95.28, 96.13, 96.55, 97.21,\n",
        "    97.70, 97.92, 98.38, 98.45, 98.69, 98.64, 98.92, 98.96, 99.00, 99.16,\n",
        "    99.29, 99.20, 99.28, 99.34, 99.39, 99.35, 99.40, 99.52, 99.44, 99.41,\n",
        "    99.59, 99.57, 99.68, 99.53, 99.55, 99.70, 99.68, 99.72, 99.73, 99.65\n",
        "]\n",
        "\n",
        "test_accuracy = [\n",
        "    77.88, 81.87, 82.64, 83.38, 83.24, 82.95, 83.44, 83.67, 84.24, 84.49,\n",
        "    84.05, 84.43, 84.85, 84.67, 84.53, 84.47, 84.10, 84.76, 84.79, 85.06,\n",
        "    84.92, 84.89, 84.88, 84.72, 84.59, 84.79, 84.71, 84.64, 84.87, 85.42,\n",
        "    84.65, 85.08, 85.46, 85.17, 85.27, 85.48, 85.06, 85.44, 85.28, 85.56\n",
        "]\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_accuracy, label='Training Accuracy', marker='o', linestyle='-')\n",
        "plt.plot(epochs, test_accuracy, label='Test Accuracy', marker='o', linestyle='-')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Accuracy vs. Test Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "r5vu4QXJBCL4",
        "outputId": "250f1360-d3f7-4edd-948b-a27066444a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}